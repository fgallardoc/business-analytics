{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "\n",
    "* [Trabajo de los datos](#1-borrarid)\n",
    "    1. [Borra ID + Separa variable dependiente](#1-borrarid)\n",
    "    2. [División de los datos](#2-dividedatos)\n",
    "    3. [Vriables constantes](#3-constantes)\n",
    "    4. [Variables con repeticiones](#4-repeticiones)\n",
    "    5. [Variables nulas](#5-nulas)\n",
    "    6. [Variables muy correlacionadas](#6-correlacionadas)\n",
    "    7. [Variables con baja contribución](#7-bajacontribucion)\n",
    "    8. [Resumen](#8-resumen)\n",
    "* [Normalización](#normalizacion)\n",
    "* [Regresión Lineal](#regresion)\n",
    "* [Árbol de Decisión](#arbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Carga de datos:\n",
    "data = pd.read_csv('X_y_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_272</th>\n",
       "      <th>feature_273</th>\n",
       "      <th>feature_274</th>\n",
       "      <th>feature_275</th>\n",
       "      <th>feature_276</th>\n",
       "      <th>feature_277</th>\n",
       "      <th>feature_278</th>\n",
       "      <th>feature_279</th>\n",
       "      <th>feature_280</th>\n",
       "      <th>feature_281</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40497</td>\n",
       "      <td>22.462366</td>\n",
       "      <td>68.340790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>38.088184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36089</td>\n",
       "      <td>2.886154</td>\n",
       "      <td>14.533652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.147692</td>\n",
       "      <td>8.377793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16765</td>\n",
       "      <td>13.690608</td>\n",
       "      <td>21.448212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.906077</td>\n",
       "      <td>12.762038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9841</td>\n",
       "      <td>6.963303</td>\n",
       "      <td>7.443890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.776758</td>\n",
       "      <td>4.237846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2618</td>\n",
       "      <td>123.869190</td>\n",
       "      <td>129.566220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>43.328970</td>\n",
       "      <td>62.774147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16981</td>\n",
       "      <td>1.675439</td>\n",
       "      <td>8.113229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>4.374749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7473</td>\n",
       "      <td>7.598214</td>\n",
       "      <td>9.585806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.220982</td>\n",
       "      <td>5.443936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24771</td>\n",
       "      <td>10.630660</td>\n",
       "      <td>17.882992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.018276</td>\n",
       "      <td>10.396790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41950</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>0.320449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040336</td>\n",
       "      <td>0.213147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23874</td>\n",
       "      <td>10.630660</td>\n",
       "      <td>17.882992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.018276</td>\n",
       "      <td>10.396790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50703</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>31.683943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.833334</td>\n",
       "      <td>16.227720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5653</td>\n",
       "      <td>122.812930</td>\n",
       "      <td>109.961100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>44.894543</td>\n",
       "      <td>74.547530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28049</td>\n",
       "      <td>38.200640</td>\n",
       "      <td>75.516920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.194221</td>\n",
       "      <td>44.316776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33897</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>1.259083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203209</td>\n",
       "      <td>0.774866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38579</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.599342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.766783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17139</td>\n",
       "      <td>2.338830</td>\n",
       "      <td>7.547891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659670</td>\n",
       "      <td>4.358683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35114</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.365769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.185577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34670</td>\n",
       "      <td>4.744361</td>\n",
       "      <td>11.464004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.849624</td>\n",
       "      <td>7.103421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50365</td>\n",
       "      <td>0.297521</td>\n",
       "      <td>0.751248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123967</td>\n",
       "      <td>0.427761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>37019</td>\n",
       "      <td>29.901410</td>\n",
       "      <td>88.236900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.549295</td>\n",
       "      <td>50.174004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>39811</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.224094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.149033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>38817</td>\n",
       "      <td>3.434286</td>\n",
       "      <td>13.722764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.325714</td>\n",
       "      <td>6.842069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5911</td>\n",
       "      <td>122.812930</td>\n",
       "      <td>109.961100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>44.894543</td>\n",
       "      <td>74.547530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>51397</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>3.043390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.526070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29843</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.341123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14334</td>\n",
       "      <td>5.588235</td>\n",
       "      <td>7.664987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>4.479437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32334</td>\n",
       "      <td>4.468750</td>\n",
       "      <td>19.360050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.671875</td>\n",
       "      <td>9.448966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10115</td>\n",
       "      <td>2.114014</td>\n",
       "      <td>6.598158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.859858</td>\n",
       "      <td>3.364878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5972</td>\n",
       "      <td>122.812930</td>\n",
       "      <td>109.961100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>44.894543</td>\n",
       "      <td>74.547530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41887</th>\n",
       "      <td>37619</td>\n",
       "      <td>28.979840</td>\n",
       "      <td>41.032190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.143146</td>\n",
       "      <td>23.392807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41888</th>\n",
       "      <td>5072</td>\n",
       "      <td>122.812930</td>\n",
       "      <td>109.961100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>44.894543</td>\n",
       "      <td>74.547530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41889</th>\n",
       "      <td>2163</td>\n",
       "      <td>123.869190</td>\n",
       "      <td>129.566220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>43.328970</td>\n",
       "      <td>62.774147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41890</th>\n",
       "      <td>38804</td>\n",
       "      <td>45.230770</td>\n",
       "      <td>51.242740</td>\n",
       "      <td>10.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>17.564102</td>\n",
       "      <td>27.144245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41891</th>\n",
       "      <td>6921</td>\n",
       "      <td>7.332609</td>\n",
       "      <td>13.594126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.143478</td>\n",
       "      <td>7.177945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41892</th>\n",
       "      <td>38984</td>\n",
       "      <td>3.434286</td>\n",
       "      <td>13.722764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.325714</td>\n",
       "      <td>6.842069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41893</th>\n",
       "      <td>27469</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.646813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.342094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41894</th>\n",
       "      <td>16921</td>\n",
       "      <td>1.675439</td>\n",
       "      <td>8.113229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>4.374749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41895</th>\n",
       "      <td>35665</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>29.781496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>17.174755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41896</th>\n",
       "      <td>24152</td>\n",
       "      <td>10.630660</td>\n",
       "      <td>17.882992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.018276</td>\n",
       "      <td>10.396790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41897</th>\n",
       "      <td>51374</td>\n",
       "      <td>12.952789</td>\n",
       "      <td>23.260280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.995708</td>\n",
       "      <td>13.574622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41898</th>\n",
       "      <td>43095</td>\n",
       "      <td>21.428572</td>\n",
       "      <td>62.045162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.928572</td>\n",
       "      <td>40.673725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41899</th>\n",
       "      <td>18983</td>\n",
       "      <td>203.327700</td>\n",
       "      <td>183.161480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>75.605804</td>\n",
       "      <td>115.534096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41900</th>\n",
       "      <td>32230</td>\n",
       "      <td>3.865672</td>\n",
       "      <td>4.031822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.492537</td>\n",
       "      <td>2.601414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41901</th>\n",
       "      <td>17089</td>\n",
       "      <td>2.338830</td>\n",
       "      <td>7.547891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659670</td>\n",
       "      <td>4.358683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41902</th>\n",
       "      <td>14650</td>\n",
       "      <td>12.907802</td>\n",
       "      <td>22.343832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.638298</td>\n",
       "      <td>11.310747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41903</th>\n",
       "      <td>39512</td>\n",
       "      <td>20.888890</td>\n",
       "      <td>54.609410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.022222</td>\n",
       "      <td>26.457083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41904</th>\n",
       "      <td>48600</td>\n",
       "      <td>3.254237</td>\n",
       "      <td>12.600233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.432203</td>\n",
       "      <td>6.600563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41905</th>\n",
       "      <td>15430</td>\n",
       "      <td>32.869564</td>\n",
       "      <td>113.068770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.550725</td>\n",
       "      <td>53.882460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41906</th>\n",
       "      <td>14935</td>\n",
       "      <td>43.435825</td>\n",
       "      <td>75.590485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.998589</td>\n",
       "      <td>44.560870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41907</th>\n",
       "      <td>46884</td>\n",
       "      <td>84.750610</td>\n",
       "      <td>91.961070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>32.814182</td>\n",
       "      <td>57.297016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41908</th>\n",
       "      <td>20757</td>\n",
       "      <td>38.631058</td>\n",
       "      <td>79.225110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.353116</td>\n",
       "      <td>45.701206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41909</th>\n",
       "      <td>41993</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>0.320449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040336</td>\n",
       "      <td>0.213147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41910</th>\n",
       "      <td>32103</td>\n",
       "      <td>3.865672</td>\n",
       "      <td>4.031822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.492537</td>\n",
       "      <td>2.601414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41911</th>\n",
       "      <td>30403</td>\n",
       "      <td>1.419355</td>\n",
       "      <td>6.253296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.607527</td>\n",
       "      <td>3.869059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41912</th>\n",
       "      <td>21243</td>\n",
       "      <td>42.802197</td>\n",
       "      <td>41.871918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>16.186813</td>\n",
       "      <td>23.439852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41913</th>\n",
       "      <td>45891</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.239742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.141392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41914</th>\n",
       "      <td>42613</td>\n",
       "      <td>49.442368</td>\n",
       "      <td>112.620125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.445482</td>\n",
       "      <td>62.619390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41915</th>\n",
       "      <td>43567</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.330719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.217945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41916</th>\n",
       "      <td>2732</td>\n",
       "      <td>123.869190</td>\n",
       "      <td>129.566220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>43.328970</td>\n",
       "      <td>62.774147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41917 rows × 282 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   feature_1   feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      40497   22.462366   68.340790        0.0      305.0        1.0   \n",
       "1      36089    2.886154   14.533652        0.0      131.0        0.0   \n",
       "2      16765   13.690608   21.448212        1.0      125.0        5.0   \n",
       "3       9841    6.963303    7.443890        0.0       48.0        5.0   \n",
       "4       2618  123.869190  129.566220        0.0     1065.0       87.0   \n",
       "5      16981    1.675439    8.113229        0.0       74.0        0.0   \n",
       "6       7473    7.598214    9.585806        0.0       76.0        4.0   \n",
       "7      24771   10.630660   17.882992        1.0      259.0        5.0   \n",
       "8      41950    0.090756    0.320449        0.0        2.0        0.0   \n",
       "9      23874   10.630660   17.882992        1.0      259.0        5.0   \n",
       "10     50703   11.833333   31.683943        0.0      125.0        0.0   \n",
       "11      5653  122.812930  109.961100        0.0     1069.0       89.0   \n",
       "12     28049   38.200640   75.516920        0.0      723.0       15.0   \n",
       "13     33897    0.459893    1.259083        0.0        8.0        0.0   \n",
       "14     38579    7.500000    7.599342        0.0       17.0        6.0   \n",
       "15     17139    2.338830    7.547891        0.0       82.0        0.0   \n",
       "16     35114    0.095238    0.365769        0.0        2.0        0.0   \n",
       "17     34670    4.744361   11.464004        0.0       63.0        1.0   \n",
       "18     50365    0.297521    0.751248        0.0        4.0        0.0   \n",
       "19     37019   29.901410   88.236900        0.0      434.0        1.0   \n",
       "20     39811    0.053030    0.224094        0.0        1.0        0.0   \n",
       "21     38817    3.434286   13.722764        0.0       85.0        0.0   \n",
       "22      5911  122.812930  109.961100        0.0     1069.0       89.0   \n",
       "23     51397    1.733333    3.043390        0.0        9.0        0.0   \n",
       "24     29843    2.000000    2.309401        0.0       10.0        1.0   \n",
       "25     14334    5.588235    7.664987        0.0       34.0        3.0   \n",
       "26     32334    4.468750   19.360050        0.0      101.0        0.0   \n",
       "27     10115    2.114014    6.598158        0.0       57.0        0.0   \n",
       "28      5972  122.812930  109.961100        0.0     1069.0       89.0   \n",
       "29     34721    0.000000    0.000000        0.0        0.0        0.0   \n",
       "...      ...         ...         ...        ...        ...        ...   \n",
       "41887  37619   28.979840   41.032190        0.0      267.0        9.0   \n",
       "41888   5072  122.812930  109.961100        0.0     1069.0       89.0   \n",
       "41889   2163  123.869190  129.566220        0.0     1065.0       87.0   \n",
       "41890  38804   45.230770   51.242740       10.0      243.0       29.0   \n",
       "41891   6921    7.332609   13.594126        0.0      113.0        2.0   \n",
       "41892  38984    3.434286   13.722764        0.0       85.0        0.0   \n",
       "41893  27469    0.214286    0.646813        0.0        3.0        0.0   \n",
       "41894  16921    1.675439    8.113229        0.0       74.0        0.0   \n",
       "41895  35665   11.250000   29.781496        0.0       92.0        0.0   \n",
       "41896  24152   10.630660   17.882992        1.0      259.0        5.0   \n",
       "41897  51374   12.952789   23.260280        0.0      168.0        4.0   \n",
       "41898  43095   21.428572   62.045162        0.0      319.0        2.0   \n",
       "41899  18983  203.327700  183.161480        0.0     2011.0      158.0   \n",
       "41900  32230    3.865672    4.031822        0.0       20.0        3.0   \n",
       "41901  17089    2.338830    7.547891        0.0       82.0        0.0   \n",
       "41902  14650   12.907802   22.343832        0.0      150.0        6.0   \n",
       "41903  39512   20.888890   54.609410        0.0      250.0        4.0   \n",
       "41904  48600    3.254237   12.600233        0.0       79.0        0.0   \n",
       "41905  15430   32.869564  113.068770        0.0      628.0        0.0   \n",
       "41906  14935   43.435825   75.590485        0.0      634.0       20.0   \n",
       "41907  46884   84.750610   91.961070        0.0      510.0       58.0   \n",
       "41908  20757   38.631058   79.225110        0.0      478.0        4.0   \n",
       "41909  41993    0.090756    0.320449        0.0        2.0        0.0   \n",
       "41910  32103    3.865672    4.031822        0.0       20.0        3.0   \n",
       "41911  30403    1.419355    6.253296        0.0       46.0        0.0   \n",
       "41912  21243   42.802197   41.871918        0.0      221.0       28.0   \n",
       "41913  45891    0.061224    0.239742        0.0        1.0        0.0   \n",
       "41914  42613   49.442368  112.620125        1.0      849.0        9.0   \n",
       "41915  43567    0.125000    0.330719        0.0        1.0        0.0   \n",
       "41916   2732  123.869190  129.566220        0.0     1065.0       87.0   \n",
       "\n",
       "       feature_6   feature_7  feature_8  feature_9     ...       feature_272  \\\n",
       "0       8.000000   38.088184        0.0      302.0     ...               0.0   \n",
       "1       1.147692    8.377793        0.0      112.0     ...               0.0   \n",
       "2       4.906077   12.762038        0.0      100.0     ...               0.0   \n",
       "3       2.776758    4.237846        0.0       27.0     ...               0.0   \n",
       "4      43.328970   62.774147        0.0      491.0     ...               0.0   \n",
       "5       0.622807    4.374749        0.0       65.0     ...               0.0   \n",
       "6       3.220982    5.443936        0.0       44.0     ...               0.0   \n",
       "7       4.018276   10.396790        0.0      235.0     ...               1.0   \n",
       "8       0.040336    0.213147        0.0        2.0     ...               0.0   \n",
       "9       4.018276   10.396790        0.0      235.0     ...               0.0   \n",
       "10      4.833334   16.227720        0.0       78.0     ...               0.0   \n",
       "11     44.894543   74.547530        0.0     1046.0     ...               0.0   \n",
       "12     16.194221   44.316776        0.0      547.0     ...               0.0   \n",
       "13      0.203209    0.774866        0.0        7.0     ...               0.0   \n",
       "14      2.666667    4.766783        0.0       14.0     ...               0.0   \n",
       "15      0.659670    4.358683        0.0       82.0     ...               0.0   \n",
       "16      0.035714    0.185577        0.0        1.0     ...               0.0   \n",
       "17      1.849624    7.103421        0.0       57.0     ...               0.0   \n",
       "18      0.123967    0.427761        0.0        3.0     ...               1.0   \n",
       "19     10.549295   50.174004        0.0      408.0     ...               0.0   \n",
       "20      0.022727    0.149033        0.0        1.0     ...               0.0   \n",
       "21      1.325714    6.842069        0.0       56.0     ...               0.0   \n",
       "22     44.894543   74.547530        0.0     1046.0     ...               0.0   \n",
       "23      0.733333    1.526070        0.0        5.0     ...               0.0   \n",
       "24      0.750000    1.341123        0.0        7.0     ...               0.0   \n",
       "25      2.333333    4.479437        0.0       30.0     ...               0.0   \n",
       "26      1.671875    9.448966        0.0       71.0     ...               0.0   \n",
       "27      0.859858    3.364878        0.0       44.0     ...               0.0   \n",
       "28     44.894543   74.547530        0.0     1046.0     ...               0.0   \n",
       "29      0.000000    0.000000        0.0        0.0     ...               1.0   \n",
       "...          ...         ...        ...        ...     ...               ...   \n",
       "41887  11.143146   23.392807        0.0      154.0     ...               0.0   \n",
       "41888  44.894543   74.547530        0.0     1046.0     ...               1.0   \n",
       "41889  43.328970   62.774147        0.0      491.0     ...               0.0   \n",
       "41890  17.564102   27.144245        0.0      153.0     ...               1.0   \n",
       "41891   3.143478    7.177945        0.0       76.0     ...               1.0   \n",
       "41892   1.325714    6.842069        0.0       56.0     ...               0.0   \n",
       "41893   0.089286    0.342094        0.0        2.0     ...               0.0   \n",
       "41894   0.622807    4.374749        0.0       65.0     ...               1.0   \n",
       "41895   3.833333   17.174755        0.0       86.0     ...               1.0   \n",
       "41896   4.018276   10.396790        0.0      235.0     ...               0.0   \n",
       "41897   4.995708   13.574622        0.0      142.0     ...               1.0   \n",
       "41898   9.928572   40.673725        0.0      310.0     ...               0.0   \n",
       "41899  75.605804  115.534096        0.0     1366.0     ...               0.0   \n",
       "41900   1.492537    2.601414        0.0       19.0     ...               0.0   \n",
       "41901   0.659670    4.358683        0.0       82.0     ...               1.0   \n",
       "41902   6.638298   11.310747        0.0      101.0     ...               0.0   \n",
       "41903   8.022222   26.457083        0.0      154.0     ...               0.0   \n",
       "41904   1.432203    6.600563        0.0       45.0     ...               0.0   \n",
       "41905  12.550725   53.882460        0.0      368.0     ...               0.0   \n",
       "41906  15.998589   44.560870        0.0      473.0     ...               0.0   \n",
       "41907  32.814182   57.297016        0.0      368.0     ...               0.0   \n",
       "41908  14.353116   45.701206        0.0      387.0     ...               0.0   \n",
       "41909   0.040336    0.213147        0.0        2.0     ...               0.0   \n",
       "41910   1.492537    2.601414        0.0       19.0     ...               0.0   \n",
       "41911   0.607527    3.869059        0.0       37.0     ...               1.0   \n",
       "41912  16.186813   23.439852        0.0      101.0     ...               0.0   \n",
       "41913   0.020408    0.141392        0.0        1.0     ...               0.0   \n",
       "41914  20.445482   62.619390        0.0      506.0     ...               0.0   \n",
       "41915   0.050000    0.217945        0.0        1.0     ...               0.0   \n",
       "41916  43.328970   62.774147        0.0      491.0     ...               1.0   \n",
       "\n",
       "       feature_273  feature_274  feature_275  feature_276  feature_277  \\\n",
       "0              0.0          1.0          0.0          0.0          0.0   \n",
       "1              0.0          1.0          0.0          0.0          1.0   \n",
       "2              0.0          0.0          0.0          1.0          0.0   \n",
       "3              0.0          1.0          0.0          0.0          0.0   \n",
       "4              0.0          0.0          0.0          0.0          0.0   \n",
       "5              0.0          0.0          0.0          0.0          0.0   \n",
       "6              0.0          0.0          0.0          1.0          0.0   \n",
       "7              0.0          0.0          0.0          0.0          0.0   \n",
       "8              0.0          0.0          0.0          1.0          0.0   \n",
       "9              0.0          0.0          0.0          1.0          0.0   \n",
       "10             0.0          0.0          0.0          1.0          0.0   \n",
       "11             0.0          0.0          0.0          0.0          0.0   \n",
       "12             0.0          0.0          0.0          0.0          0.0   \n",
       "13             1.0          0.0          0.0          0.0          0.0   \n",
       "14             0.0          1.0          0.0          0.0          0.0   \n",
       "15             0.0          1.0          0.0          0.0          0.0   \n",
       "16             0.0          0.0          0.0          1.0          0.0   \n",
       "17             0.0          0.0          0.0          0.0          0.0   \n",
       "18             0.0          0.0          0.0          0.0          0.0   \n",
       "19             0.0          0.0          1.0          0.0          0.0   \n",
       "20             0.0          0.0          0.0          0.0          0.0   \n",
       "21             1.0          0.0          0.0          0.0          0.0   \n",
       "22             0.0          1.0          0.0          0.0          0.0   \n",
       "23             0.0          1.0          0.0          0.0          0.0   \n",
       "24             1.0          0.0          0.0          0.0          0.0   \n",
       "25             0.0          0.0          0.0          0.0          0.0   \n",
       "26             0.0          1.0          0.0          0.0          0.0   \n",
       "27             0.0          0.0          0.0          0.0          0.0   \n",
       "28             0.0          0.0          0.0          0.0          0.0   \n",
       "29             0.0          0.0          0.0          0.0          0.0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "41887          0.0          1.0          0.0          0.0          0.0   \n",
       "41888          0.0          0.0          0.0          0.0          0.0   \n",
       "41889          1.0          0.0          0.0          0.0          0.0   \n",
       "41890          0.0          0.0          0.0          0.0          0.0   \n",
       "41891          0.0          0.0          0.0          0.0          0.0   \n",
       "41892          1.0          0.0          0.0          0.0          0.0   \n",
       "41893          0.0          0.0          0.0          0.0          0.0   \n",
       "41894          0.0          0.0          0.0          0.0          0.0   \n",
       "41895          0.0          0.0          0.0          0.0          0.0   \n",
       "41896          0.0          0.0          0.0          0.0          0.0   \n",
       "41897          0.0          0.0          0.0          0.0          0.0   \n",
       "41898          0.0          1.0          0.0          0.0          0.0   \n",
       "41899          0.0          0.0          0.0          0.0          0.0   \n",
       "41900          0.0          1.0          0.0          0.0          0.0   \n",
       "41901          0.0          0.0          0.0          0.0          0.0   \n",
       "41902          1.0          0.0          0.0          0.0          0.0   \n",
       "41903          0.0          0.0          0.0          0.0          0.0   \n",
       "41904          0.0          0.0          1.0          0.0          0.0   \n",
       "41905          0.0          0.0          0.0          0.0          0.0   \n",
       "41906          1.0          0.0          0.0          0.0          0.0   \n",
       "41907          0.0          0.0          1.0          0.0          0.0   \n",
       "41908          0.0          0.0          0.0          1.0          0.0   \n",
       "41909          0.0          0.0          0.0          1.0          0.0   \n",
       "41910          0.0          1.0          0.0          0.0          0.0   \n",
       "41911          0.0          0.0          0.0          0.0          0.0   \n",
       "41912          1.0          0.0          0.0          0.0          0.0   \n",
       "41913          0.0          1.0          0.0          0.0          0.0   \n",
       "41914          1.0          0.0          0.0          0.0          0.0   \n",
       "41915          0.0          0.0          0.0          0.0          0.0   \n",
       "41916          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "       feature_278  feature_279  feature_280  feature_281  \n",
       "0              0.0          0.0          0.0          0.0  \n",
       "1              0.0          0.0          0.0          0.0  \n",
       "2              0.0          0.0          0.0          1.0  \n",
       "3              0.0          0.0          0.0          4.0  \n",
       "4              0.0          0.0          0.0          3.0  \n",
       "5              0.0          0.0          0.0          0.0  \n",
       "6              0.0          0.0          0.0          1.0  \n",
       "7              0.0          0.0          0.0          0.0  \n",
       "8              0.0          0.0          0.0          0.0  \n",
       "9              0.0          0.0          0.0          3.0  \n",
       "10             0.0          0.0          0.0          0.0  \n",
       "11             0.0          0.0          0.0          2.0  \n",
       "12             0.0          0.0          0.0          2.0  \n",
       "13             0.0          0.0          0.0          0.0  \n",
       "14             0.0          0.0          0.0          0.0  \n",
       "15             0.0          0.0          0.0          0.0  \n",
       "16             0.0          0.0          0.0          0.0  \n",
       "17             0.0          0.0          0.0          0.0  \n",
       "18             0.0          0.0          0.0          0.0  \n",
       "19             0.0          0.0          0.0         26.0  \n",
       "20             0.0          0.0          0.0          0.0  \n",
       "21             0.0          0.0          0.0          0.0  \n",
       "22             0.0          0.0          0.0         18.0  \n",
       "23             0.0          0.0          0.0          0.0  \n",
       "24             0.0          0.0          0.0          3.0  \n",
       "25             0.0          0.0          0.0          0.0  \n",
       "26             0.0          0.0          0.0          1.0  \n",
       "27             0.0          0.0          0.0          0.0  \n",
       "28             0.0          0.0          0.0         48.0  \n",
       "29             0.0          0.0          0.0          0.0  \n",
       "...            ...          ...          ...          ...  \n",
       "41887          0.0          0.0          0.0          6.0  \n",
       "41888          0.0          0.0          0.0          1.0  \n",
       "41889          0.0          0.0          0.0          6.0  \n",
       "41890          0.0          0.0          0.0          0.0  \n",
       "41891          0.0          0.0          0.0          2.0  \n",
       "41892          0.0          0.0          0.0          0.0  \n",
       "41893          0.0          0.0          0.0          0.0  \n",
       "41894          0.0          0.0          0.0          0.0  \n",
       "41895          0.0          0.0          0.0          0.0  \n",
       "41896          0.0          0.0          0.0          0.0  \n",
       "41897          0.0          0.0          0.0          0.0  \n",
       "41898          0.0          0.0          0.0          0.0  \n",
       "41899          0.0          0.0          0.0         29.0  \n",
       "41900          0.0          0.0          0.0          0.0  \n",
       "41901          0.0          0.0          0.0          0.0  \n",
       "41902          0.0          0.0          0.0          2.0  \n",
       "41903          0.0          0.0          0.0          0.0  \n",
       "41904          0.0          0.0          0.0          0.0  \n",
       "41905          0.0          0.0          0.0          0.0  \n",
       "41906          0.0          0.0          0.0          0.0  \n",
       "41907          0.0          0.0          0.0          3.0  \n",
       "41908          0.0          0.0          0.0         63.0  \n",
       "41909          0.0          0.0          0.0          0.0  \n",
       "41910          0.0          0.0          0.0          0.0  \n",
       "41911          0.0          0.0          0.0          0.0  \n",
       "41912          0.0          0.0          0.0          0.0  \n",
       "41913          0.0          0.0          0.0          0.0  \n",
       "41914          0.0          0.0          0.0          0.0  \n",
       "41915          0.0          0.0          0.0          0.0  \n",
       "41916          0.0          0.0          0.0          1.0  \n",
       "\n",
       "[41917 rows x 282 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-borrarid'></a>\n",
    "### 1.- Borramos el ID y separamos la variable dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**(N,M) en entrenamiento**: ( 41917, 280 )"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# borramos el ID \n",
    "data = data.drop('Id', axis=1)\n",
    "dependiente = ['feature_281']\n",
    "var_names = [x for x in data.columns if x not in dependiente]\n",
    "X = data[var_names]\n",
    "y = data[dependiente]\n",
    "#print('(N,M) en entrenamiento: (' + str(len(X)) +',' + str(len(var_names))+')')\n",
    "display(Markdown( '**(N,M) en entrenamiento**: ( ' + str(len(X)) +',' + ' '+str(len(var_names))+' )' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41917 entries, 0 to 41916\n",
      "Columns: 281 entries, feature_1 to feature_281\n",
      "dtypes: float64(281)\n",
      "memory usage: 89.9 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-dividedatos'></a>\n",
    "### 2.- Se dividen los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División datos:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-constantes'></a>\n",
    "### 3.- Se eliminan variables constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Cantidad de variables eliminadas por ser constantes**: 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Variables eliminadas*: feature_13 / feature_33 / feature_38 / feature_278"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constantes = []\n",
    "for var in X[var_names]:\n",
    "    cant_unique = len(X_train[var].unique())\n",
    "    if cant_unique == 1:\n",
    "        constantes.append(var)\n",
    "#print('Cantidad de variables eliminadas por ser constantes: %.0f' %len(constantes))\n",
    "variables_sin_constantes = [x for x in X[var_names] if x not in constantes]\n",
    "\n",
    "display(Markdown( '**Cantidad de variables eliminadas por ser constantes**: %.0f' %len(constantes) ))\n",
    "if len(constantes) != 0:\n",
    "    display(Markdown( '*Variables eliminadas*: '+ ' / '.join(constantes) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-repeticiones'></a>\n",
    "### 4.- Se eliminan variables con repeticiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Cantidad de variables eliminadas por repetición**: 27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Variables eliminadas*: feature_18 / feature_43 / feature_73 / feature_76 / feature_80 / feature_81 / feature_94 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_76 / feature_80 / feature_81 / feature_94 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_80 / feature_81 / feature_94 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_81 / feature_94 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_94 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_97 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_124 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_130 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_132 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_149 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_155 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_156 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_161 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_166 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_169 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_172 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_179 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_190 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_198 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_200 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_212 / feature_223 / feature_236 / feature_243 / feature_250 / feature_223 / feature_236 / feature_243 / feature_250 / feature_236 / feature_243 / feature_250 / feature_243 / feature_250 / feature_250"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "equal_features = []\n",
    "for compare in list(combinations(variables_sin_constantes,2)): #Variables filtradas sin constante.\n",
    "    is_equal = (X_train[compare[0]]==X_train[compare[1]]).all()\n",
    "    if is_equal:\n",
    "            equal_features.append(compare[1])\n",
    "# print('Cantidad de variables eliminadas por repetición: %.0f' %len(set(equal_features))) \n",
    "variables_sin_repeticiones = [x for x in variables_sin_constantes if x not in equal_features]\n",
    "\n",
    "display(Markdown( '**Cantidad de variables eliminadas por repetición**: %.0f' %len(set(equal_features)) ))\n",
    "if len(set(equal_features)) != 0:\n",
    "    display(Markdown( '*Variables eliminadas*: '+ ' / '.join(equal_features) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-nulas'></a>\n",
    "### 5.- Se eliminan varibles nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Cantidad de variables eliminadas por nulas**: 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nulos = []\n",
    "for var in variables_sin_repeticiones:\n",
    "    if X_train[var].isnull().sum() / len(X) > 0.045:\n",
    "        nulos.append(var)\n",
    "#print('Cantidad de variables eliminadas por nulas: %.0f' %len(nulos)) \n",
    "variables_sin_nulos = [x for x in variables_sin_repeticiones if x not in nulos]\n",
    "\n",
    "display(Markdown( '**Cantidad de variables eliminadas por nulas**: %.0f' %len(nulos) ))\n",
    "if len(nulos) != 0:\n",
    "    display(Markdown( '*Variables eliminadas*: '+ ' / '.join(equal_features) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-correlacionadas'></a>\n",
    "### 6.- Se eliminan variables muy correlacionadas\n",
    "Acá tengo dudas, porque esto se podría mejorar viendo lo de la última clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Cantidad de variables eliminadas por estar correlacionadas**: 41"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Variables eliminadas*: feature_39 / feature_32 / feature_7 / feature_31 / feature_26 / feature_1 / feature_24 / feature_9 / feature_12 / feature_11 / feature_20 / feature_49 / feature_42 / feature_37 / feature_16 / feature_41 / feature_44 / feature_48 / feature_2 / feature_17 / feature_27 / feature_56 / feature_15 / feature_5 / feature_19 / feature_51 / feature_29 / feature_23 / feature_47 / feature_22 / feature_14 / feature_45 / feature_30 / feature_6 / feature_21 / feature_4 / feature_35 / feature_280 / feature_65 / feature_34 / feature_143"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation = X_train.corr()\n",
    "correlaciones = pd.DataFrame(columns=['v1','v2','c1','c2','c'])\n",
    "\n",
    "for i in range(len(correlation.index)):\n",
    "    for j in range(len(correlation.index)):\n",
    "        if j < i:\n",
    "            if np.abs(correlation.iloc[i,j]) > 0.8:\n",
    "                cor_i = np.corrcoef(X_train[correlation.index[i]].values,y_train.values.ravel())[0,1]        \n",
    "                cor_j = np.corrcoef(X_train[correlation.index[j]].values,y_train.values.ravel())[0,1]  \n",
    "                if np.abs(cor_i) > np.abs(cor_j):\n",
    "                    correlaciones.loc[len(correlaciones)]= [correlation.index[i], correlation.index[j], cor_i, cor_j, np.abs(correlation.iloc[i,j])]\n",
    "                else:\n",
    "                    correlaciones.loc[len(correlaciones)]= [correlation.index[j], correlation.index[i], cor_j, cor_i, np.abs(correlation.iloc[i,j])]\n",
    "        else:\n",
    "            break\n",
    "correlaciones = correlaciones.sort_values(by=['c'], ascending=False)\n",
    "#print('Cantidad de variables eliminadas por estar correlacionadas: %.0f' %len(correlaciones['v2'].unique()))\n",
    "variables_sin_corr = [x for x in variables_sin_nulos if x not in correlaciones['v2'].unique()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown( '**Cantidad de variables eliminadas por estar correlacionadas**: %.0f' %(len(variables_sin_nulos) - len(variables_sin_corr)) ))\n",
    "if (len(variables_sin_nulos) - len(variables_sin_corr)) != 0:\n",
    "    display(Markdown( '*Variables eliminadas*: '+ ' / '.join([x for x in correlaciones['v2'].unique() if x in variables_sin_nulos]) ))\n",
    "    \n",
    "# Principalmente pueden corresponder a palabras que siempre van juntas, como artículos u otros\n",
    "# Del 63 al 262 son palabra smás repetidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7-bajacontribucion'></a>\n",
    "### 7.- Se eliminan variables con baja contruibucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Variables eliminadas por baja contribución**: 181"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Variables eliminadas*: feature_25 / feature_28 / feature_40 / feature_50 / feature_62 / feature_63 / feature_64 / feature_66 / feature_68 / feature_70 / feature_71 / feature_72 / feature_74 / feature_75 / feature_77 / feature_78 / feature_79 / feature_82 / feature_83 / feature_84 / feature_85 / feature_86 / feature_87 / feature_88 / feature_89 / feature_90 / feature_91 / feature_92 / feature_93 / feature_95 / feature_96 / feature_98 / feature_99 / feature_100 / feature_103 / feature_104 / feature_105 / feature_106 / feature_107 / feature_108 / feature_109 / feature_110 / feature_111 / feature_112 / feature_113 / feature_114 / feature_115 / feature_116 / feature_117 / feature_118 / feature_119 / feature_120 / feature_121 / feature_122 / feature_123 / feature_125 / feature_126 / feature_127 / feature_128 / feature_129 / feature_131 / feature_133 / feature_134 / feature_135 / feature_136 / feature_137 / feature_138 / feature_140 / feature_141 / feature_142 / feature_144 / feature_145 / feature_146 / feature_147 / feature_148 / feature_150 / feature_151 / feature_152 / feature_153 / feature_157 / feature_159 / feature_160 / feature_162 / feature_163 / feature_164 / feature_165 / feature_167 / feature_168 / feature_170 / feature_171 / feature_173 / feature_174 / feature_175 / feature_176 / feature_177 / feature_178 / feature_180 / feature_181 / feature_182 / feature_183 / feature_184 / feature_185 / feature_186 / feature_187 / feature_188 / feature_189 / feature_191 / feature_192 / feature_193 / feature_195 / feature_196 / feature_197 / feature_199 / feature_201 / feature_202 / feature_203 / feature_204 / feature_205 / feature_206 / feature_207 / feature_208 / feature_209 / feature_211 / feature_214 / feature_215 / feature_216 / feature_217 / feature_218 / feature_219 / feature_220 / feature_221 / feature_222 / feature_224 / feature_225 / feature_226 / feature_227 / feature_228 / feature_229 / feature_230 / feature_231 / feature_233 / feature_234 / feature_235 / feature_237 / feature_238 / feature_239 / feature_240 / feature_241 / feature_242 / feature_244 / feature_245 / feature_247 / feature_249 / feature_251 / feature_252 / feature_253 / feature_254 / feature_255 / feature_256 / feature_257 / feature_258 / feature_259 / feature_260 / feature_261 / feature_262 / feature_263 / feature_264 / feature_265 / feature_266 / feature_267 / feature_268 / feature_269 / feature_270 / feature_271 / feature_272 / feature_273 / feature_274 / feature_275 / feature_276 / feature_277 / feature_279"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baja_contribucion = []\n",
    "for var in variables_sin_corr:\n",
    "    if np.abs(np.corrcoef(X_train[var].values,y_train.values.ravel())[0,1])  < 0.05:\n",
    "        baja_contribucion.append(var)\n",
    "# print('Variables eliminadas por baja contribución: %.0f' %len(baja_contribucion))\n",
    "variables_sin_bajacont = [x for x in variables_sin_corr if x not in baja_contribucion]\n",
    "\n",
    "display(Markdown( '**Variables eliminadas por baja contribución**: %.0f' %len(baja_contribucion) ))\n",
    "if len(baja_contribucion) != 0:\n",
    "    display(Markdown( '*Variables eliminadas*: '+ ' / '.join(baja_contribucion) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_3',\n",
       " 'feature_8',\n",
       " 'feature_10',\n",
       " 'feature_36',\n",
       " 'feature_46',\n",
       " 'feature_52',\n",
       " 'feature_53',\n",
       " 'feature_54',\n",
       " 'feature_55',\n",
       " 'feature_57',\n",
       " 'feature_58',\n",
       " 'feature_59',\n",
       " 'feature_60',\n",
       " 'feature_61',\n",
       " 'feature_67',\n",
       " 'feature_69',\n",
       " 'feature_101',\n",
       " 'feature_102',\n",
       " 'feature_139',\n",
       " 'feature_154',\n",
       " 'feature_158',\n",
       " 'feature_194',\n",
       " 'feature_210',\n",
       " 'feature_213',\n",
       " 'feature_232',\n",
       " 'feature_246',\n",
       " 'feature_248']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_sin_bajacont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_train[baja_contribucion[0]], y_train, marker='^', c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<a id='8-resumen'></a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 8.- Resumen:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**INICIALES**: 41917"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Eliminadas por ser constantes*: 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Eliminadas por repetición*: 27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Eliminadas por nulas*: 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Eliminadas por estar correlacionadas*: 41"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*Eliminadas por baja contribución*: 181"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**FINALES**: 27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print('******* RESUMEN:')\n",
    "# print('* INICIALES: %.0f' %len(X[var_names]))\n",
    "# print('Eliminadas por ser constantes: %.0f' %len(constantes))\n",
    "# print('Eliminadas por repetición: %.0f' %len(set(equal_features))) \n",
    "# print('Eliminadas por nulas: %.0f' %len(nulos)) \n",
    "# print('Eliminadas por estar correlacionadas: %.0f' %len(correlaciones['v2'].unique())) \n",
    "# print('Eliminadas por baja contribución: %.0f' %len(baja_contribucion))\n",
    "# print('* FINALES: %.0f' %len(variables_sin_bajacont))\n",
    "\n",
    "display(Markdown( \"<a id='8-resumen'></a>\" ))\n",
    "display(Markdown('### 8.- Resumen:'))\n",
    "display(Markdown('**INICIALES**: %.0f' %len(X[var_names])))\n",
    "display(Markdown('*Eliminadas por ser constantes*: %.0f' %len(constantes)))\n",
    "display(Markdown('*Eliminadas por repetición*: %.0f' %len(set(equal_features))) )\n",
    "display(Markdown('*Eliminadas por nulas*: %.0f' %len(nulos)) )\n",
    "display(Markdown('*Eliminadas por estar correlacionadas*: %.0f' %(len(variables_sin_nulos) - len(variables_sin_corr)) ) )\n",
    "display(Markdown('*Eliminadas por baja contribución*: %.0f' %len(baja_contribucion)))\n",
    "display(Markdown('**FINALES**: %.0f' %len(variables_sin_bajacont)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='normalizacion'></a>\n",
    "## Normalizacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "X_train = X_train[variables_sin_bajacont]\n",
    "X_test = X_test[variables_sin_bajacont]\n",
    "norm_mean = preprocessing.StandardScaler().fit(X_train)\n",
    "X_norm = pd.DataFrame(norm_mean.transform(X_train), \n",
    "                            columns = variables_sin_bajacont)\n",
    "X_norm_test = pd.DataFrame(norm_mean.transform(X_test), \n",
    "                           columns = variables_sin_bajacont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_281</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41917.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.866116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.385340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1424.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_281\n",
       "count  41917.000000\n",
       "mean       6.866116\n",
       "std       38.385340\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        2.000000\n",
       "max     1424.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist =  y_train.hist(bins=50)\n",
    "plt.show()\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regresion'></a>\n",
    "### REGRESION LINEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**L1_Ratio**: 0.8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Penalty Alpha**: 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr =  ElasticNet(random_state=0)\n",
    "params ={'l1_ratio':[0,0.2,0.5,0.8,1], 'alpha':[0,0.1,1]}\n",
    "# Verbose para saber en qué está el proceso, menos incertidumbre\n",
    "# reg = GridSearchCV(lr, params, scoring = 'neg_median_absolute_error', cv = 10,  verbose=2)\n",
    "reg = GridSearchCV(lr, params, scoring = 'neg_median_absolute_error', cv = 10,  verbose=0)\n",
    "reg.fit(X_norm, y_train)\n",
    "\n",
    "display(Markdown('**L1_Ratio**: ' +str(reg.best_params_['l1_ratio'])))\n",
    "display(Markdown('**Penalty Alpha**: ' +str(reg.best_params_['alpha'])))\n",
    "\n",
    "#print('L1_Ratio: ' +str(reg.best_params_['l1_ratio']))\n",
    "#print('Penalty Alpha: ' +str(reg.best_params_['alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SE GUARDA EL MODELO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**MAE en entrenamiento**: 8.41 *(69.81243658065796 seconds)*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Guardamos modelo serializado:\n",
    "lr =  ElasticNet(l1_ratio = reg.best_params_['l1_ratio'], alpha = reg.best_params_['alpha'], random_state=0)\n",
    "lr.fit(X_norm, y_train)\n",
    "\n",
    "filename = 'rl.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))\n",
    "\n",
    "#print('MAE en entrenamiento: %.2f' %mean_absolute_error(lr.predict(X_norm),y_train))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "display(Markdown('**MAE en entrenamiento**: %.2f' %mean_absolute_error(lr.predict(X_norm),y_train)+ \" *(%s seconds)*\" % (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arbol'></a>\n",
    "### ARBOL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Profundidad**: 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Mínimo muestra en hoja**: 0.01"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Maxima cantidad de hojas finales**: 20"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt =  DecisionTreeRegressor(random_state=0)\n",
    "#params ={'max_depth':[5,10,20,40,80], 'min_samples_leaf':[0.01,0.05,0.1,0.2] , 'criterion': [\"mae\", \"mse\"]}\n",
    "params ={'max_depth':[5,10,20,40,80], 'min_samples_leaf':[0.01,0.05,0.1,0.2], \"max_leaf_nodes\": [5, 20, 100] }\n",
    "#reg = GridSearchCV(dt, params, scoring = 'neg_median_absolute_error', cv = 20, verbose = 2)\n",
    "reg = GridSearchCV(dt, params, scoring = 'neg_median_absolute_error', cv = 20)\n",
    "reg.fit(X_train, y_train) # Se hace sobre datos normales, apropiado para dt.\n",
    "\n",
    "#print('Profundidad: ' +str(reg.best_params_['max_depth']))\n",
    "#print('Mínimo muestra en hoja: ' +str(reg.best_params_['min_samples_leaf']))\n",
    "\n",
    "display(Markdown( '**Profundidad**: ' +str(reg.best_params_['max_depth']) ))\n",
    "display(Markdown( '**Mínimo muestra en hoja**: ' +str(reg.best_params_['min_samples_leaf']) ))\n",
    "display(Markdown( '**Maxima cantidad de hojas finales**: ' +str(reg.best_params_['max_leaf_nodes']) ))\n",
    "#display(Markdown( '**Criterion**: ' +str(reg.best_params_['criterion']) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**MAE en entrenamiento**: 7.33 *(214.26834321022034 seconds)*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Modelo serializado:\n",
    "dt = DecisionTreeRegressor(random_state=0, max_depth=reg.best_params_['max_depth'],\n",
    "                               min_samples_leaf=reg.best_params_['min_samples_leaf'], max_leaf_nodes=reg.best_params_['max_leaf_nodes'])\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "filename = 'dt.sav'\n",
    "pickle.dump(dt, open(filename, 'wb'))\n",
    "\n",
    "#print('MAE en entrenamiento: %.2f' %mean_absolute_error(dt.predict(X_train),y_train))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "display(Markdown('**MAE en entrenamiento**: %.2f' %mean_absolute_error(dt.predict(X_train),y_train)+ \" *(%s seconds)*\" % (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.initializers import he_normal \n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def Red_Neuronal(optimizer = 'Adam', activation='elu' ):\n",
    "    keras.backend.clear_session()\n",
    "    nn = Sequential()\n",
    "\n",
    "    #Capa 1:\n",
    "    nn.add(Dense(len(variables_sin_bajacont), input_dim=len(variables_sin_bajacont), kernel_initializer=he_normal()))\n",
    "    nn.add(Activation(activation))\n",
    "    nn.add(Dropout(0.3))\n",
    "\n",
    "    #Capa 2:\n",
    "    nn.add(Dense(len(variables_sin_bajacont), kernel_initializer=he_normal()))\n",
    "    nn.add(Activation(activation))\n",
    "    nn.add(Dropout(0.1))\n",
    "\n",
    "    #Capa 2:\n",
    "    nn.add(Dense(len(variables_sin_bajacont), kernel_initializer=he_normal()))\n",
    "    nn.add(Activation(activation))\n",
    "    nn.add(Dropout(0.1))\n",
    "\n",
    "    #Capa 4 - Final:\n",
    "    nn.add(Dense(1))\n",
    "    nn.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=[\"mae\", \"accuracy\"])\n",
    "    nn.summary()\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREAMOS RED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "nn1 = KerasClassifier(build_fn=Red_Neuronal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se determinan los hyperparametros para la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping y checkpoint para guardar la mejor configuación\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "stopped = EarlyStopping(monitor='val_loss', verbose=1, patience=50,min_delta=1e-7, mode='min', baseline=0.18,restore_best_weights=False )\n",
    "best_model = ModelCheckpoint(filepath='Best_NN.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16766/16766 [==============================] - 1s 56us/step - loss: 7.5221 - mean_absolute_error: 7.5221 - acc: 0.3386 - val_loss: 7.0830 - val_mean_absolute_error: 7.0830 - val_acc: 0.6116\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 5.0094 - mean_absolute_error: 5.0094 - acc: 0.5709 - val_loss: 7.0297 - val_mean_absolute_error: 7.0297 - val_acc: 0.5415\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.6243 - mean_absolute_error: 4.6243 - acc: 0.5981 - val_loss: 7.0252 - val_mean_absolute_error: 7.0252 - val_acc: 0.5583\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.5044 - mean_absolute_error: 4.5044 - acc: 0.6125 - val_loss: 6.9690 - val_mean_absolute_error: 6.9690 - val_acc: 0.5991\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.5325 - mean_absolute_error: 4.5325 - acc: 0.6073 - val_loss: 6.9398 - val_mean_absolute_error: 6.9398 - val_acc: 0.5596\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.5350 - mean_absolute_error: 4.5350 - acc: 0.6124 - val_loss: 6.9303 - val_mean_absolute_error: 6.9303 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.5004 - mean_absolute_error: 4.5004 - acc: 0.6094 - val_loss: 6.9110 - val_mean_absolute_error: 6.9110 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.4467 - mean_absolute_error: 4.4467 - acc: 0.6108 - val_loss: 6.9271 - val_mean_absolute_error: 6.9271 - val_acc: 0.6087\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.4428 - mean_absolute_error: 4.4428 - acc: 0.6103 - val_loss: 6.9004 - val_mean_absolute_error: 6.9004 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3619 - mean_absolute_error: 4.3619 - acc: 0.6121 - val_loss: 6.8778 - val_mean_absolute_error: 6.8778 - val_acc: 0.6072\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.4110 - mean_absolute_error: 4.4110 - acc: 0.6115 - val_loss: 6.8690 - val_mean_absolute_error: 6.8690 - val_acc: 0.6132\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3406 - mean_absolute_error: 4.3406 - acc: 0.6133 - val_loss: 6.8722 - val_mean_absolute_error: 6.8722 - val_acc: 0.6155\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/150\n",
      "16766/16766 [==============================] - 1s 43us/step - loss: 4.3511 - mean_absolute_error: 4.3511 - acc: 0.6126 - val_loss: 6.8443 - val_mean_absolute_error: 6.8443 - val_acc: 0.6181\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3583 - mean_absolute_error: 4.3583 - acc: 0.6127 - val_loss: 6.8613 - val_mean_absolute_error: 6.8613 - val_acc: 0.6141\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.3319 - mean_absolute_error: 4.3319 - acc: 0.6125 - val_loss: 6.8448 - val_mean_absolute_error: 6.8448 - val_acc: 0.6205\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2754 - mean_absolute_error: 4.2754 - acc: 0.6135 - val_loss: 6.8838 - val_mean_absolute_error: 6.8838 - val_acc: 0.6244\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.3338 - mean_absolute_error: 4.3338 - acc: 0.6146 - val_loss: 6.9082 - val_mean_absolute_error: 6.9082 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3218 - mean_absolute_error: 4.3218 - acc: 0.6157 - val_loss: 6.9175 - val_mean_absolute_error: 6.9175 - val_acc: 0.6232\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.3772 - mean_absolute_error: 4.3772 - acc: 0.6149 - val_loss: 6.9034 - val_mean_absolute_error: 6.9034 - val_acc: 0.6229\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.2622 - mean_absolute_error: 4.2622 - acc: 0.6151 - val_loss: 6.9560 - val_mean_absolute_error: 6.9560 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3575 - mean_absolute_error: 4.3575 - acc: 0.6204 - val_loss: 6.9112 - val_mean_absolute_error: 6.9112 - val_acc: 0.6237\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2816 - mean_absolute_error: 4.2816 - acc: 0.6164 - val_loss: 6.8598 - val_mean_absolute_error: 6.8598 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2924 - mean_absolute_error: 4.2924 - acc: 0.6154 - val_loss: 6.8940 - val_mean_absolute_error: 6.8940 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3233 - mean_absolute_error: 4.3233 - acc: 0.6147 - val_loss: 6.8967 - val_mean_absolute_error: 6.8967 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3017 - mean_absolute_error: 4.3017 - acc: 0.6155 - val_loss: 6.9144 - val_mean_absolute_error: 6.9144 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2979 - mean_absolute_error: 4.2979 - acc: 0.6167 - val_loss: 6.8892 - val_mean_absolute_error: 6.8892 - val_acc: 0.6256\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2182 - mean_absolute_error: 4.2182 - acc: 0.6198 - val_loss: 6.9050 - val_mean_absolute_error: 6.9050 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2868 - mean_absolute_error: 4.2868 - acc: 0.6201 - val_loss: 6.8885 - val_mean_absolute_error: 6.8885 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2371 - mean_absolute_error: 4.2371 - acc: 0.6212 - val_loss: 6.8843 - val_mean_absolute_error: 6.8843 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3137 - mean_absolute_error: 4.3137 - acc: 0.6208 - val_loss: 6.9097 - val_mean_absolute_error: 6.9097 - val_acc: 0.6249\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2486 - mean_absolute_error: 4.2486 - acc: 0.6191 - val_loss: 6.8423 - val_mean_absolute_error: 6.8423 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2348 - mean_absolute_error: 4.2348 - acc: 0.6218 - val_loss: 6.8931 - val_mean_absolute_error: 6.8931 - val_acc: 0.6233\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2435 - mean_absolute_error: 4.2435 - acc: 0.6237 - val_loss: 6.8872 - val_mean_absolute_error: 6.8872 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2630 - mean_absolute_error: 4.2630 - acc: 0.6195 - val_loss: 6.9029 - val_mean_absolute_error: 6.9029 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2183 - mean_absolute_error: 4.2183 - acc: 0.6184 - val_loss: 6.8999 - val_mean_absolute_error: 6.8999 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2126 - mean_absolute_error: 4.2126 - acc: 0.6240 - val_loss: 6.8990 - val_mean_absolute_error: 6.8990 - val_acc: 0.6234\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2294 - mean_absolute_error: 4.2294 - acc: 0.6227 - val_loss: 6.8568 - val_mean_absolute_error: 6.8568 - val_acc: 0.6227\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2439 - mean_absolute_error: 4.2439 - acc: 0.6184 - val_loss: 6.8611 - val_mean_absolute_error: 6.8611 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2073 - mean_absolute_error: 4.2073 - acc: 0.6193 - val_loss: 6.9152 - val_mean_absolute_error: 6.9152 - val_acc: 0.6244\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.1915 - mean_absolute_error: 4.1915 - acc: 0.6164 - val_loss: 6.9167 - val_mean_absolute_error: 6.9167 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/150\n",
      "16766/16766 [==============================] - 0s 30us/step - loss: 4.1461 - mean_absolute_error: 4.1461 - acc: 0.6131 - val_loss: 6.9030 - val_mean_absolute_error: 6.9030 - val_acc: 0.6236\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2019 - mean_absolute_error: 4.2019 - acc: 0.6177 - val_loss: 6.8872 - val_mean_absolute_error: 6.8872 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1413 - mean_absolute_error: 4.1413 - acc: 0.6210 - val_loss: 6.9260 - val_mean_absolute_error: 6.9260 - val_acc: 0.6267\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2275 - mean_absolute_error: 4.2275 - acc: 0.6217 - val_loss: 6.8832 - val_mean_absolute_error: 6.8832 - val_acc: 0.6245\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1689 - mean_absolute_error: 4.1689 - acc: 0.6194 - val_loss: 6.9008 - val_mean_absolute_error: 6.9008 - val_acc: 0.6244\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1476 - mean_absolute_error: 4.1476 - acc: 0.6202 - val_loss: 6.8959 - val_mean_absolute_error: 6.8959 - val_acc: 0.6230\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1968 - mean_absolute_error: 4.1968 - acc: 0.6152 - val_loss: 6.9099 - val_mean_absolute_error: 6.9099 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1513 - mean_absolute_error: 4.1513 - acc: 0.6173 - val_loss: 6.9203 - val_mean_absolute_error: 6.9203 - val_acc: 0.6255\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1182 - mean_absolute_error: 4.1182 - acc: 0.6183 - val_loss: 6.9219 - val_mean_absolute_error: 6.9219 - val_acc: 0.6275\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2113 - mean_absolute_error: 4.2113 - acc: 0.6177 - val_loss: 6.8675 - val_mean_absolute_error: 6.8675 - val_acc: 0.6254\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 10us/step\n",
      "16766/16766 [==============================] - 0s 9us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 50us/step - loss: 6.1075 - mean_absolute_error: 6.1075 - acc: 0.4621 - val_loss: 7.0356 - val_mean_absolute_error: 7.0356 - val_acc: 0.5782\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.7952 - mean_absolute_error: 4.7952 - acc: 0.6071 - val_loss: 7.0150 - val_mean_absolute_error: 7.0150 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.7740 - mean_absolute_error: 4.7740 - acc: 0.6079 - val_loss: 6.9888 - val_mean_absolute_error: 6.9888 - val_acc: 0.4597\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.6394 - mean_absolute_error: 4.6394 - acc: 0.6121 - val_loss: 6.9384 - val_mean_absolute_error: 6.9384 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.5960 - mean_absolute_error: 4.5960 - acc: 0.6130 - val_loss: 6.8861 - val_mean_absolute_error: 6.8861 - val_acc: 0.5860\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5795 - mean_absolute_error: 4.5795 - acc: 0.6156 - val_loss: 6.8722 - val_mean_absolute_error: 6.8722 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5144 - mean_absolute_error: 4.5144 - acc: 0.6170 - val_loss: 6.9199 - val_mean_absolute_error: 6.9199 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4596 - mean_absolute_error: 4.4596 - acc: 0.6174 - val_loss: 6.9393 - val_mean_absolute_error: 6.9393 - val_acc: 0.6186\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4487 - mean_absolute_error: 4.4487 - acc: 0.6151 - val_loss: 6.9333 - val_mean_absolute_error: 6.9333 - val_acc: 0.6162\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4098 - mean_absolute_error: 4.4098 - acc: 0.6095 - val_loss: 6.8777 - val_mean_absolute_error: 6.8777 - val_acc: 0.6134\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4236 - mean_absolute_error: 4.4236 - acc: 0.6175 - val_loss: 6.8777 - val_mean_absolute_error: 6.8777 - val_acc: 0.6065\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4695 - mean_absolute_error: 4.4695 - acc: 0.6156 - val_loss: 6.8761 - val_mean_absolute_error: 6.8761 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4257 - mean_absolute_error: 4.4257 - acc: 0.6119 - val_loss: 6.9121 - val_mean_absolute_error: 6.9121 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4650 - mean_absolute_error: 4.4650 - acc: 0.6174 - val_loss: 6.9130 - val_mean_absolute_error: 6.9130 - val_acc: 0.6114\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3917 - mean_absolute_error: 4.3917 - acc: 0.6207 - val_loss: 6.9108 - val_mean_absolute_error: 6.9108 - val_acc: 0.6113\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4152 - mean_absolute_error: 4.4152 - acc: 0.6204 - val_loss: 6.9033 - val_mean_absolute_error: 6.9033 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.3754 - mean_absolute_error: 4.3754 - acc: 0.6175 - val_loss: 6.9109 - val_mean_absolute_error: 6.9109 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.3416 - mean_absolute_error: 4.3416 - acc: 0.6194 - val_loss: 6.9474 - val_mean_absolute_error: 6.9474 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.3866 - mean_absolute_error: 4.3866 - acc: 0.6189 - val_loss: 6.9322 - val_mean_absolute_error: 6.9322 - val_acc: 0.6073\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3837 - mean_absolute_error: 4.3837 - acc: 0.6205 - val_loss: 6.9295 - val_mean_absolute_error: 6.9295 - val_acc: 0.6106\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3532 - mean_absolute_error: 4.3532 - acc: 0.6173 - val_loss: 6.9264 - val_mean_absolute_error: 6.9264 - val_acc: 0.6147\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.3245 - mean_absolute_error: 4.3245 - acc: 0.6213 - val_loss: 6.9259 - val_mean_absolute_error: 6.9259 - val_acc: 0.6161\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3764 - mean_absolute_error: 4.3764 - acc: 0.6160 - val_loss: 6.9253 - val_mean_absolute_error: 6.9253 - val_acc: 0.6143\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3627 - mean_absolute_error: 4.3627 - acc: 0.6166 - val_loss: 6.9308 - val_mean_absolute_error: 6.9308 - val_acc: 0.6127\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3246 - mean_absolute_error: 4.3246 - acc: 0.6162 - val_loss: 6.9325 - val_mean_absolute_error: 6.9325 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3249 - mean_absolute_error: 4.3249 - acc: 0.6171 - val_loss: 6.9413 - val_mean_absolute_error: 6.9413 - val_acc: 0.6097\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.2993 - mean_absolute_error: 4.2993 - acc: 0.6217 - val_loss: 6.9042 - val_mean_absolute_error: 6.9042 - val_acc: 0.6107\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3804 - mean_absolute_error: 4.3804 - acc: 0.6185 - val_loss: 6.9148 - val_mean_absolute_error: 6.9148 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3251 - mean_absolute_error: 4.3251 - acc: 0.6184 - val_loss: 6.9316 - val_mean_absolute_error: 6.9316 - val_acc: 0.6103\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2495 - mean_absolute_error: 4.2495 - acc: 0.6212 - val_loss: 6.9690 - val_mean_absolute_error: 6.9690 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.2812 - mean_absolute_error: 4.2812 - acc: 0.6209 - val_loss: 6.9758 - val_mean_absolute_error: 6.9758 - val_acc: 0.6058\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3335 - mean_absolute_error: 4.3335 - acc: 0.6166 - val_loss: 6.9698 - val_mean_absolute_error: 6.9698 - val_acc: 0.6093\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2624 - mean_absolute_error: 4.2624 - acc: 0.6205 - val_loss: 6.9594 - val_mean_absolute_error: 6.9594 - val_acc: 0.6132\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3055 - mean_absolute_error: 4.3055 - acc: 0.6219 - val_loss: 6.9725 - val_mean_absolute_error: 6.9725 - val_acc: 0.6091\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2753 - mean_absolute_error: 4.2753 - acc: 0.6222 - val_loss: 6.9779 - val_mean_absolute_error: 6.9779 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3360 - mean_absolute_error: 4.3360 - acc: 0.6197 - val_loss: 7.0619 - val_mean_absolute_error: 7.0619 - val_acc: 0.6040\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2587 - mean_absolute_error: 4.2587 - acc: 0.6192 - val_loss: 7.0226 - val_mean_absolute_error: 7.0226 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2810 - mean_absolute_error: 4.2810 - acc: 0.6185 - val_loss: 7.0765 - val_mean_absolute_error: 7.0765 - val_acc: 0.6068\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2459 - mean_absolute_error: 4.2459 - acc: 0.6183 - val_loss: 7.0371 - val_mean_absolute_error: 7.0371 - val_acc: 0.6063\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.2404 - mean_absolute_error: 4.2404 - acc: 0.6169 - val_loss: 7.0810 - val_mean_absolute_error: 7.0810 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2990 - mean_absolute_error: 4.2990 - acc: 0.6220 - val_loss: 6.9955 - val_mean_absolute_error: 6.9955 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2689 - mean_absolute_error: 4.2689 - acc: 0.6208 - val_loss: 7.0629 - val_mean_absolute_error: 7.0629 - val_acc: 0.6088\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2704 - mean_absolute_error: 4.2704 - acc: 0.6213 - val_loss: 7.0260 - val_mean_absolute_error: 7.0260 - val_acc: 0.6059\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2308 - mean_absolute_error: 4.2308 - acc: 0.6195 - val_loss: 7.0107 - val_mean_absolute_error: 7.0107 - val_acc: 0.6097\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2515 - mean_absolute_error: 4.2515 - acc: 0.6201 - val_loss: 7.1506 - val_mean_absolute_error: 7.1506 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.3097 - mean_absolute_error: 4.3097 - acc: 0.6164 - val_loss: 7.1019 - val_mean_absolute_error: 7.1019 - val_acc: 0.6054\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2346 - mean_absolute_error: 4.2346 - acc: 0.6201 - val_loss: 7.0895 - val_mean_absolute_error: 7.0895 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2662 - mean_absolute_error: 4.2662 - acc: 0.6224 - val_loss: 7.0688 - val_mean_absolute_error: 7.0688 - val_acc: 0.6096\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.2569 - mean_absolute_error: 4.2569 - acc: 0.6191 - val_loss: 7.0733 - val_mean_absolute_error: 7.0733 - val_acc: 0.6062\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2226 - mean_absolute_error: 4.2226 - acc: 0.6228 - val_loss: 7.2498 - val_mean_absolute_error: 7.2498 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "16767/16767 [==============================] - 0s 10us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16766/16766 [==============================] - 1s 52us/step - loss: 7.4798 - mean_absolute_error: 7.4798 - acc: 0.2335 - val_loss: 7.0936 - val_mean_absolute_error: 7.0936 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 5.1608 - mean_absolute_error: 5.1608 - acc: 0.4897 - val_loss: 7.0387 - val_mean_absolute_error: 7.0387 - val_acc: 0.5055\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.8148 - mean_absolute_error: 4.8148 - acc: 0.5803 - val_loss: 7.0816 - val_mean_absolute_error: 7.0816 - val_acc: 0.4464\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.5256 - mean_absolute_error: 4.5256 - acc: 0.6016 - val_loss: 7.0538 - val_mean_absolute_error: 7.0538 - val_acc: 0.4529\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.4798 - mean_absolute_error: 4.4798 - acc: 0.6074 - val_loss: 7.0043 - val_mean_absolute_error: 7.0043 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.4681 - mean_absolute_error: 4.4681 - acc: 0.6099 - val_loss: 6.9572 - val_mean_absolute_error: 6.9572 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.4248 - mean_absolute_error: 4.4248 - acc: 0.6062 - val_loss: 6.9666 - val_mean_absolute_error: 6.9666 - val_acc: 0.4984\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4162 - mean_absolute_error: 4.4162 - acc: 0.6094 - val_loss: 6.9554 - val_mean_absolute_error: 6.9554 - val_acc: 0.5033\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.4010 - mean_absolute_error: 4.4010 - acc: 0.6121 - val_loss: 6.9415 - val_mean_absolute_error: 6.9415 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.4100 - mean_absolute_error: 4.4100 - acc: 0.6124 - val_loss: 6.9278 - val_mean_absolute_error: 6.9278 - val_acc: 0.5171\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2974 - mean_absolute_error: 4.2974 - acc: 0.6110 - val_loss: 6.9210 - val_mean_absolute_error: 6.9210 - val_acc: 0.5228\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3671 - mean_absolute_error: 4.3671 - acc: 0.6140 - val_loss: 6.8825 - val_mean_absolute_error: 6.8825 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3614 - mean_absolute_error: 4.3614 - acc: 0.6092 - val_loss: 6.9145 - val_mean_absolute_error: 6.9145 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3778 - mean_absolute_error: 4.3778 - acc: 0.6093 - val_loss: 6.8978 - val_mean_absolute_error: 6.8978 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3108 - mean_absolute_error: 4.3108 - acc: 0.6114 - val_loss: 6.9272 - val_mean_absolute_error: 6.9272 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.3194 - mean_absolute_error: 4.3194 - acc: 0.6110 - val_loss: 6.8834 - val_mean_absolute_error: 6.8834 - val_acc: 0.5396\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3289 - mean_absolute_error: 4.3289 - acc: 0.6102 - val_loss: 6.8592 - val_mean_absolute_error: 6.8592 - val_acc: 0.5619\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/150\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2522 - mean_absolute_error: 4.2522 - acc: 0.6106 - val_loss: 6.8564 - val_mean_absolute_error: 6.8564 - val_acc: 0.5415\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2544 - mean_absolute_error: 4.2544 - acc: 0.6135 - val_loss: 6.8602 - val_mean_absolute_error: 6.8602 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2721 - mean_absolute_error: 4.2721 - acc: 0.6131 - val_loss: 6.8568 - val_mean_absolute_error: 6.8568 - val_acc: 0.5317\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3160 - mean_absolute_error: 4.3160 - acc: 0.6134 - val_loss: 6.8634 - val_mean_absolute_error: 6.8634 - val_acc: 0.5421\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2535 - mean_absolute_error: 4.2535 - acc: 0.6107 - val_loss: 6.8670 - val_mean_absolute_error: 6.8670 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2917 - mean_absolute_error: 4.2917 - acc: 0.6125 - val_loss: 6.8707 - val_mean_absolute_error: 6.8707 - val_acc: 0.5379\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/150\n",
      "16766/16766 [==============================] - 0s 29us/step - loss: 4.2587 - mean_absolute_error: 4.2587 - acc: 0.6111 - val_loss: 6.8583 - val_mean_absolute_error: 6.8583 - val_acc: 0.5342\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.2635 - mean_absolute_error: 4.2635 - acc: 0.6103 - val_loss: 6.8727 - val_mean_absolute_error: 6.8727 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.2211 - mean_absolute_error: 4.2211 - acc: 0.6149 - val_loss: 6.8725 - val_mean_absolute_error: 6.8725 - val_acc: 0.5204\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2565 - mean_absolute_error: 4.2565 - acc: 0.6113 - val_loss: 6.8499 - val_mean_absolute_error: 6.8499 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2489 - mean_absolute_error: 4.2489 - acc: 0.6112 - val_loss: 6.8599 - val_mean_absolute_error: 6.8599 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.2235 - mean_absolute_error: 4.2235 - acc: 0.6145 - val_loss: 6.8461 - val_mean_absolute_error: 6.8461 - val_acc: 0.5267\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/150\n",
      "16766/16766 [==============================] - 0s 30us/step - loss: 4.2337 - mean_absolute_error: 4.2337 - acc: 0.6162 - val_loss: 6.8410 - val_mean_absolute_error: 6.8410 - val_acc: 0.5371\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/150\n",
      "16766/16766 [==============================] - 0s 29us/step - loss: 4.2428 - mean_absolute_error: 4.2428 - acc: 0.6139 - val_loss: 6.8491 - val_mean_absolute_error: 6.8491 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2339 - mean_absolute_error: 4.2339 - acc: 0.6114 - val_loss: 6.8559 - val_mean_absolute_error: 6.8559 - val_acc: 0.5330\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1651 - mean_absolute_error: 4.1651 - acc: 0.6132 - val_loss: 6.8594 - val_mean_absolute_error: 6.8594 - val_acc: 0.5163\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1802 - mean_absolute_error: 4.1802 - acc: 0.6156 - val_loss: 6.8661 - val_mean_absolute_error: 6.8661 - val_acc: 0.5177\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1983 - mean_absolute_error: 4.1983 - acc: 0.6106 - val_loss: 6.8611 - val_mean_absolute_error: 6.8611 - val_acc: 0.5128\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2168 - mean_absolute_error: 4.2168 - acc: 0.6140 - val_loss: 6.8511 - val_mean_absolute_error: 6.8511 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/150\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.1942 - mean_absolute_error: 4.1942 - acc: 0.6155 - val_loss: 6.8492 - val_mean_absolute_error: 6.8492 - val_acc: 0.5308\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1422 - mean_absolute_error: 4.1422 - acc: 0.6111 - val_loss: 6.8452 - val_mean_absolute_error: 6.8452 - val_acc: 0.5411\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1953 - mean_absolute_error: 4.1953 - acc: 0.6121 - val_loss: 6.8511 - val_mean_absolute_error: 6.8511 - val_acc: 0.5415\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1796 - mean_absolute_error: 4.1796 - acc: 0.6115 - val_loss: 6.8648 - val_mean_absolute_error: 6.8648 - val_acc: 0.5432\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2263 - mean_absolute_error: 4.2263 - acc: 0.6146 - val_loss: 6.8677 - val_mean_absolute_error: 6.8677 - val_acc: 0.5303\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1614 - mean_absolute_error: 4.1614 - acc: 0.6112 - val_loss: 6.8786 - val_mean_absolute_error: 6.8786 - val_acc: 0.5230\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2128 - mean_absolute_error: 4.2128 - acc: 0.6165 - val_loss: 6.8835 - val_mean_absolute_error: 6.8835 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.1729 - mean_absolute_error: 4.1729 - acc: 0.6189 - val_loss: 6.8961 - val_mean_absolute_error: 6.8961 - val_acc: 0.5060\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.2242 - mean_absolute_error: 4.2242 - acc: 0.6170 - val_loss: 6.8897 - val_mean_absolute_error: 6.8897 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2058 - mean_absolute_error: 4.2058 - acc: 0.6162 - val_loss: 6.9072 - val_mean_absolute_error: 6.9072 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1929 - mean_absolute_error: 4.1929 - acc: 0.6149 - val_loss: 6.9376 - val_mean_absolute_error: 6.9376 - val_acc: 0.4144\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1222 - mean_absolute_error: 4.1222 - acc: 0.6155 - val_loss: 6.9226 - val_mean_absolute_error: 6.9226 - val_acc: 0.4430\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1326 - mean_absolute_error: 4.1326 - acc: 0.6162 - val_loss: 6.9607 - val_mean_absolute_error: 6.9607 - val_acc: 0.4166\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1874 - mean_absolute_error: 4.1874 - acc: 0.6148 - val_loss: 6.9238 - val_mean_absolute_error: 6.9238 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 12us/step\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16767/16767 [==============================] - 1s 50us/step - loss: 6.9632 - mean_absolute_error: 6.9632 - acc: 0.2569 - val_loss: 7.1474 - val_mean_absolute_error: 7.1474 - val_acc: 0.5057\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 5.1926 - mean_absolute_error: 5.1926 - acc: 0.5013 - val_loss: 7.1280 - val_mean_absolute_error: 7.1280 - val_acc: 0.4913\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.8835 - mean_absolute_error: 4.8835 - acc: 0.5929 - val_loss: 7.1597 - val_mean_absolute_error: 7.1597 - val_acc: 0.4344\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.6675 - mean_absolute_error: 4.6675 - acc: 0.6019 - val_loss: 7.1756 - val_mean_absolute_error: 7.1756 - val_acc: 0.4321\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.6533 - mean_absolute_error: 4.6533 - acc: 0.6061 - val_loss: 7.1935 - val_mean_absolute_error: 7.1935 - val_acc: 0.4239\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.6242 - mean_absolute_error: 4.6242 - acc: 0.6098 - val_loss: 7.2168 - val_mean_absolute_error: 7.2168 - val_acc: 0.4095\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.5694 - mean_absolute_error: 4.5694 - acc: 0.6119 - val_loss: 7.2366 - val_mean_absolute_error: 7.2366 - val_acc: 0.4101\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.5800 - mean_absolute_error: 4.5800 - acc: 0.6114 - val_loss: 7.1728 - val_mean_absolute_error: 7.1728 - val_acc: 0.4404\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/150\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.5777 - mean_absolute_error: 4.5777 - acc: 0.6157 - val_loss: 7.1476 - val_mean_absolute_error: 7.1476 - val_acc: 0.4464\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.5939 - mean_absolute_error: 4.5939 - acc: 0.6117 - val_loss: 7.1490 - val_mean_absolute_error: 7.1490 - val_acc: 0.4473\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.5187 - mean_absolute_error: 4.5187 - acc: 0.6153 - val_loss: 7.1564 - val_mean_absolute_error: 7.1564 - val_acc: 0.4313\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.5287 - mean_absolute_error: 4.5287 - acc: 0.6136 - val_loss: 7.1935 - val_mean_absolute_error: 7.1935 - val_acc: 0.4171\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4791 - mean_absolute_error: 4.4791 - acc: 0.6138 - val_loss: 7.1912 - val_mean_absolute_error: 7.1912 - val_acc: 0.4094\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4580 - mean_absolute_error: 4.4580 - acc: 0.6147 - val_loss: 7.1853 - val_mean_absolute_error: 7.1853 - val_acc: 0.4086\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4649 - mean_absolute_error: 4.4649 - acc: 0.6144 - val_loss: 7.2314 - val_mean_absolute_error: 7.2314 - val_acc: 0.3838\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4646 - mean_absolute_error: 4.4646 - acc: 0.6118 - val_loss: 7.2065 - val_mean_absolute_error: 7.2065 - val_acc: 0.4043\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4554 - mean_absolute_error: 4.4554 - acc: 0.6117 - val_loss: 7.1796 - val_mean_absolute_error: 7.1796 - val_acc: 0.4105\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4438 - mean_absolute_error: 4.4438 - acc: 0.6131 - val_loss: 7.1615 - val_mean_absolute_error: 7.1615 - val_acc: 0.4196\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.4470 - mean_absolute_error: 4.4470 - acc: 0.6141 - val_loss: 7.1613 - val_mean_absolute_error: 7.1613 - val_acc: 0.4383\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.4625 - mean_absolute_error: 4.4625 - acc: 0.6153 - val_loss: 7.1916 - val_mean_absolute_error: 7.1916 - val_acc: 0.4142\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4490 - mean_absolute_error: 4.4490 - acc: 0.6151 - val_loss: 7.1719 - val_mean_absolute_error: 7.1719 - val_acc: 0.4068\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4271 - mean_absolute_error: 4.4271 - acc: 0.6162 - val_loss: 7.1267 - val_mean_absolute_error: 7.1267 - val_acc: 0.4425\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3977 - mean_absolute_error: 4.3977 - acc: 0.6187 - val_loss: 7.1080 - val_mean_absolute_error: 7.1080 - val_acc: 0.4486\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4093 - mean_absolute_error: 4.4093 - acc: 0.6169 - val_loss: 7.1103 - val_mean_absolute_error: 7.1103 - val_acc: 0.4519\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4410 - mean_absolute_error: 4.4410 - acc: 0.6181 - val_loss: 7.0394 - val_mean_absolute_error: 7.0394 - val_acc: 0.4927\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4260 - mean_absolute_error: 4.4260 - acc: 0.6152 - val_loss: 7.0295 - val_mean_absolute_error: 7.0295 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3690 - mean_absolute_error: 4.3690 - acc: 0.6185 - val_loss: 7.0238 - val_mean_absolute_error: 7.0238 - val_acc: 0.4928\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3697 - mean_absolute_error: 4.3697 - acc: 0.6173 - val_loss: 7.0086 - val_mean_absolute_error: 7.0086 - val_acc: 0.4948\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.3837 - mean_absolute_error: 4.3837 - acc: 0.6182 - val_loss: 7.0136 - val_mean_absolute_error: 7.0136 - val_acc: 0.5016\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3198 - mean_absolute_error: 4.3198 - acc: 0.6163 - val_loss: 7.0259 - val_mean_absolute_error: 7.0259 - val_acc: 0.4977\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.4479 - mean_absolute_error: 4.4479 - acc: 0.6216 - val_loss: 7.0190 - val_mean_absolute_error: 7.0190 - val_acc: 0.5078\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3954 - mean_absolute_error: 4.3954 - acc: 0.6159 - val_loss: 7.0384 - val_mean_absolute_error: 7.0384 - val_acc: 0.4957\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3342 - mean_absolute_error: 4.3342 - acc: 0.6198 - val_loss: 7.0686 - val_mean_absolute_error: 7.0686 - val_acc: 0.4708\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/150\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.3362 - mean_absolute_error: 4.3362 - acc: 0.6197 - val_loss: 7.0706 - val_mean_absolute_error: 7.0706 - val_acc: 0.4569\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.4099 - mean_absolute_error: 4.4099 - acc: 0.6187 - val_loss: 7.1639 - val_mean_absolute_error: 7.1639 - val_acc: 0.3659\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.2746 - mean_absolute_error: 4.2746 - acc: 0.6184 - val_loss: 7.1098 - val_mean_absolute_error: 7.1098 - val_acc: 0.3872\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3516 - mean_absolute_error: 4.3516 - acc: 0.6155 - val_loss: 7.1371 - val_mean_absolute_error: 7.1371 - val_acc: 0.3752\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3621 - mean_absolute_error: 4.3621 - acc: 0.6200 - val_loss: 7.0970 - val_mean_absolute_error: 7.0970 - val_acc: 0.3964\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3723 - mean_absolute_error: 4.3723 - acc: 0.6194 - val_loss: 7.0835 - val_mean_absolute_error: 7.0835 - val_acc: 0.4055\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3583 - mean_absolute_error: 4.3583 - acc: 0.6196 - val_loss: 7.0790 - val_mean_absolute_error: 7.0790 - val_acc: 0.4116\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.3355 - mean_absolute_error: 4.3355 - acc: 0.6145 - val_loss: 7.0704 - val_mean_absolute_error: 7.0704 - val_acc: 0.4065\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3524 - mean_absolute_error: 4.3524 - acc: 0.6163 - val_loss: 7.0726 - val_mean_absolute_error: 7.0726 - val_acc: 0.4111\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/150\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 4.3400 - mean_absolute_error: 4.3400 - acc: 0.6193 - val_loss: 7.1249 - val_mean_absolute_error: 7.1249 - val_acc: 0.3961\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3181 - mean_absolute_error: 4.3181 - acc: 0.6138 - val_loss: 7.1514 - val_mean_absolute_error: 7.1514 - val_acc: 0.3918\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.2415 - mean_absolute_error: 4.2415 - acc: 0.6116 - val_loss: 7.2150 - val_mean_absolute_error: 7.2150 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3184 - mean_absolute_error: 4.3184 - acc: 0.6108 - val_loss: 7.2558 - val_mean_absolute_error: 7.2558 - val_acc: 0.3844\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.3507 - mean_absolute_error: 4.3507 - acc: 0.6136 - val_loss: 7.1701 - val_mean_absolute_error: 7.1701 - val_acc: 0.3966\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.2769 - mean_absolute_error: 4.2769 - acc: 0.6148 - val_loss: 7.2340 - val_mean_absolute_error: 7.2340 - val_acc: 0.3944\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/150\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 4.3548 - mean_absolute_error: 4.3548 - acc: 0.6148 - val_loss: 7.1905 - val_mean_absolute_error: 7.1905 - val_acc: 0.4108\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/150\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 4.2528 - mean_absolute_error: 4.2528 - acc: 0.6145 - val_loss: 7.2323 - val_mean_absolute_error: 7.2323 - val_acc: 0.4033\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 10us/step\n",
      "16767/16767 [==============================] - 0s 10us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16766/16766 [==============================] - 1s 50us/step - loss: 5.6465 - mean_absolute_error: 5.6465 - acc: 0.4733 - val_loss: 7.1050 - val_mean_absolute_error: 7.1050 - val_acc: 0.5299\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.6794 - mean_absolute_error: 4.6794 - acc: 0.6004 - val_loss: 7.1839 - val_mean_absolute_error: 7.1839 - val_acc: 0.4125\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.6099 - mean_absolute_error: 4.6099 - acc: 0.6080 - val_loss: 7.0983 - val_mean_absolute_error: 7.0983 - val_acc: 0.3962\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.5746 - mean_absolute_error: 4.5746 - acc: 0.6134 - val_loss: 7.0913 - val_mean_absolute_error: 7.0913 - val_acc: 0.4077\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4876 - mean_absolute_error: 4.4876 - acc: 0.6112 - val_loss: 7.0949 - val_mean_absolute_error: 7.0949 - val_acc: 0.3868\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4316 - mean_absolute_error: 4.4316 - acc: 0.6109 - val_loss: 7.0318 - val_mean_absolute_error: 7.0318 - val_acc: 0.4405\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4707 - mean_absolute_error: 4.4707 - acc: 0.6111 - val_loss: 7.0759 - val_mean_absolute_error: 7.0759 - val_acc: 0.4490\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4377 - mean_absolute_error: 4.4377 - acc: 0.6115 - val_loss: 7.0205 - val_mean_absolute_error: 7.0205 - val_acc: 0.4683\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4354 - mean_absolute_error: 4.4354 - acc: 0.6107 - val_loss: 7.0393 - val_mean_absolute_error: 7.0393 - val_acc: 0.4406s - loss: 4.2876 - mean_absolute_error: 4.2876 - acc: 0.60\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2837 - mean_absolute_error: 4.2837 - acc: 0.6132 - val_loss: 7.0017 - val_mean_absolute_error: 7.0017 - val_acc: 0.4559\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3488 - mean_absolute_error: 4.3488 - acc: 0.6109 - val_loss: 6.9332 - val_mean_absolute_error: 6.9332 - val_acc: 0.5120 - loss: 4.4193 - mean_absolute_error: 4.4193 - acc: 0.61\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3228 - mean_absolute_error: 4.3228 - acc: 0.6100 - val_loss: 6.9416 - val_mean_absolute_error: 6.9416 - val_acc: 0.5218loss: 4.2939 - mean_absolute_error: 4.2939 - acc: 0.6\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3678 - mean_absolute_error: 4.3678 - acc: 0.6135 - val_loss: 6.9660 - val_mean_absolute_error: 6.9660 - val_acc: 0.4975\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3059 - mean_absolute_error: 4.3059 - acc: 0.6165 - val_loss: 6.9820 - val_mean_absolute_error: 6.9820 - val_acc: 0.5180\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2994 - mean_absolute_error: 4.2994 - acc: 0.6131 - val_loss: 6.9786 - val_mean_absolute_error: 6.9786 - val_acc: 0.5210\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3678 - mean_absolute_error: 4.3678 - acc: 0.6170 - val_loss: 7.0154 - val_mean_absolute_error: 7.0154 - val_acc: 0.4999\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3443 - mean_absolute_error: 4.3443 - acc: 0.6149 - val_loss: 6.9877 - val_mean_absolute_error: 6.9877 - val_acc: 0.5187\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3802 - mean_absolute_error: 4.3802 - acc: 0.6143 - val_loss: 6.9666 - val_mean_absolute_error: 6.9666 - val_acc: 0.5444\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3421 - mean_absolute_error: 4.3421 - acc: 0.6137 - val_loss: 7.0104 - val_mean_absolute_error: 7.0104 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2583 - mean_absolute_error: 4.2583 - acc: 0.6191 - val_loss: 6.9452 - val_mean_absolute_error: 6.9452 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2394 - mean_absolute_error: 4.2394 - acc: 0.6217 - val_loss: 6.9318 - val_mean_absolute_error: 6.9318 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2993 - mean_absolute_error: 4.2993 - acc: 0.6178 - val_loss: 6.9514 - val_mean_absolute_error: 6.9514 - val_acc: 0.5703\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2364 - mean_absolute_error: 4.2364 - acc: 0.6187 - val_loss: 6.9348 - val_mean_absolute_error: 6.9348 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2352 - mean_absolute_error: 4.2352 - acc: 0.6180 - val_loss: 6.9499 - val_mean_absolute_error: 6.9499 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2353 - mean_absolute_error: 4.2353 - acc: 0.6181 - val_loss: 6.9339 - val_mean_absolute_error: 6.9339 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2621 - mean_absolute_error: 4.2621 - acc: 0.6213 - val_loss: 6.9329 - val_mean_absolute_error: 6.9329 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2375 - mean_absolute_error: 4.2375 - acc: 0.6191 - val_loss: 6.9393 - val_mean_absolute_error: 6.9393 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2242 - mean_absolute_error: 4.2242 - acc: 0.6177 - val_loss: 6.9205 - val_mean_absolute_error: 6.9205 - val_acc: 0.5794\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2408 - mean_absolute_error: 4.2408 - acc: 0.6167 - val_loss: 6.9230 - val_mean_absolute_error: 6.9230 - val_acc: 0.5837\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2717 - mean_absolute_error: 4.2717 - acc: 0.6179 - val_loss: 6.9090 - val_mean_absolute_error: 6.9090 - val_acc: 0.5912\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/100\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.2545 - mean_absolute_error: 4.2545 - acc: 0.6177 - val_loss: 6.8751 - val_mean_absolute_error: 6.8751 - val_acc: 0.6020\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2782 - mean_absolute_error: 4.2782 - acc: 0.6164 - val_loss: 6.8837 - val_mean_absolute_error: 6.8837 - val_acc: 0.6026\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1944 - mean_absolute_error: 4.1944 - acc: 0.6183 - val_loss: 6.8812 - val_mean_absolute_error: 6.8812 - val_acc: 0.6003\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/100\n",
      "16766/16766 [==============================] - 0s 30us/step - loss: 4.2401 - mean_absolute_error: 4.2401 - acc: 0.6195 - val_loss: 6.8727 - val_mean_absolute_error: 6.8727 - val_acc: 0.6001\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2502 - mean_absolute_error: 4.2502 - acc: 0.6204 - val_loss: 6.8389 - val_mean_absolute_error: 6.8389 - val_acc: 0.5978\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1851 - mean_absolute_error: 4.1851 - acc: 0.6178 - val_loss: 6.8943 - val_mean_absolute_error: 6.8943 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2374 - mean_absolute_error: 4.2374 - acc: 0.6174 - val_loss: 6.8768 - val_mean_absolute_error: 6.8768 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/100\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.1746 - mean_absolute_error: 4.1746 - acc: 0.6217 - val_loss: 6.8680 - val_mean_absolute_error: 6.8680 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/100\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.2651 - mean_absolute_error: 4.2651 - acc: 0.6197 - val_loss: 6.8805 - val_mean_absolute_error: 6.8805 - val_acc: 0.5973\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/100\n",
      "16766/16766 [==============================] - 0s 30us/step - loss: 4.2426 - mean_absolute_error: 4.2426 - acc: 0.6189 - val_loss: 6.8466 - val_mean_absolute_error: 6.8466 - val_acc: 0.6003\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2061 - mean_absolute_error: 4.2061 - acc: 0.6220 - val_loss: 6.8723 - val_mean_absolute_error: 6.8723 - val_acc: 0.6044\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2587 - mean_absolute_error: 4.2587 - acc: 0.6194 - val_loss: 6.8934 - val_mean_absolute_error: 6.8934 - val_acc: 0.6046\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1781 - mean_absolute_error: 4.1781 - acc: 0.6190 - val_loss: 6.8786 - val_mean_absolute_error: 6.8786 - val_acc: 0.6095\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1885 - mean_absolute_error: 4.1885 - acc: 0.6205 - val_loss: 6.8669 - val_mean_absolute_error: 6.8669 - val_acc: 0.6023\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/100\n",
      "16766/16766 [==============================] - 1s 30us/step - loss: 4.1465 - mean_absolute_error: 4.1465 - acc: 0.6206 - val_loss: 6.9137 - val_mean_absolute_error: 6.9137 - val_acc: 0.6144\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2159 - mean_absolute_error: 4.2159 - acc: 0.6173 - val_loss: 6.9123 - val_mean_absolute_error: 6.9123 - val_acc: 0.6167\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1549 - mean_absolute_error: 4.1549 - acc: 0.6205 - val_loss: 6.8760 - val_mean_absolute_error: 6.8760 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/100\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1760 - mean_absolute_error: 4.1760 - acc: 0.6183 - val_loss: 6.8594 - val_mean_absolute_error: 6.8594 - val_acc: 0.6060\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/100\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1532 - mean_absolute_error: 4.1532 - acc: 0.6171 - val_loss: 6.8457 - val_mean_absolute_error: 6.8457 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/100\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1537 - mean_absolute_error: 4.1537 - acc: 0.6191 - val_loss: 6.8690 - val_mean_absolute_error: 6.8690 - val_acc: 0.6161\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16767/16767 [==============================] - 1s 54us/step - loss: 6.4904 - mean_absolute_error: 6.4904 - acc: 0.3967 - val_loss: 7.0583 - val_mean_absolute_error: 7.0583 - val_acc: 0.5694\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.9085 - mean_absolute_error: 4.9085 - acc: 0.5889 - val_loss: 7.0569 - val_mean_absolute_error: 7.0569 - val_acc: 0.5178\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.7576 - mean_absolute_error: 4.7576 - acc: 0.6030 - val_loss: 7.0562 - val_mean_absolute_error: 7.0562 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.6994 - mean_absolute_error: 4.6994 - acc: 0.6074 - val_loss: 7.0794 - val_mean_absolute_error: 7.0794 - val_acc: 0.4664\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.6151 - mean_absolute_error: 4.6151 - acc: 0.6102 - val_loss: 7.0285 - val_mean_absolute_error: 7.0285 - val_acc: 0.4962\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.6589 - mean_absolute_error: 4.6589 - acc: 0.6154 - val_loss: 7.0508 - val_mean_absolute_error: 7.0508 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.6100 - mean_absolute_error: 4.6100 - acc: 0.6142 - val_loss: 7.0630 - val_mean_absolute_error: 7.0630 - val_acc: 0.4890\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5630 - mean_absolute_error: 4.5630 - acc: 0.6125 - val_loss: 7.0247 - val_mean_absolute_error: 7.0247 - val_acc: 0.5241\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.5236 - mean_absolute_error: 4.5236 - acc: 0.6150 - val_loss: 6.9968 - val_mean_absolute_error: 6.9968 - val_acc: 0.5680\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.4953 - mean_absolute_error: 4.4953 - acc: 0.6164 - val_loss: 7.0071 - val_mean_absolute_error: 7.0071 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/100\n",
      "16767/16767 [==============================] - 1s 41us/step - loss: 4.5113 - mean_absolute_error: 4.5113 - acc: 0.6180 - val_loss: 7.0374 - val_mean_absolute_error: 7.0374 - val_acc: 0.5061\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.5443 - mean_absolute_error: 4.5443 - acc: 0.6124 - val_loss: 7.0092 - val_mean_absolute_error: 7.0092 - val_acc: 0.5636\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.4429 - mean_absolute_error: 4.4429 - acc: 0.6183 - val_loss: 6.9778 - val_mean_absolute_error: 6.9778 - val_acc: 0.5908\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.5006 - mean_absolute_error: 4.5006 - acc: 0.6138 - val_loss: 7.0057 - val_mean_absolute_error: 7.0057 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4819 - mean_absolute_error: 4.4819 - acc: 0.6136 - val_loss: 6.9940 - val_mean_absolute_error: 6.9940 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.4684 - mean_absolute_error: 4.4684 - acc: 0.6169 - val_loss: 6.9989 - val_mean_absolute_error: 6.9989 - val_acc: 0.5872\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4247 - mean_absolute_error: 4.4247 - acc: 0.6143 - val_loss: 6.9997 - val_mean_absolute_error: 6.9997 - val_acc: 0.5955\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3831 - mean_absolute_error: 4.3831 - acc: 0.6167 - val_loss: 7.0110 - val_mean_absolute_error: 7.0110 - val_acc: 0.5878\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.4271 - mean_absolute_error: 4.4271 - acc: 0.6188 - val_loss: 7.0089 - val_mean_absolute_error: 7.0089 - val_acc: 0.5941\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4388 - mean_absolute_error: 4.4388 - acc: 0.6185 - val_loss: 7.0466 - val_mean_absolute_error: 7.0466 - val_acc: 0.5852\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.4619 - mean_absolute_error: 4.4619 - acc: 0.6135 - val_loss: 7.0071 - val_mean_absolute_error: 7.0071 - val_acc: 0.5986\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.4311 - mean_absolute_error: 4.4311 - acc: 0.6141 - val_loss: 7.0078 - val_mean_absolute_error: 7.0078 - val_acc: 0.5948\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.4011 - mean_absolute_error: 4.4011 - acc: 0.6163 - val_loss: 7.0657 - val_mean_absolute_error: 7.0657 - val_acc: 0.5285\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/100\n",
      "16767/16767 [==============================] - 1s 41us/step - loss: 4.3665 - mean_absolute_error: 4.3665 - acc: 0.6132 - val_loss: 7.1148 - val_mean_absolute_error: 7.1148 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.3493 - mean_absolute_error: 4.3493 - acc: 0.6229 - val_loss: 7.1014 - val_mean_absolute_error: 7.1014 - val_acc: 0.4008\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.3658 - mean_absolute_error: 4.3658 - acc: 0.6160 - val_loss: 7.1519 - val_mean_absolute_error: 7.1519 - val_acc: 0.3984\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3050 - mean_absolute_error: 4.3050 - acc: 0.6142 - val_loss: 7.0904 - val_mean_absolute_error: 7.0904 - val_acc: 0.4074\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3263 - mean_absolute_error: 4.3263 - acc: 0.6127 - val_loss: 7.1263 - val_mean_absolute_error: 7.1263 - val_acc: 0.4023\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3780 - mean_absolute_error: 4.3780 - acc: 0.6155 - val_loss: 7.1462 - val_mean_absolute_error: 7.1462 - val_acc: 0.4014\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.3284 - mean_absolute_error: 4.3284 - acc: 0.6151 - val_loss: 7.2351 - val_mean_absolute_error: 7.2351 - val_acc: 0.3999\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/100\n",
      "16767/16767 [==============================] - 1s 43us/step - loss: 4.3440 - mean_absolute_error: 4.3440 - acc: 0.6121 - val_loss: 7.1867 - val_mean_absolute_error: 7.1867 - val_acc: 0.4049\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/100\n",
      "16767/16767 [==============================] - 1s 43us/step - loss: 4.3489 - mean_absolute_error: 4.3489 - acc: 0.6135 - val_loss: 7.2826 - val_mean_absolute_error: 7.2826 - val_acc: 0.4080\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3364 - mean_absolute_error: 4.3364 - acc: 0.6102 - val_loss: 7.1693 - val_mean_absolute_error: 7.1693 - val_acc: 0.4146\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3041 - mean_absolute_error: 4.3041 - acc: 0.6166 - val_loss: 7.2014 - val_mean_absolute_error: 7.2014 - val_acc: 0.4117\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3070 - mean_absolute_error: 4.3070 - acc: 0.6162 - val_loss: 7.3287 - val_mean_absolute_error: 7.3287 - val_acc: 0.4115\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3122 - mean_absolute_error: 4.3122 - acc: 0.6147 - val_loss: 7.3676 - val_mean_absolute_error: 7.3676 - val_acc: 0.4277\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3491 - mean_absolute_error: 4.3491 - acc: 0.6173 - val_loss: 7.3132 - val_mean_absolute_error: 7.3132 - val_acc: 0.4154\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.3113 - mean_absolute_error: 4.3113 - acc: 0.6158 - val_loss: 7.4132 - val_mean_absolute_error: 7.4132 - val_acc: 0.4271\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.2723 - mean_absolute_error: 4.2723 - acc: 0.6181 - val_loss: 7.2841 - val_mean_absolute_error: 7.2841 - val_acc: 0.4312\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.2748 - mean_absolute_error: 4.2748 - acc: 0.6130 - val_loss: 7.1771 - val_mean_absolute_error: 7.1771 - val_acc: 0.4369\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.3466 - mean_absolute_error: 4.3466 - acc: 0.6156 - val_loss: 7.2796 - val_mean_absolute_error: 7.2796 - val_acc: 0.4387\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3429 - mean_absolute_error: 4.3429 - acc: 0.6120 - val_loss: 7.3659 - val_mean_absolute_error: 7.3659 - val_acc: 0.4394\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2747 - mean_absolute_error: 4.2747 - acc: 0.6130 - val_loss: 7.4304 - val_mean_absolute_error: 7.4304 - val_acc: 0.4290\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3562 - mean_absolute_error: 4.3562 - acc: 0.6124 - val_loss: 7.3589 - val_mean_absolute_error: 7.3589 - val_acc: 0.4342\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2122 - mean_absolute_error: 4.2122 - acc: 0.6138 - val_loss: 7.5680 - val_mean_absolute_error: 7.5680 - val_acc: 0.4247\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2021 - mean_absolute_error: 4.2021 - acc: 0.6170 - val_loss: 7.3402 - val_mean_absolute_error: 7.3402 - val_acc: 0.4414\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2790 - mean_absolute_error: 4.2790 - acc: 0.6139 - val_loss: 7.3612 - val_mean_absolute_error: 7.3612 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2237 - mean_absolute_error: 4.2237 - acc: 0.6177 - val_loss: 7.4604 - val_mean_absolute_error: 7.4604 - val_acc: 0.4369\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.2251 - mean_absolute_error: 4.2251 - acc: 0.6123 - val_loss: 7.5435 - val_mean_absolute_error: 7.5435 - val_acc: 0.4332\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.2452 - mean_absolute_error: 4.2452 - acc: 0.6130 - val_loss: 7.3881 - val_mean_absolute_error: 7.3881 - val_acc: 0.4288\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16766/16766 [==============================] - 1s 50us/step - loss: 9.4005 - mean_absolute_error: 9.4005 - acc: 0.2194 - val_loss: 7.1497 - val_mean_absolute_error: 7.1497 - val_acc: 0.5268\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 5.6937 - mean_absolute_error: 5.6937 - acc: 0.4797 - val_loss: 7.1012 - val_mean_absolute_error: 7.1012 - val_acc: 0.5646\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 5.0836 - mean_absolute_error: 5.0836 - acc: 0.5725 - val_loss: 7.0940 - val_mean_absolute_error: 7.0940 - val_acc: 0.4835\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.8235 - mean_absolute_error: 4.8235 - acc: 0.5955 - val_loss: 7.1058 - val_mean_absolute_error: 7.1058 - val_acc: 0.4662\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.7235 - mean_absolute_error: 4.7235 - acc: 0.6055 - val_loss: 7.1045 - val_mean_absolute_error: 7.1045 - val_acc: 0.4655\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.6752 - mean_absolute_error: 4.6752 - acc: 0.6055 - val_loss: 7.0873 - val_mean_absolute_error: 7.0873 - val_acc: 0.4528\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.6351 - mean_absolute_error: 4.6351 - acc: 0.6081 - val_loss: 7.0899 - val_mean_absolute_error: 7.0899 - val_acc: 0.4592\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.5760 - mean_absolute_error: 4.5760 - acc: 0.6120 - val_loss: 7.1554 - val_mean_absolute_error: 7.1554 - val_acc: 0.4251\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.4713 - mean_absolute_error: 4.4713 - acc: 0.6072 - val_loss: 7.1542 - val_mean_absolute_error: 7.1542 - val_acc: 0.4380\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.4647 - mean_absolute_error: 4.4647 - acc: 0.6043 - val_loss: 7.0929 - val_mean_absolute_error: 7.0929 - val_acc: 0.4636\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.4552 - mean_absolute_error: 4.4552 - acc: 0.6099 - val_loss: 7.0857 - val_mean_absolute_error: 7.0857 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4864 - mean_absolute_error: 4.4864 - acc: 0.6077 - val_loss: 7.1123 - val_mean_absolute_error: 7.1123 - val_acc: 0.4552\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/100\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.3931 - mean_absolute_error: 4.3931 - acc: 0.6070 - val_loss: 7.0583 - val_mean_absolute_error: 7.0583 - val_acc: 0.4872\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/100\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.3961 - mean_absolute_error: 4.3961 - acc: 0.6122 - val_loss: 7.0870 - val_mean_absolute_error: 7.0870 - val_acc: 0.4661\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.4338 - mean_absolute_error: 4.4338 - acc: 0.6084 - val_loss: 7.0563 - val_mean_absolute_error: 7.0563 - val_acc: 0.4837\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3977 - mean_absolute_error: 4.3977 - acc: 0.6077 - val_loss: 7.0314 - val_mean_absolute_error: 7.0314 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.4127 - mean_absolute_error: 4.4127 - acc: 0.6050 - val_loss: 6.9894 - val_mean_absolute_error: 6.9894 - val_acc: 0.5111\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3052 - mean_absolute_error: 4.3052 - acc: 0.6054 - val_loss: 7.0352 - val_mean_absolute_error: 7.0352 - val_acc: 0.4728\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3197 - mean_absolute_error: 4.3197 - acc: 0.6087 - val_loss: 6.9866 - val_mean_absolute_error: 6.9866 - val_acc: 0.5019\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2869 - mean_absolute_error: 4.2869 - acc: 0.6055 - val_loss: 7.0009 - val_mean_absolute_error: 7.0009 - val_acc: 0.4859\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3149 - mean_absolute_error: 4.3149 - acc: 0.6082 - val_loss: 6.9487 - val_mean_absolute_error: 6.9487 - val_acc: 0.5231\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/100\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.3359 - mean_absolute_error: 4.3359 - acc: 0.6067 - val_loss: 6.9245 - val_mean_absolute_error: 6.9245 - val_acc: 0.5371\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3005 - mean_absolute_error: 4.3005 - acc: 0.6075 - val_loss: 6.9162 - val_mean_absolute_error: 6.9162 - val_acc: 0.5359\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.2660 - mean_absolute_error: 4.2660 - acc: 0.6097 - val_loss: 6.9341 - val_mean_absolute_error: 6.9341 - val_acc: 0.5333\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2599 - mean_absolute_error: 4.2599 - acc: 0.6101 - val_loss: 6.9291 - val_mean_absolute_error: 6.9291 - val_acc: 0.5281\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3701 - mean_absolute_error: 4.3701 - acc: 0.6067 - val_loss: 6.8799 - val_mean_absolute_error: 6.8799 - val_acc: 0.5623\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2490 - mean_absolute_error: 4.2490 - acc: 0.6102 - val_loss: 6.9040 - val_mean_absolute_error: 6.9040 - val_acc: 0.5463\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3236 - mean_absolute_error: 4.3236 - acc: 0.6059 - val_loss: 6.8845 - val_mean_absolute_error: 6.8845 - val_acc: 0.5593\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2734 - mean_absolute_error: 4.2734 - acc: 0.6100 - val_loss: 6.8852 - val_mean_absolute_error: 6.8852 - val_acc: 0.5650\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3130 - mean_absolute_error: 4.3130 - acc: 0.6083 - val_loss: 6.8767 - val_mean_absolute_error: 6.8767 - val_acc: 0.5673\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2558 - mean_absolute_error: 4.2558 - acc: 0.6114 - val_loss: 6.8718 - val_mean_absolute_error: 6.8718 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2973 - mean_absolute_error: 4.2973 - acc: 0.6122 - val_loss: 6.8491 - val_mean_absolute_error: 6.8491 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2917 - mean_absolute_error: 4.2917 - acc: 0.6112 - val_loss: 6.8534 - val_mean_absolute_error: 6.8534 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2262 - mean_absolute_error: 4.2262 - acc: 0.6063 - val_loss: 6.8499 - val_mean_absolute_error: 6.8499 - val_acc: 0.5593\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2957 - mean_absolute_error: 4.2957 - acc: 0.6091 - val_loss: 6.8717 - val_mean_absolute_error: 6.8717 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2423 - mean_absolute_error: 4.2423 - acc: 0.6120 - val_loss: 6.8943 - val_mean_absolute_error: 6.8943 - val_acc: 0.5527\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2519 - mean_absolute_error: 4.2519 - acc: 0.6099 - val_loss: 6.8978 - val_mean_absolute_error: 6.8978 - val_acc: 0.5719\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1685 - mean_absolute_error: 4.1685 - acc: 0.6135 - val_loss: 6.8609 - val_mean_absolute_error: 6.8609 - val_acc: 0.5623\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2281 - mean_absolute_error: 4.2281 - acc: 0.6103 - val_loss: 6.8897 - val_mean_absolute_error: 6.8897 - val_acc: 0.5590\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.2382 - mean_absolute_error: 4.2382 - acc: 0.6108 - val_loss: 6.9306 - val_mean_absolute_error: 6.9306 - val_acc: 0.4667\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1984 - mean_absolute_error: 4.1984 - acc: 0.6119 - val_loss: 6.9313 - val_mean_absolute_error: 6.9313 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2382 - mean_absolute_error: 4.2382 - acc: 0.6112 - val_loss: 6.9274 - val_mean_absolute_error: 6.9274 - val_acc: 0.5101\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1764 - mean_absolute_error: 4.1764 - acc: 0.6101 - val_loss: 6.9852 - val_mean_absolute_error: 6.9852 - val_acc: 0.4293\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1633 - mean_absolute_error: 4.1633 - acc: 0.6120 - val_loss: 6.9790 - val_mean_absolute_error: 6.9790 - val_acc: 0.4239\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1608 - mean_absolute_error: 4.1608 - acc: 0.6130 - val_loss: 6.9821 - val_mean_absolute_error: 6.9821 - val_acc: 0.4259\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1751 - mean_absolute_error: 4.1751 - acc: 0.6131 - val_loss: 6.9302 - val_mean_absolute_error: 6.9302 - val_acc: 0.4422\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1919 - mean_absolute_error: 4.1919 - acc: 0.6117 - val_loss: 6.9192 - val_mean_absolute_error: 6.9192 - val_acc: 0.4336\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1941 - mean_absolute_error: 4.1941 - acc: 0.6127 - val_loss: 6.9356 - val_mean_absolute_error: 6.9356 - val_acc: 0.4295\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1754 - mean_absolute_error: 4.1754 - acc: 0.6143 - val_loss: 6.9533 - val_mean_absolute_error: 6.9533 - val_acc: 0.4282\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1981 - mean_absolute_error: 4.1981 - acc: 0.6135 - val_loss: 7.0258 - val_mean_absolute_error: 7.0258 - val_acc: 0.4300\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "16766/16766 [==============================] - 0s 10us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16767/16767 [==============================] - 1s 56us/step - loss: 6.9014 - mean_absolute_error: 6.9014 - acc: 0.3601 - val_loss: 7.1053 - val_mean_absolute_error: 7.1053 - val_acc: 0.5627\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 5.2528 - mean_absolute_error: 5.2528 - acc: 0.5554 - val_loss: 7.0939 - val_mean_absolute_error: 7.0939 - val_acc: 0.5508\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.9790 - mean_absolute_error: 4.9790 - acc: 0.5975 - val_loss: 7.0831 - val_mean_absolute_error: 7.0831 - val_acc: 0.5134\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.8420 - mean_absolute_error: 4.8420 - acc: 0.6048 - val_loss: 7.0940 - val_mean_absolute_error: 7.0940 - val_acc: 0.4475\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.7506 - mean_absolute_error: 4.7506 - acc: 0.6049 - val_loss: 7.0716 - val_mean_absolute_error: 7.0716 - val_acc: 0.4625\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.7604 - mean_absolute_error: 4.7604 - acc: 0.6046 - val_loss: 7.1037 - val_mean_absolute_error: 7.1037 - val_acc: 0.4473\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.6880 - mean_absolute_error: 4.6880 - acc: 0.6054 - val_loss: 7.1805 - val_mean_absolute_error: 7.1805 - val_acc: 0.4030\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.6559 - mean_absolute_error: 4.6559 - acc: 0.6065 - val_loss: 7.1040 - val_mean_absolute_error: 7.1040 - val_acc: 0.4417\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.6203 - mean_absolute_error: 4.6203 - acc: 0.6110 - val_loss: 7.1269 - val_mean_absolute_error: 7.1269 - val_acc: 0.4333\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.5313 - mean_absolute_error: 4.5313 - acc: 0.6102 - val_loss: 7.1255 - val_mean_absolute_error: 7.1255 - val_acc: 0.4333\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.5602 - mean_absolute_error: 4.5602 - acc: 0.6101 - val_loss: 7.0968 - val_mean_absolute_error: 7.0968 - val_acc: 0.4448\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.5060 - mean_absolute_error: 4.5060 - acc: 0.6092 - val_loss: 7.0504 - val_mean_absolute_error: 7.0504 - val_acc: 0.4729\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.5410 - mean_absolute_error: 4.5410 - acc: 0.6094 - val_loss: 7.0262 - val_mean_absolute_error: 7.0262 - val_acc: 0.4955\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5088 - mean_absolute_error: 4.5088 - acc: 0.6105 - val_loss: 6.9791 - val_mean_absolute_error: 6.9791 - val_acc: 0.5111\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4788 - mean_absolute_error: 4.4788 - acc: 0.6086 - val_loss: 6.9579 - val_mean_absolute_error: 6.9579 - val_acc: 0.5012\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4444 - mean_absolute_error: 4.4444 - acc: 0.6128 - val_loss: 6.9731 - val_mean_absolute_error: 6.9731 - val_acc: 0.5651\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4655 - mean_absolute_error: 4.4655 - acc: 0.6150 - val_loss: 6.9770 - val_mean_absolute_error: 6.9770 - val_acc: 0.5524\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.4909 - mean_absolute_error: 4.4909 - acc: 0.6083 - val_loss: 6.9330 - val_mean_absolute_error: 6.9330 - val_acc: 0.5273\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/100\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4521 - mean_absolute_error: 4.4521 - acc: 0.6079 - val_loss: 6.9433 - val_mean_absolute_error: 6.9433 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4516 - mean_absolute_error: 4.4516 - acc: 0.6094 - val_loss: 6.9375 - val_mean_absolute_error: 6.9375 - val_acc: 0.5707\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4046 - mean_absolute_error: 4.4046 - acc: 0.6146 - val_loss: 6.9421 - val_mean_absolute_error: 6.9421 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/100\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4412 - mean_absolute_error: 4.4412 - acc: 0.6106 - val_loss: 6.9072 - val_mean_absolute_error: 6.9072 - val_acc: 0.5799\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4390 - mean_absolute_error: 4.4390 - acc: 0.6115 - val_loss: 6.8995 - val_mean_absolute_error: 6.8995 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4189 - mean_absolute_error: 4.4189 - acc: 0.6127 - val_loss: 6.9232 - val_mean_absolute_error: 6.9232 - val_acc: 0.5864\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4165 - mean_absolute_error: 4.4165 - acc: 0.6144 - val_loss: 6.8818 - val_mean_absolute_error: 6.8818 - val_acc: 0.5763\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/100\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.4108 - mean_absolute_error: 4.4108 - acc: 0.6145 - val_loss: 6.9105 - val_mean_absolute_error: 6.9105 - val_acc: 0.5821\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3950 - mean_absolute_error: 4.3950 - acc: 0.6094 - val_loss: 6.9267 - val_mean_absolute_error: 6.9267 - val_acc: 0.5895\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4027 - mean_absolute_error: 4.4027 - acc: 0.6120 - val_loss: 6.9220 - val_mean_absolute_error: 6.9220 - val_acc: 0.5945\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3889 - mean_absolute_error: 4.3889 - acc: 0.6107 - val_loss: 6.9363 - val_mean_absolute_error: 6.9363 - val_acc: 0.5886\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3420 - mean_absolute_error: 4.3420 - acc: 0.6105 - val_loss: 6.9307 - val_mean_absolute_error: 6.9307 - val_acc: 0.5830\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3359 - mean_absolute_error: 4.3359 - acc: 0.6086 - val_loss: 6.9368 - val_mean_absolute_error: 6.9368 - val_acc: 0.5818\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3575 - mean_absolute_error: 4.3575 - acc: 0.6126 - val_loss: 6.9584 - val_mean_absolute_error: 6.9584 - val_acc: 0.5874\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3939 - mean_absolute_error: 4.3939 - acc: 0.6136 - val_loss: 6.9706 - val_mean_absolute_error: 6.9706 - val_acc: 0.5805\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3919 - mean_absolute_error: 4.3919 - acc: 0.6136 - val_loss: 6.9858 - val_mean_absolute_error: 6.9858 - val_acc: 0.5817\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3690 - mean_absolute_error: 4.3690 - acc: 0.6139 - val_loss: 6.9855 - val_mean_absolute_error: 6.9855 - val_acc: 0.5799\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3705 - mean_absolute_error: 4.3705 - acc: 0.6099 - val_loss: 7.0016 - val_mean_absolute_error: 7.0016 - val_acc: 0.5706\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3605 - mean_absolute_error: 4.3605 - acc: 0.6108 - val_loss: 7.0122 - val_mean_absolute_error: 7.0122 - val_acc: 0.5707\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3072 - mean_absolute_error: 4.3072 - acc: 0.6122 - val_loss: 7.0028 - val_mean_absolute_error: 7.0028 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3212 - mean_absolute_error: 4.3212 - acc: 0.6116 - val_loss: 6.9898 - val_mean_absolute_error: 6.9898 - val_acc: 0.5812\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3275 - mean_absolute_error: 4.3275 - acc: 0.6120 - val_loss: 7.0193 - val_mean_absolute_error: 7.0193 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3319 - mean_absolute_error: 4.3319 - acc: 0.6108 - val_loss: 7.0151 - val_mean_absolute_error: 7.0151 - val_acc: 0.5762\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3126 - mean_absolute_error: 4.3126 - acc: 0.6117 - val_loss: 7.0582 - val_mean_absolute_error: 7.0582 - val_acc: 0.5717\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.81411\n",
      "Epoch 43/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2494 - mean_absolute_error: 4.2494 - acc: 0.6109 - val_loss: 7.0917 - val_mean_absolute_error: 7.0917 - val_acc: 0.5681\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.81411\n",
      "Epoch 44/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2995 - mean_absolute_error: 4.2995 - acc: 0.6134 - val_loss: 7.0776 - val_mean_absolute_error: 7.0776 - val_acc: 0.5648\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.81411\n",
      "Epoch 45/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3054 - mean_absolute_error: 4.3054 - acc: 0.6108 - val_loss: 7.0621 - val_mean_absolute_error: 7.0621 - val_acc: 0.5743\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.81411\n",
      "Epoch 46/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2759 - mean_absolute_error: 4.2759 - acc: 0.6144 - val_loss: 7.1049 - val_mean_absolute_error: 7.1049 - val_acc: 0.5512\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.81411\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2806 - mean_absolute_error: 4.2806 - acc: 0.6150 - val_loss: 7.0961 - val_mean_absolute_error: 7.0961 - val_acc: 0.5575\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.81411\n",
      "Epoch 48/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2899 - mean_absolute_error: 4.2899 - acc: 0.6154 - val_loss: 7.1216 - val_mean_absolute_error: 7.1216 - val_acc: 0.5495\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.81411\n",
      "Epoch 49/100\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 4.2427 - mean_absolute_error: 4.2427 - acc: 0.6158 - val_loss: 7.1354 - val_mean_absolute_error: 7.1354 - val_acc: 0.4381\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.81411\n",
      "Epoch 50/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2067 - mean_absolute_error: 4.2067 - acc: 0.6191 - val_loss: 7.1068 - val_mean_absolute_error: 7.1068 - val_acc: 0.5569\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.81411\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "16767/16767 [==============================] - 0s 10us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16766/16766 [==============================] - 1s 53us/step - loss: 6.3673 - mean_absolute_error: 6.3673 - acc: 0.2964 - val_loss: 7.1198 - val_mean_absolute_error: 7.1198 - val_acc: 0.4871\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.81411\n",
      "Epoch 2/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.8953 - mean_absolute_error: 4.8953 - acc: 0.5054 - val_loss: 7.2099 - val_mean_absolute_error: 7.2099 - val_acc: 0.4113\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.81411\n",
      "Epoch 3/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.6057 - mean_absolute_error: 4.6057 - acc: 0.5493 - val_loss: 7.1368 - val_mean_absolute_error: 7.1368 - val_acc: 0.4303\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.81411\n",
      "Epoch 4/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.5537 - mean_absolute_error: 4.5537 - acc: 0.5743 - val_loss: 7.0689 - val_mean_absolute_error: 7.0689 - val_acc: 0.4286\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.81411\n",
      "Epoch 5/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.5352 - mean_absolute_error: 4.5352 - acc: 0.5848 - val_loss: 7.0027 - val_mean_absolute_error: 7.0027 - val_acc: 0.4627\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.81411\n",
      "Epoch 6/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4355 - mean_absolute_error: 4.4355 - acc: 0.5887 - val_loss: 6.9946 - val_mean_absolute_error: 6.9946 - val_acc: 0.4704\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.81411\n",
      "Epoch 7/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4727 - mean_absolute_error: 4.4727 - acc: 0.5953 - val_loss: 6.9148 - val_mean_absolute_error: 6.9148 - val_acc: 0.5134\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.81411\n",
      "Epoch 8/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.4250 - mean_absolute_error: 4.4250 - acc: 0.6004 - val_loss: 6.9391 - val_mean_absolute_error: 6.9391 - val_acc: 0.4968\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.81411\n",
      "Epoch 9/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3712 - mean_absolute_error: 4.3712 - acc: 0.6027 - val_loss: 6.9185 - val_mean_absolute_error: 6.9185 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.81411\n",
      "Epoch 10/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3902 - mean_absolute_error: 4.3902 - acc: 0.6059 - val_loss: 6.8758 - val_mean_absolute_error: 6.8758 - val_acc: 0.5392\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.81411\n",
      "Epoch 11/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3218 - mean_absolute_error: 4.3218 - acc: 0.6082 - val_loss: 6.9311 - val_mean_absolute_error: 6.9311 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.81411\n",
      "Epoch 12/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3419 - mean_absolute_error: 4.3419 - acc: 0.6063 - val_loss: 6.9231 - val_mean_absolute_error: 6.9231 - val_acc: 0.5448\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.81411\n",
      "Epoch 13/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3458 - mean_absolute_error: 4.3458 - acc: 0.6114 - val_loss: 6.8794 - val_mean_absolute_error: 6.8794 - val_acc: 0.5402\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.81411\n",
      "Epoch 14/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2765 - mean_absolute_error: 4.2765 - acc: 0.6091 - val_loss: 6.8703 - val_mean_absolute_error: 6.8703 - val_acc: 0.5496\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.81411\n",
      "Epoch 15/150\n",
      "16766/16766 [==============================] - 1s 41us/step - loss: 4.2763 - mean_absolute_error: 4.2763 - acc: 0.6120 - val_loss: 6.8908 - val_mean_absolute_error: 6.8908 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.81411\n",
      "Epoch 16/150\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.3474 - mean_absolute_error: 4.3474 - acc: 0.6057 - val_loss: 6.8988 - val_mean_absolute_error: 6.8988 - val_acc: 0.5516\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.81411\n",
      "Epoch 17/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3682 - mean_absolute_error: 4.3682 - acc: 0.6135 - val_loss: 6.9070 - val_mean_absolute_error: 6.9070 - val_acc: 0.5508\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.81411\n",
      "Epoch 18/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.3116 - mean_absolute_error: 4.3116 - acc: 0.6100 - val_loss: 6.9009 - val_mean_absolute_error: 6.9009 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.81411\n",
      "Epoch 19/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2933 - mean_absolute_error: 4.2933 - acc: 0.6109 - val_loss: 6.8953 - val_mean_absolute_error: 6.8953 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.81411\n",
      "Epoch 20/150\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.2467 - mean_absolute_error: 4.2467 - acc: 0.6165 - val_loss: 6.8205 - val_mean_absolute_error: 6.8205 - val_acc: 0.5664\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.81411\n",
      "Epoch 21/150\n",
      "16766/16766 [==============================] - 1s 42us/step - loss: 4.3143 - mean_absolute_error: 4.3143 - acc: 0.6096 - val_loss: 6.8717 - val_mean_absolute_error: 6.8717 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.81411\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.2743 - mean_absolute_error: 4.2743 - acc: 0.6127 - val_loss: 6.8494 - val_mean_absolute_error: 6.8494 - val_acc: 0.5798\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.81411\n",
      "Epoch 23/150\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.2978 - mean_absolute_error: 4.2978 - acc: 0.6142 - val_loss: 6.8551 - val_mean_absolute_error: 6.8551 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.81411\n",
      "Epoch 24/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3313 - mean_absolute_error: 4.3313 - acc: 0.6123 - val_loss: 6.8757 - val_mean_absolute_error: 6.8757 - val_acc: 0.5771\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.81411\n",
      "Epoch 25/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.2954 - mean_absolute_error: 4.2954 - acc: 0.6123 - val_loss: 6.8777 - val_mean_absolute_error: 6.8777 - val_acc: 0.5829\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.81411\n",
      "Epoch 26/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.2444 - mean_absolute_error: 4.2444 - acc: 0.6118 - val_loss: 6.9065 - val_mean_absolute_error: 6.9065 - val_acc: 0.5847\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.81411\n",
      "Epoch 27/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.3264 - mean_absolute_error: 4.3264 - acc: 0.6069 - val_loss: 6.9747 - val_mean_absolute_error: 6.9747 - val_acc: 0.5575\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.81411\n",
      "Epoch 28/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.2791 - mean_absolute_error: 4.2791 - acc: 0.6131 - val_loss: 6.8905 - val_mean_absolute_error: 6.8905 - val_acc: 0.5989\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.81411\n",
      "Epoch 29/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2359 - mean_absolute_error: 4.2359 - acc: 0.6074 - val_loss: 6.9062 - val_mean_absolute_error: 6.9062 - val_acc: 0.5989\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.81411\n",
      "Epoch 30/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2740 - mean_absolute_error: 4.2740 - acc: 0.6087 - val_loss: 6.8950 - val_mean_absolute_error: 6.8950 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.81411\n",
      "Epoch 31/150\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.2846 - mean_absolute_error: 4.2846 - acc: 0.6108 - val_loss: 6.8906 - val_mean_absolute_error: 6.8906 - val_acc: 0.5958\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.81411\n",
      "Epoch 32/150\n",
      "16766/16766 [==============================] - 1s 44us/step - loss: 4.2376 - mean_absolute_error: 4.2376 - acc: 0.6123 - val_loss: 6.8938 - val_mean_absolute_error: 6.8938 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.81411\n",
      "Epoch 33/150\n",
      "16766/16766 [==============================] - 1s 40us/step - loss: 4.2307 - mean_absolute_error: 4.2307 - acc: 0.6077 - val_loss: 6.8898 - val_mean_absolute_error: 6.8898 - val_acc: 0.5957\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.81411\n",
      "Epoch 34/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2820 - mean_absolute_error: 4.2820 - acc: 0.6118 - val_loss: 6.8787 - val_mean_absolute_error: 6.8787 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.81411\n",
      "Epoch 35/150\n",
      "16766/16766 [==============================] - 1s 41us/step - loss: 4.2226 - mean_absolute_error: 4.2226 - acc: 0.6111 - val_loss: 6.8729 - val_mean_absolute_error: 6.8729 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.81411\n",
      "Epoch 36/150\n",
      "16766/16766 [==============================] - 1s 44us/step - loss: 4.2456 - mean_absolute_error: 4.2456 - acc: 0.6103 - val_loss: 6.8554 - val_mean_absolute_error: 6.8554 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.81411\n",
      "Epoch 37/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2047 - mean_absolute_error: 4.2047 - acc: 0.6165 - val_loss: 6.8747 - val_mean_absolute_error: 6.8747 - val_acc: 0.5791\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.81411\n",
      "Epoch 38/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2353 - mean_absolute_error: 4.2353 - acc: 0.6078 - val_loss: 6.8660 - val_mean_absolute_error: 6.8660 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.81411\n",
      "Epoch 39/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2551 - mean_absolute_error: 4.2551 - acc: 0.6089 - val_loss: 6.8624 - val_mean_absolute_error: 6.8624 - val_acc: 0.5817\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.81411\n",
      "Epoch 40/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.1803 - mean_absolute_error: 4.1803 - acc: 0.6109 - val_loss: 6.8380 - val_mean_absolute_error: 6.8380 - val_acc: 0.5806\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.81411\n",
      "Epoch 41/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1755 - mean_absolute_error: 4.1755 - acc: 0.6125 - val_loss: 6.8798 - val_mean_absolute_error: 6.8798 - val_acc: 0.5689\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.81411\n",
      "Epoch 42/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.2098 - mean_absolute_error: 4.2098 - acc: 0.6100 - val_loss: 6.8046 - val_mean_absolute_error: 6.8046 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00042: val_loss improved from 6.81411 to 6.80457, saving model to Best_NN.h5\n",
      "Epoch 43/150\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.1978 - mean_absolute_error: 4.1978 - acc: 0.6103 - val_loss: 6.8393 - val_mean_absolute_error: 6.8393 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.80457\n",
      "Epoch 44/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.2153 - mean_absolute_error: 4.2153 - acc: 0.6124 - val_loss: 6.8201 - val_mean_absolute_error: 6.8201 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.80457\n",
      "Epoch 45/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.1929 - mean_absolute_error: 4.1929 - acc: 0.6100 - val_loss: 6.8717 - val_mean_absolute_error: 6.8717 - val_acc: 0.5637\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.80457\n",
      "Epoch 46/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1922 - mean_absolute_error: 4.1922 - acc: 0.6097 - val_loss: 6.8566 - val_mean_absolute_error: 6.8566 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.80457\n",
      "Epoch 47/150\n",
      "16766/16766 [==============================] - 1s 38us/step - loss: 4.1779 - mean_absolute_error: 4.1779 - acc: 0.6103 - val_loss: 6.8389 - val_mean_absolute_error: 6.8389 - val_acc: 0.5870\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.80457\n",
      "Epoch 48/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1526 - mean_absolute_error: 4.1526 - acc: 0.6133 - val_loss: 6.8258 - val_mean_absolute_error: 6.8258 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.80457\n",
      "Epoch 49/150\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.1737 - mean_absolute_error: 4.1737 - acc: 0.6097 - val_loss: 6.8278 - val_mean_absolute_error: 6.8278 - val_acc: 0.5939\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.80457\n",
      "Epoch 50/150\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1893 - mean_absolute_error: 4.1893 - acc: 0.6071 - val_loss: 6.8382 - val_mean_absolute_error: 6.8382 - val_acc: 0.5881\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.80457\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16767/16767 [==============================] - ETA: 0s - loss: 7.6879 - mean_absolute_error: 7.6879 - acc: 0.314 - 1s 53us/step - loss: 7.4308 - mean_absolute_error: 7.4308 - acc: 0.3293 - val_loss: 7.1660 - val_mean_absolute_error: 7.1660 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.80457\n",
      "Epoch 2/150\n",
      "16767/16767 [==============================] - 1s 41us/step - loss: 5.1612 - mean_absolute_error: 5.1612 - acc: 0.5299 - val_loss: 7.0851 - val_mean_absolute_error: 7.0851 - val_acc: 0.4201\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.80457\n",
      "Epoch 3/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.9068 - mean_absolute_error: 4.9068 - acc: 0.5754 - val_loss: 7.0987 - val_mean_absolute_error: 7.0987 - val_acc: 0.4272\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.80457\n",
      "Epoch 4/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.7823 - mean_absolute_error: 4.7823 - acc: 0.5796 - val_loss: 7.0619 - val_mean_absolute_error: 7.0619 - val_acc: 0.4498\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.80457\n",
      "Epoch 5/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.6722 - mean_absolute_error: 4.6722 - acc: 0.5928 - val_loss: 7.0016 - val_mean_absolute_error: 7.0016 - val_acc: 0.4722\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.80457\n",
      "Epoch 6/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.6901 - mean_absolute_error: 4.6901 - acc: 0.5937 - val_loss: 6.9244 - val_mean_absolute_error: 6.9244 - val_acc: 0.5181\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.80457\n",
      "Epoch 7/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.5475 - mean_absolute_error: 4.5475 - acc: 0.5983 - val_loss: 6.9361 - val_mean_absolute_error: 6.9361 - val_acc: 0.5095\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.80457\n",
      "Epoch 8/150\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.6301 - mean_absolute_error: 4.6301 - acc: 0.6054 - val_loss: 6.9444 - val_mean_absolute_error: 6.9444 - val_acc: 0.5169\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.80457\n",
      "Epoch 9/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.5943 - mean_absolute_error: 4.5943 - acc: 0.6041 - val_loss: 7.0027 - val_mean_absolute_error: 7.0027 - val_acc: 0.5011\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.80457\n",
      "Epoch 10/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4979 - mean_absolute_error: 4.4979 - acc: 0.6068 - val_loss: 7.0228 - val_mean_absolute_error: 7.0228 - val_acc: 0.5099\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.80457\n",
      "Epoch 11/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5270 - mean_absolute_error: 4.5270 - acc: 0.6097 - val_loss: 6.9852 - val_mean_absolute_error: 6.9852 - val_acc: 0.5259\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.80457\n",
      "Epoch 12/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4653 - mean_absolute_error: 4.4653 - acc: 0.6142 - val_loss: 6.9870 - val_mean_absolute_error: 6.9870 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.80457\n",
      "Epoch 13/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.4979 - mean_absolute_error: 4.4979 - acc: 0.6162 - val_loss: 6.9841 - val_mean_absolute_error: 6.9841 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.80457\n",
      "Epoch 14/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4457 - mean_absolute_error: 4.4457 - acc: 0.6158 - val_loss: 7.0183 - val_mean_absolute_error: 7.0183 - val_acc: 0.5477\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.80457\n",
      "Epoch 15/150\n",
      "16767/16767 [==============================] - 1s 41us/step - loss: 4.4858 - mean_absolute_error: 4.4858 - acc: 0.6127 - val_loss: 7.0387 - val_mean_absolute_error: 7.0387 - val_acc: 0.5448\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.80457\n",
      "Epoch 16/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3899 - mean_absolute_error: 4.3899 - acc: 0.6137 - val_loss: 7.0254 - val_mean_absolute_error: 7.0254 - val_acc: 0.5532\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.80457\n",
      "Epoch 17/150\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.4584 - mean_absolute_error: 4.4584 - acc: 0.6154 - val_loss: 7.0312 - val_mean_absolute_error: 7.0312 - val_acc: 0.5557\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.80457\n",
      "Epoch 18/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.4188 - mean_absolute_error: 4.4188 - acc: 0.6138 - val_loss: 7.0820 - val_mean_absolute_error: 7.0820 - val_acc: 0.5396\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.80457\n",
      "Epoch 19/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3763 - mean_absolute_error: 4.3763 - acc: 0.6184 - val_loss: 7.1139 - val_mean_absolute_error: 7.1139 - val_acc: 0.5415\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.80457\n",
      "Epoch 20/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3873 - mean_absolute_error: 4.3873 - acc: 0.6159 - val_loss: 7.0854 - val_mean_absolute_error: 7.0854 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.80457\n",
      "Epoch 21/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4220 - mean_absolute_error: 4.4220 - acc: 0.6119 - val_loss: 7.1425 - val_mean_absolute_error: 7.1425 - val_acc: 0.5493\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.80457\n",
      "Epoch 22/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3942 - mean_absolute_error: 4.3942 - acc: 0.6188 - val_loss: 7.2208 - val_mean_absolute_error: 7.2208 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.80457\n",
      "Epoch 23/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3461 - mean_absolute_error: 4.3461 - acc: 0.6130 - val_loss: 7.1389 - val_mean_absolute_error: 7.1389 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.80457\n",
      "Epoch 24/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3673 - mean_absolute_error: 4.3673 - acc: 0.6139 - val_loss: 7.2599 - val_mean_absolute_error: 7.2599 - val_acc: 0.5537\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.80457\n",
      "Epoch 25/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3840 - mean_absolute_error: 4.3840 - acc: 0.6150 - val_loss: 7.2775 - val_mean_absolute_error: 7.2775 - val_acc: 0.5689\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.80457\n",
      "Epoch 26/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3681 - mean_absolute_error: 4.3681 - acc: 0.6176 - val_loss: 7.6211 - val_mean_absolute_error: 7.6211 - val_acc: 0.5551\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.80457\n",
      "Epoch 27/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3673 - mean_absolute_error: 4.3673 - acc: 0.6208 - val_loss: 7.4286 - val_mean_absolute_error: 7.4286 - val_acc: 0.5599\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.80457\n",
      "Epoch 28/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3333 - mean_absolute_error: 4.3333 - acc: 0.6182 - val_loss: 7.5496 - val_mean_absolute_error: 7.5496 - val_acc: 0.5454\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.80457\n",
      "Epoch 29/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3501 - mean_absolute_error: 4.3501 - acc: 0.6159 - val_loss: 7.7105 - val_mean_absolute_error: 7.7105 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.80457\n",
      "Epoch 30/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3721 - mean_absolute_error: 4.3721 - acc: 0.6139 - val_loss: 7.6318 - val_mean_absolute_error: 7.6318 - val_acc: 0.5604\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.80457\n",
      "Epoch 31/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.3107 - mean_absolute_error: 4.3107 - acc: 0.6144 - val_loss: 7.4653 - val_mean_absolute_error: 7.4653 - val_acc: 0.5587\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.80457\n",
      "Epoch 32/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.2985 - mean_absolute_error: 4.2985 - acc: 0.6136 - val_loss: 7.7430 - val_mean_absolute_error: 7.7430 - val_acc: 0.5375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_loss did not improve from 6.80457\n",
      "Epoch 33/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3275 - mean_absolute_error: 4.3275 - acc: 0.6170 - val_loss: 7.6598 - val_mean_absolute_error: 7.6598 - val_acc: 0.5525\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.80457\n",
      "Epoch 34/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3182 - mean_absolute_error: 4.3182 - acc: 0.6167 - val_loss: 7.7247 - val_mean_absolute_error: 7.7247 - val_acc: 0.5612\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.80457\n",
      "Epoch 35/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2512 - mean_absolute_error: 4.2512 - acc: 0.6190 - val_loss: 7.7192 - val_mean_absolute_error: 7.7192 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.80457\n",
      "Epoch 36/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3080 - mean_absolute_error: 4.3080 - acc: 0.6181 - val_loss: 8.0102 - val_mean_absolute_error: 8.0102 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.80457\n",
      "Epoch 37/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3447 - mean_absolute_error: 4.3447 - acc: 0.6176 - val_loss: 7.9143 - val_mean_absolute_error: 7.9143 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.80457\n",
      "Epoch 38/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2329 - mean_absolute_error: 4.2329 - acc: 0.6192 - val_loss: 8.0545 - val_mean_absolute_error: 8.0545 - val_acc: 0.5562\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.80457\n",
      "Epoch 39/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2657 - mean_absolute_error: 4.2657 - acc: 0.6165 - val_loss: 7.8516 - val_mean_absolute_error: 7.8516 - val_acc: 0.5576\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.80457\n",
      "Epoch 40/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2862 - mean_absolute_error: 4.2862 - acc: 0.6171 - val_loss: 7.9117 - val_mean_absolute_error: 7.9117 - val_acc: 0.5474\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.80457\n",
      "Epoch 41/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2130 - mean_absolute_error: 4.2130 - acc: 0.6159 - val_loss: 8.5872 - val_mean_absolute_error: 8.5872 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.80457\n",
      "Epoch 42/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2500 - mean_absolute_error: 4.2500 - acc: 0.6172 - val_loss: 8.4098 - val_mean_absolute_error: 8.4098 - val_acc: 0.5332\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.80457\n",
      "Epoch 43/150\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.2235 - mean_absolute_error: 4.2235 - acc: 0.6183 - val_loss: 8.5231 - val_mean_absolute_error: 8.5231 - val_acc: 0.5447\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.80457\n",
      "Epoch 44/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2299 - mean_absolute_error: 4.2299 - acc: 0.6207 - val_loss: 8.1489 - val_mean_absolute_error: 8.1489 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.80457\n",
      "Epoch 45/150\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.3483 - mean_absolute_error: 4.3483 - acc: 0.6185 - val_loss: 8.1892 - val_mean_absolute_error: 8.1892 - val_acc: 0.5568\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.80457\n",
      "Epoch 46/150\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.1996 - mean_absolute_error: 4.1996 - acc: 0.6156 - val_loss: 8.9328 - val_mean_absolute_error: 8.9328 - val_acc: 0.5445\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.80457\n",
      "Epoch 47/150\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.2944 - mean_absolute_error: 4.2944 - acc: 0.6190 - val_loss: 8.3311 - val_mean_absolute_error: 8.3311 - val_acc: 0.5528\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.80457\n",
      "Epoch 48/150\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2292 - mean_absolute_error: 4.2292 - acc: 0.6133 - val_loss: 8.2122 - val_mean_absolute_error: 8.2122 - val_acc: 0.5621\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.80457\n",
      "Epoch 49/150\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.2029 - mean_absolute_error: 4.2029 - acc: 0.6203 - val_loss: 8.7941 - val_mean_absolute_error: 8.7941 - val_acc: 0.5629\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.80457\n",
      "Epoch 50/150\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.1996 - mean_absolute_error: 4.1996 - acc: 0.6168 - val_loss: 8.4805 - val_mean_absolute_error: 8.4805 - val_acc: 0.5641\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.80457\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 14us/step\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16766/16766 [==============================] - 1s 53us/step - loss: 7.6637 - mean_absolute_error: 7.6637 - acc: 0.2094 - val_loss: 7.1391 - val_mean_absolute_error: 7.1391 - val_acc: 0.4602\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.80457\n",
      "Epoch 2/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 5.4132 - mean_absolute_error: 5.4132 - acc: 0.4024 - val_loss: 7.1796 - val_mean_absolute_error: 7.1796 - val_acc: 0.4296\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.80457\n",
      "Epoch 3/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.9289 - mean_absolute_error: 4.9289 - acc: 0.4967 - val_loss: 7.1098 - val_mean_absolute_error: 7.1098 - val_acc: 0.4506\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.80457\n",
      "Epoch 4/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.7224 - mean_absolute_error: 4.7224 - acc: 0.5382 - val_loss: 7.0523 - val_mean_absolute_error: 7.0523 - val_acc: 0.4821\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.80457\n",
      "Epoch 5/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.6398 - mean_absolute_error: 4.6398 - acc: 0.5546 - val_loss: 7.0196 - val_mean_absolute_error: 7.0196 - val_acc: 0.4834\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.80457\n",
      "Epoch 6/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.6276 - mean_absolute_error: 4.6276 - acc: 0.5673 - val_loss: 7.0094 - val_mean_absolute_error: 7.0094 - val_acc: 0.4863\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.80457\n",
      "Epoch 7/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.6333 - mean_absolute_error: 4.6333 - acc: 0.5697 - val_loss: 6.9673 - val_mean_absolute_error: 6.9673 - val_acc: 0.4967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_loss did not improve from 6.80457\n",
      "Epoch 8/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.5159 - mean_absolute_error: 4.5159 - acc: 0.5842 - val_loss: 6.9406 - val_mean_absolute_error: 6.9406 - val_acc: 0.5165\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.80457\n",
      "Epoch 9/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4827 - mean_absolute_error: 4.4827 - acc: 0.5856 - val_loss: 6.9379 - val_mean_absolute_error: 6.9379 - val_acc: 0.5160\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.80457\n",
      "Epoch 10/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3935 - mean_absolute_error: 4.3935 - acc: 0.5884 - val_loss: 6.9205 - val_mean_absolute_error: 6.9205 - val_acc: 0.5153\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.80457\n",
      "Epoch 11/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4357 - mean_absolute_error: 4.4357 - acc: 0.5932 - val_loss: 6.9231 - val_mean_absolute_error: 6.9231 - val_acc: 0.5174\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.80457\n",
      "Epoch 12/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4192 - mean_absolute_error: 4.4192 - acc: 0.5958 - val_loss: 6.9367 - val_mean_absolute_error: 6.9367 - val_acc: 0.5100\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.80457\n",
      "Epoch 13/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3283 - mean_absolute_error: 4.3283 - acc: 0.6025 - val_loss: 6.8954 - val_mean_absolute_error: 6.8954 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.80457\n",
      "Epoch 14/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3509 - mean_absolute_error: 4.3509 - acc: 0.6048 - val_loss: 6.8813 - val_mean_absolute_error: 6.8813 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.80457\n",
      "Epoch 15/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3100 - mean_absolute_error: 4.3100 - acc: 0.6062 - val_loss: 6.8731 - val_mean_absolute_error: 6.8731 - val_acc: 0.5290\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.80457\n",
      "Epoch 16/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3714 - mean_absolute_error: 4.3714 - acc: 0.6088 - val_loss: 6.9213 - val_mean_absolute_error: 6.9213 - val_acc: 0.5281\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.80457\n",
      "Epoch 17/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.3443 - mean_absolute_error: 4.3443 - acc: 0.6068 - val_loss: 6.9138 - val_mean_absolute_error: 6.9138 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.80457\n",
      "Epoch 18/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2825 - mean_absolute_error: 4.2825 - acc: 0.6125 - val_loss: 6.8846 - val_mean_absolute_error: 6.8846 - val_acc: 0.5316\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.80457\n",
      "Epoch 19/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2795 - mean_absolute_error: 4.2795 - acc: 0.6106 - val_loss: 6.8751 - val_mean_absolute_error: 6.8751 - val_acc: 0.5443\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.80457\n",
      "Epoch 20/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2973 - mean_absolute_error: 4.2973 - acc: 0.6075 - val_loss: 6.8557 - val_mean_absolute_error: 6.8557 - val_acc: 0.5488\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.80457\n",
      "Epoch 21/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3501 - mean_absolute_error: 4.3501 - acc: 0.6145 - val_loss: 6.8747 - val_mean_absolute_error: 6.8747 - val_acc: 0.5448\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.80457\n",
      "Epoch 22/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2761 - mean_absolute_error: 4.2761 - acc: 0.6116 - val_loss: 6.8939 - val_mean_absolute_error: 6.8939 - val_acc: 0.5439\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.80457\n",
      "Epoch 23/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2923 - mean_absolute_error: 4.2923 - acc: 0.6115 - val_loss: 6.8651 - val_mean_absolute_error: 6.8651 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.80457\n",
      "Epoch 24/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3019 - mean_absolute_error: 4.3019 - acc: 0.6146 - val_loss: 6.8392 - val_mean_absolute_error: 6.8392 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.80457\n",
      "Epoch 25/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2690 - mean_absolute_error: 4.2690 - acc: 0.6130 - val_loss: 6.8932 - val_mean_absolute_error: 6.8932 - val_acc: 0.5443\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.80457\n",
      "Epoch 26/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2023 - mean_absolute_error: 4.2023 - acc: 0.6080 - val_loss: 6.8737 - val_mean_absolute_error: 6.8737 - val_acc: 0.5513\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.80457\n",
      "Epoch 27/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2709 - mean_absolute_error: 4.2709 - acc: 0.6114 - val_loss: 6.8454 - val_mean_absolute_error: 6.8454 - val_acc: 0.5626\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.80457\n",
      "Epoch 28/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2395 - mean_absolute_error: 4.2395 - acc: 0.6075 - val_loss: 6.8508 - val_mean_absolute_error: 6.8508 - val_acc: 0.5674\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.80457\n",
      "Epoch 29/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1983 - mean_absolute_error: 4.1983 - acc: 0.6093 - val_loss: 6.8465 - val_mean_absolute_error: 6.8465 - val_acc: 0.5719\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.80457\n",
      "Epoch 30/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2243 - mean_absolute_error: 4.2243 - acc: 0.6105 - val_loss: 6.8282 - val_mean_absolute_error: 6.8282 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.80457\n",
      "Epoch 31/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2548 - mean_absolute_error: 4.2548 - acc: 0.6080 - val_loss: 6.8048 - val_mean_absolute_error: 6.8048 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.80457\n",
      "Epoch 32/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2913 - mean_absolute_error: 4.2913 - acc: 0.6108 - val_loss: 6.8278 - val_mean_absolute_error: 6.8278 - val_acc: 0.5873\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.80457\n",
      "Epoch 33/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2861 - mean_absolute_error: 4.2861 - acc: 0.6084 - val_loss: 6.7878 - val_mean_absolute_error: 6.7878 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00033: val_loss improved from 6.80457 to 6.78779, saving model to Best_NN.h5\n",
      "Epoch 34/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2215 - mean_absolute_error: 4.2215 - acc: 0.6080 - val_loss: 6.7879 - val_mean_absolute_error: 6.7879 - val_acc: 0.5877\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.78779\n",
      "Epoch 35/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1843 - mean_absolute_error: 4.1843 - acc: 0.6117 - val_loss: 6.7862 - val_mean_absolute_error: 6.7862 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00035: val_loss improved from 6.78779 to 6.78623, saving model to Best_NN.h5\n",
      "Epoch 36/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2264 - mean_absolute_error: 4.2264 - acc: 0.6102 - val_loss: 6.8143 - val_mean_absolute_error: 6.8143 - val_acc: 0.5906\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.78623\n",
      "Epoch 37/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2341 - mean_absolute_error: 4.2341 - acc: 0.6093 - val_loss: 6.8166 - val_mean_absolute_error: 6.8166 - val_acc: 0.5866\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.78623\n",
      "Epoch 38/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1573 - mean_absolute_error: 4.1573 - acc: 0.6115 - val_loss: 6.8026 - val_mean_absolute_error: 6.8026 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.78623\n",
      "Epoch 39/150\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2076 - mean_absolute_error: 4.2076 - acc: 0.6106 - val_loss: 6.8152 - val_mean_absolute_error: 6.8152 - val_acc: 0.5918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_loss did not improve from 6.78623\n",
      "Epoch 40/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1869 - mean_absolute_error: 4.1869 - acc: 0.6115 - val_loss: 6.7956 - val_mean_absolute_error: 6.7956 - val_acc: 0.5963\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.78623\n",
      "Epoch 41/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2100 - mean_absolute_error: 4.2100 - acc: 0.6050 - val_loss: 6.8067 - val_mean_absolute_error: 6.8067 - val_acc: 0.5989\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.78623\n",
      "Epoch 42/150\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1517 - mean_absolute_error: 4.1517 - acc: 0.6133 - val_loss: 6.8014 - val_mean_absolute_error: 6.8014 - val_acc: 0.6000\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.78623\n",
      "Epoch 43/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1409 - mean_absolute_error: 4.1409 - acc: 0.6072 - val_loss: 6.7981 - val_mean_absolute_error: 6.7981 - val_acc: 0.6016\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.78623\n",
      "Epoch 44/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2022 - mean_absolute_error: 4.2022 - acc: 0.6091 - val_loss: 6.8106 - val_mean_absolute_error: 6.8106 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.78623\n",
      "Epoch 45/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1568 - mean_absolute_error: 4.1568 - acc: 0.6090 - val_loss: 6.8182 - val_mean_absolute_error: 6.8182 - val_acc: 0.6029\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.78623\n",
      "Epoch 46/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2150 - mean_absolute_error: 4.2150 - acc: 0.6133 - val_loss: 6.7950 - val_mean_absolute_error: 6.7950 - val_acc: 0.5967\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.78623\n",
      "Epoch 47/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1592 - mean_absolute_error: 4.1592 - acc: 0.6086 - val_loss: 6.8495 - val_mean_absolute_error: 6.8495 - val_acc: 0.6014\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.78623\n",
      "Epoch 48/150\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2290 - mean_absolute_error: 4.2290 - acc: 0.6111 - val_loss: 6.8061 - val_mean_absolute_error: 6.8061 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.78623\n",
      "Epoch 49/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1680 - mean_absolute_error: 4.1680 - acc: 0.6120 - val_loss: 6.7816 - val_mean_absolute_error: 6.7816 - val_acc: 0.5992\n",
      "\n",
      "Epoch 00049: val_loss improved from 6.78623 to 6.78160, saving model to Best_NN.h5\n",
      "Epoch 50/150\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1220 - mean_absolute_error: 4.1220 - acc: 0.6125 - val_loss: 6.7788 - val_mean_absolute_error: 6.7788 - val_acc: 0.6046\n",
      "\n",
      "Epoch 00050: val_loss improved from 6.78160 to 6.77880, saving model to Best_NN.h5\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "16767/16767 [==============================] - 1s 53us/step - loss: 7.2464 - mean_absolute_error: 7.2464 - acc: 0.2692 - val_loss: 7.0790 - val_mean_absolute_error: 7.0790 - val_acc: 0.5017\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.77880\n",
      "Epoch 2/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 5.3615 - mean_absolute_error: 5.3615 - acc: 0.4682 - val_loss: 7.0083 - val_mean_absolute_error: 7.0083 - val_acc: 0.5307\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.77880\n",
      "Epoch 3/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 5.0441 - mean_absolute_error: 5.0441 - acc: 0.5299 - val_loss: 7.0087 - val_mean_absolute_error: 7.0087 - val_acc: 0.5512\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.77880\n",
      "Epoch 4/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.8881 - mean_absolute_error: 4.8881 - acc: 0.5635 - val_loss: 6.9977 - val_mean_absolute_error: 6.9977 - val_acc: 0.5340\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.77880\n",
      "Epoch 5/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.7138 - mean_absolute_error: 4.7138 - acc: 0.5726 - val_loss: 6.9971 - val_mean_absolute_error: 6.9971 - val_acc: 0.5116\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.77880\n",
      "Epoch 6/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.7407 - mean_absolute_error: 4.7407 - acc: 0.5864 - val_loss: 6.9613 - val_mean_absolute_error: 6.9613 - val_acc: 0.5315\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.77880\n",
      "Epoch 7/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5679 - mean_absolute_error: 4.5679 - acc: 0.5916 - val_loss: 6.9615 - val_mean_absolute_error: 6.9615 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.77880\n",
      "Epoch 8/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.6264 - mean_absolute_error: 4.6264 - acc: 0.6002 - val_loss: 7.0017 - val_mean_absolute_error: 7.0017 - val_acc: 0.5332\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.77880\n",
      "Epoch 9/150\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5373 - mean_absolute_error: 4.5373 - acc: 0.6024 - val_loss: 6.9706 - val_mean_absolute_error: 6.9706 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.77880\n",
      "Epoch 10/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.5636 - mean_absolute_error: 4.5636 - acc: 0.6063 - val_loss: 6.9706 - val_mean_absolute_error: 6.9706 - val_acc: 0.5530\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.77880\n",
      "Epoch 11/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.5059 - mean_absolute_error: 4.5059 - acc: 0.6097 - val_loss: 6.9930 - val_mean_absolute_error: 6.9930 - val_acc: 0.5464\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.77880\n",
      "Epoch 12/150\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4553 - mean_absolute_error: 4.4553 - acc: 0.6087 - val_loss: 7.0115 - val_mean_absolute_error: 7.0115 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.77880\n",
      "Epoch 13/150\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.5121 - mean_absolute_error: 4.5121 - acc: 0.6094 - val_loss: 7.0366 - val_mean_absolute_error: 7.0366 - val_acc: 0.5482\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.77880\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4658 - mean_absolute_error: 4.4658 - acc: 0.6119 - val_loss: 7.0725 - val_mean_absolute_error: 7.0725 - val_acc: 0.5315\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.77880\n",
      "Epoch 15/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3989 - mean_absolute_error: 4.3989 - acc: 0.6123 - val_loss: 7.0634 - val_mean_absolute_error: 7.0634 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.77880\n",
      "Epoch 16/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4493 - mean_absolute_error: 4.4493 - acc: 0.6147 - val_loss: 7.0785 - val_mean_absolute_error: 7.0785 - val_acc: 0.5519\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.77880\n",
      "Epoch 17/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3812 - mean_absolute_error: 4.3812 - acc: 0.6112 - val_loss: 7.1639 - val_mean_absolute_error: 7.1639 - val_acc: 0.5315\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.77880\n",
      "Epoch 18/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4483 - mean_absolute_error: 4.4483 - acc: 0.6150 - val_loss: 7.1573 - val_mean_absolute_error: 7.1573 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.77880\n",
      "Epoch 19/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3895 - mean_absolute_error: 4.3895 - acc: 0.6138 - val_loss: 7.1924 - val_mean_absolute_error: 7.1924 - val_acc: 0.5518\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.77880\n",
      "Epoch 20/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4048 - mean_absolute_error: 4.4048 - acc: 0.6160 - val_loss: 7.1866 - val_mean_absolute_error: 7.1866 - val_acc: 0.5524\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.77880\n",
      "Epoch 21/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3747 - mean_absolute_error: 4.3747 - acc: 0.6150 - val_loss: 7.2110 - val_mean_absolute_error: 7.2110 - val_acc: 0.5604\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.77880\n",
      "Epoch 22/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4094 - mean_absolute_error: 4.4094 - acc: 0.6169 - val_loss: 7.1963 - val_mean_absolute_error: 7.1963 - val_acc: 0.5617\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.77880\n",
      "Epoch 23/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3509 - mean_absolute_error: 4.3509 - acc: 0.6180 - val_loss: 7.2520 - val_mean_absolute_error: 7.2520 - val_acc: 0.5658\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.77880\n",
      "Epoch 24/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3448 - mean_absolute_error: 4.3448 - acc: 0.6154 - val_loss: 7.3523 - val_mean_absolute_error: 7.3523 - val_acc: 0.5605\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.77880\n",
      "Epoch 25/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3299 - mean_absolute_error: 4.3299 - acc: 0.6161 - val_loss: 7.3081 - val_mean_absolute_error: 7.3081 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.77880\n",
      "Epoch 26/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.3238 - mean_absolute_error: 4.3238 - acc: 0.6119 - val_loss: 7.2949 - val_mean_absolute_error: 7.2949 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.77880\n",
      "Epoch 27/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3341 - mean_absolute_error: 4.3341 - acc: 0.6134 - val_loss: 7.2897 - val_mean_absolute_error: 7.2897 - val_acc: 0.5643\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.77880\n",
      "Epoch 28/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2979 - mean_absolute_error: 4.2979 - acc: 0.6137 - val_loss: 7.2703 - val_mean_absolute_error: 7.2703 - val_acc: 0.5693\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.77880\n",
      "Epoch 29/150\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.3269 - mean_absolute_error: 4.3269 - acc: 0.6096 - val_loss: 7.2581 - val_mean_absolute_error: 7.2581 - val_acc: 0.5686\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.77880\n",
      "Epoch 30/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3511 - mean_absolute_error: 4.3511 - acc: 0.6156 - val_loss: 7.2961 - val_mean_absolute_error: 7.2961 - val_acc: 0.5713\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.77880\n",
      "Epoch 31/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3024 - mean_absolute_error: 4.3024 - acc: 0.6166 - val_loss: 7.2920 - val_mean_absolute_error: 7.2920 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.77880\n",
      "Epoch 32/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2574 - mean_absolute_error: 4.2574 - acc: 0.6172 - val_loss: 7.3007 - val_mean_absolute_error: 7.3007 - val_acc: 0.5728\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.77880\n",
      "Epoch 33/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2769 - mean_absolute_error: 4.2769 - acc: 0.6176 - val_loss: 7.3358 - val_mean_absolute_error: 7.3358 - val_acc: 0.5730\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.77880\n",
      "Epoch 34/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2501 - mean_absolute_error: 4.2501 - acc: 0.6159 - val_loss: 7.6441 - val_mean_absolute_error: 7.6441 - val_acc: 0.5673\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.77880\n",
      "Epoch 35/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2981 - mean_absolute_error: 4.2981 - acc: 0.6176 - val_loss: 7.4979 - val_mean_absolute_error: 7.4979 - val_acc: 0.5697\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.77880\n",
      "Epoch 36/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2616 - mean_absolute_error: 4.2616 - acc: 0.6183 - val_loss: 7.4018 - val_mean_absolute_error: 7.4018 - val_acc: 0.5742\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.77880\n",
      "Epoch 37/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2375 - mean_absolute_error: 4.2375 - acc: 0.6159 - val_loss: 7.4830 - val_mean_absolute_error: 7.4830 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.77880\n",
      "Epoch 38/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2111 - mean_absolute_error: 4.2111 - acc: 0.6156 - val_loss: 7.2995 - val_mean_absolute_error: 7.2995 - val_acc: 0.5796\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.77880\n",
      "Epoch 39/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2099 - mean_absolute_error: 4.2099 - acc: 0.6167 - val_loss: 7.5448 - val_mean_absolute_error: 7.5448 - val_acc: 0.5762\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.77880\n",
      "Epoch 40/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2977 - mean_absolute_error: 4.2977 - acc: 0.6192 - val_loss: 7.5298 - val_mean_absolute_error: 7.5298 - val_acc: 0.5743\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.77880\n",
      "Epoch 41/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2764 - mean_absolute_error: 4.2764 - acc: 0.6156 - val_loss: 7.5037 - val_mean_absolute_error: 7.5037 - val_acc: 0.5741\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.77880\n",
      "Epoch 42/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2843 - mean_absolute_error: 4.2843 - acc: 0.6143 - val_loss: 7.5624 - val_mean_absolute_error: 7.5624 - val_acc: 0.5720\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.77880\n",
      "Epoch 43/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2510 - mean_absolute_error: 4.2510 - acc: 0.6160 - val_loss: 7.5355 - val_mean_absolute_error: 7.5355 - val_acc: 0.5774\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.77880\n",
      "Epoch 44/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2330 - mean_absolute_error: 4.2330 - acc: 0.6167 - val_loss: 7.5174 - val_mean_absolute_error: 7.5174 - val_acc: 0.5755\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.77880\n",
      "Epoch 45/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2141 - mean_absolute_error: 4.2141 - acc: 0.6191 - val_loss: 7.5875 - val_mean_absolute_error: 7.5875 - val_acc: 0.5773\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.77880\n",
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2745 - mean_absolute_error: 4.2745 - acc: 0.6209 - val_loss: 7.4822 - val_mean_absolute_error: 7.4822 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.77880\n",
      "Epoch 47/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2847 - mean_absolute_error: 4.2847 - acc: 0.6197 - val_loss: 7.5842 - val_mean_absolute_error: 7.5842 - val_acc: 0.5740\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.77880\n",
      "Epoch 48/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2590 - mean_absolute_error: 4.2590 - acc: 0.6203 - val_loss: 7.6123 - val_mean_absolute_error: 7.6123 - val_acc: 0.5687\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.77880\n",
      "Epoch 49/150\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2053 - mean_absolute_error: 4.2053 - acc: 0.6176 - val_loss: 7.6860 - val_mean_absolute_error: 7.6860 - val_acc: 0.5693\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.77880\n",
      "Epoch 50/150\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.1683 - mean_absolute_error: 4.1683 - acc: 0.6148 - val_loss: 7.9181 - val_mean_absolute_error: 7.9181 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.77880\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 12us/step\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16766/16766 [==============================] - 1s 53us/step - loss: 6.3386 - mean_absolute_error: 6.3386 - acc: 0.3389 - val_loss: 7.0938 - val_mean_absolute_error: 7.0938 - val_acc: 0.5110\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.77880\n",
      "Epoch 2/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.9282 - mean_absolute_error: 4.9282 - acc: 0.5142 - val_loss: 7.0813 - val_mean_absolute_error: 7.0813 - val_acc: 0.4559\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.77880\n",
      "Epoch 3/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.5991 - mean_absolute_error: 4.5991 - acc: 0.5546 - val_loss: 7.0360 - val_mean_absolute_error: 7.0360 - val_acc: 0.4594\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.77880\n",
      "Epoch 4/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.6017 - mean_absolute_error: 4.6017 - acc: 0.5672 - val_loss: 6.9815 - val_mean_absolute_error: 6.9815 - val_acc: 0.4850\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.77880\n",
      "Epoch 5/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.5792 - mean_absolute_error: 4.5792 - acc: 0.5818 - val_loss: 7.0091 - val_mean_absolute_error: 7.0091 - val_acc: 0.4645\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.77880\n",
      "Epoch 6/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4962 - mean_absolute_error: 4.4962 - acc: 0.5948 - val_loss: 7.0099 - val_mean_absolute_error: 7.0099 - val_acc: 0.4516\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.77880\n",
      "Epoch 7/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.5096 - mean_absolute_error: 4.5096 - acc: 0.6035 - val_loss: 6.9577 - val_mean_absolute_error: 6.9577 - val_acc: 0.4325\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.77880\n",
      "Epoch 8/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.4110 - mean_absolute_error: 4.4110 - acc: 0.6065 - val_loss: 6.9859 - val_mean_absolute_error: 6.9859 - val_acc: 0.4803\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.77880\n",
      "Epoch 9/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3954 - mean_absolute_error: 4.3954 - acc: 0.6052 - val_loss: 6.9779 - val_mean_absolute_error: 6.9779 - val_acc: 0.4722\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.77880\n",
      "Epoch 10/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.4374 - mean_absolute_error: 4.4374 - acc: 0.6097 - val_loss: 6.9259 - val_mean_absolute_error: 6.9259 - val_acc: 0.5140\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.77880\n",
      "Epoch 11/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.4007 - mean_absolute_error: 4.4007 - acc: 0.6086 - val_loss: 6.9112 - val_mean_absolute_error: 6.9112 - val_acc: 0.5527\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.77880\n",
      "Epoch 12/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.4008 - mean_absolute_error: 4.4008 - acc: 0.6112 - val_loss: 6.8290 - val_mean_absolute_error: 6.8290 - val_acc: 0.5639\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.77880\n",
      "Epoch 13/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3752 - mean_absolute_error: 4.3752 - acc: 0.6125 - val_loss: 6.8154 - val_mean_absolute_error: 6.8154 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.77880\n",
      "Epoch 14/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3850 - mean_absolute_error: 4.3850 - acc: 0.6080 - val_loss: 6.8104 - val_mean_absolute_error: 6.8104 - val_acc: 0.5859\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.77880\n",
      "Epoch 15/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3065 - mean_absolute_error: 4.3065 - acc: 0.6146 - val_loss: 6.8401 - val_mean_absolute_error: 6.8401 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.77880\n",
      "Epoch 16/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3170 - mean_absolute_error: 4.3170 - acc: 0.6091 - val_loss: 6.8273 - val_mean_absolute_error: 6.8273 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.77880\n",
      "Epoch 17/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3232 - mean_absolute_error: 4.3232 - acc: 0.6107 - val_loss: 6.8166 - val_mean_absolute_error: 6.8166 - val_acc: 0.5791\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.77880\n",
      "Epoch 18/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.3015 - mean_absolute_error: 4.3015 - acc: 0.6136 - val_loss: 6.8300 - val_mean_absolute_error: 6.8300 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.77880\n",
      "Epoch 19/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3043 - mean_absolute_error: 4.3043 - acc: 0.6105 - val_loss: 6.8083 - val_mean_absolute_error: 6.8083 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.77880\n",
      "Epoch 20/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3369 - mean_absolute_error: 4.3369 - acc: 0.6090 - val_loss: 6.7923 - val_mean_absolute_error: 6.7923 - val_acc: 0.5995\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.77880\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2944 - mean_absolute_error: 4.2944 - acc: 0.6109 - val_loss: 6.7953 - val_mean_absolute_error: 6.7953 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.77880\n",
      "Epoch 22/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2423 - mean_absolute_error: 4.2423 - acc: 0.6097 - val_loss: 6.7992 - val_mean_absolute_error: 6.7992 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.77880\n",
      "Epoch 23/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2867 - mean_absolute_error: 4.2867 - acc: 0.6100 - val_loss: 6.8458 - val_mean_absolute_error: 6.8458 - val_acc: 0.6125\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.77880\n",
      "Epoch 24/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.2515 - mean_absolute_error: 4.2515 - acc: 0.6096 - val_loss: 6.7858 - val_mean_absolute_error: 6.7858 - val_acc: 0.5910\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.77880\n",
      "Epoch 25/100\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.2679 - mean_absolute_error: 4.2679 - acc: 0.6089 - val_loss: 6.8304 - val_mean_absolute_error: 6.8304 - val_acc: 0.6126\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.77880\n",
      "Epoch 26/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3093 - mean_absolute_error: 4.3093 - acc: 0.6114 - val_loss: 6.7458 - val_mean_absolute_error: 6.7458 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00026: val_loss improved from 6.77880 to 6.74584, saving model to Best_NN.h5\n",
      "Epoch 27/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2968 - mean_absolute_error: 4.2968 - acc: 0.6116 - val_loss: 6.8258 - val_mean_absolute_error: 6.8258 - val_acc: 0.5742\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.74584\n",
      "Epoch 28/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2339 - mean_absolute_error: 4.2339 - acc: 0.6083 - val_loss: 6.8403 - val_mean_absolute_error: 6.8403 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.74584\n",
      "Epoch 29/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2629 - mean_absolute_error: 4.2629 - acc: 0.6109 - val_loss: 6.8483 - val_mean_absolute_error: 6.8483 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.74584\n",
      "Epoch 30/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3047 - mean_absolute_error: 4.3047 - acc: 0.6117 - val_loss: 6.7913 - val_mean_absolute_error: 6.7913 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.74584\n",
      "Epoch 31/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.3122 - mean_absolute_error: 4.3122 - acc: 0.6108 - val_loss: 6.8086 - val_mean_absolute_error: 6.8086 - val_acc: 0.5821\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.74584\n",
      "Epoch 32/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1911 - mean_absolute_error: 4.1911 - acc: 0.6078 - val_loss: 6.8519 - val_mean_absolute_error: 6.8519 - val_acc: 0.6022\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.74584\n",
      "Epoch 33/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2716 - mean_absolute_error: 4.2716 - acc: 0.6083 - val_loss: 6.8027 - val_mean_absolute_error: 6.8027 - val_acc: 0.5939\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.74584\n",
      "Epoch 34/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2716 - mean_absolute_error: 4.2716 - acc: 0.6104 - val_loss: 6.8394 - val_mean_absolute_error: 6.8394 - val_acc: 0.5949\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.74584\n",
      "Epoch 35/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2638 - mean_absolute_error: 4.2638 - acc: 0.6100 - val_loss: 6.8250 - val_mean_absolute_error: 6.8250 - val_acc: 0.6051\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.74584\n",
      "Epoch 36/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2446 - mean_absolute_error: 4.2446 - acc: 0.6114 - val_loss: 6.8444 - val_mean_absolute_error: 6.8444 - val_acc: 0.5868\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.74584\n",
      "Epoch 37/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2120 - mean_absolute_error: 4.2120 - acc: 0.6130 - val_loss: 6.8874 - val_mean_absolute_error: 6.8874 - val_acc: 0.5877\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.74584\n",
      "Epoch 38/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2526 - mean_absolute_error: 4.2526 - acc: 0.6102 - val_loss: 6.8864 - val_mean_absolute_error: 6.8864 - val_acc: 0.5917\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.74584\n",
      "Epoch 39/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2318 - mean_absolute_error: 4.2318 - acc: 0.6096 - val_loss: 6.9064 - val_mean_absolute_error: 6.9064 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.74584\n",
      "Epoch 40/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2008 - mean_absolute_error: 4.2008 - acc: 0.6118 - val_loss: 6.8960 - val_mean_absolute_error: 6.8960 - val_acc: 0.6032\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.74584\n",
      "Epoch 41/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.1972 - mean_absolute_error: 4.1972 - acc: 0.6087 - val_loss: 6.8945 - val_mean_absolute_error: 6.8945 - val_acc: 0.5846\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.74584\n",
      "Epoch 42/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1545 - mean_absolute_error: 4.1545 - acc: 0.6097 - val_loss: 6.9610 - val_mean_absolute_error: 6.9610 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.74584\n",
      "Epoch 43/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1848 - mean_absolute_error: 4.1848 - acc: 0.6079 - val_loss: 6.9497 - val_mean_absolute_error: 6.9497 - val_acc: 0.5839\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.74584\n",
      "Epoch 44/100\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.1654 - mean_absolute_error: 4.1654 - acc: 0.6083 - val_loss: 6.8997 - val_mean_absolute_error: 6.8997 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.74584\n",
      "Epoch 45/100\n",
      "16766/16766 [==============================] - 1s 36us/step - loss: 4.1705 - mean_absolute_error: 4.1705 - acc: 0.6112 - val_loss: 6.8992 - val_mean_absolute_error: 6.8992 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.74584\n",
      "Epoch 46/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1571 - mean_absolute_error: 4.1571 - acc: 0.6055 - val_loss: 6.9004 - val_mean_absolute_error: 6.9004 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.74584\n",
      "Epoch 47/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1775 - mean_absolute_error: 4.1775 - acc: 0.6107 - val_loss: 6.8792 - val_mean_absolute_error: 6.8792 - val_acc: 0.5988\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.74584\n",
      "Epoch 48/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2215 - mean_absolute_error: 4.2215 - acc: 0.6108 - val_loss: 6.8566 - val_mean_absolute_error: 6.8566 - val_acc: 0.5942\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.74584\n",
      "Epoch 49/100\n",
      "16766/16766 [==============================] - 1s 37us/step - loss: 4.2208 - mean_absolute_error: 4.2208 - acc: 0.6128 - val_loss: 6.8293 - val_mean_absolute_error: 6.8293 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.74584\n",
      "Epoch 50/100\n",
      "16766/16766 [==============================] - 1s 39us/step - loss: 4.1868 - mean_absolute_error: 4.1868 - acc: 0.6121 - val_loss: 6.8948 - val_mean_absolute_error: 6.8948 - val_acc: 0.5995\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.74584\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 13us/step\n",
      "16766/16766 [==============================] - 0s 12us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16767/16767 [==============================] - 1s 58us/step - loss: 6.2520 - mean_absolute_error: 6.2520 - acc: 0.3202 - val_loss: 7.1885 - val_mean_absolute_error: 7.1885 - val_acc: 0.4049\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.74584\n",
      "Epoch 2/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.9356 - mean_absolute_error: 4.9356 - acc: 0.5105 - val_loss: 7.0218 - val_mean_absolute_error: 7.0218 - val_acc: 0.4292\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.74584\n",
      "Epoch 3/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.7119 - mean_absolute_error: 4.7119 - acc: 0.5527 - val_loss: 6.9492 - val_mean_absolute_error: 6.9492 - val_acc: 0.4466\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.74584\n",
      "Epoch 4/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.6518 - mean_absolute_error: 4.6518 - acc: 0.5753 - val_loss: 6.9478 - val_mean_absolute_error: 6.9478 - val_acc: 0.4970\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.74584\n",
      "Epoch 5/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.6128 - mean_absolute_error: 4.6128 - acc: 0.5874 - val_loss: 7.0121 - val_mean_absolute_error: 7.0121 - val_acc: 0.4395\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.74584\n",
      "Epoch 6/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.5723 - mean_absolute_error: 4.5723 - acc: 0.5918 - val_loss: 7.0011 - val_mean_absolute_error: 7.0011 - val_acc: 0.4717\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.74584\n",
      "Epoch 7/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5564 - mean_absolute_error: 4.5564 - acc: 0.6048 - val_loss: 7.0356 - val_mean_absolute_error: 7.0356 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.74584\n",
      "Epoch 8/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5354 - mean_absolute_error: 4.5354 - acc: 0.6016 - val_loss: 7.0756 - val_mean_absolute_error: 7.0756 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.74584\n",
      "Epoch 9/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5323 - mean_absolute_error: 4.5323 - acc: 0.6076 - val_loss: 7.1465 - val_mean_absolute_error: 7.1465 - val_acc: 0.4744\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.74584\n",
      "Epoch 10/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4492 - mean_absolute_error: 4.4492 - acc: 0.6119 - val_loss: 7.0587 - val_mean_absolute_error: 7.0587 - val_acc: 0.5197\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.74584\n",
      "Epoch 11/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4296 - mean_absolute_error: 4.4296 - acc: 0.6148 - val_loss: 7.1750 - val_mean_absolute_error: 7.1750 - val_acc: 0.4940\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.74584\n",
      "Epoch 12/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4260 - mean_absolute_error: 4.4260 - acc: 0.6144 - val_loss: 7.2649 - val_mean_absolute_error: 7.2649 - val_acc: 0.4769\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.74584\n",
      "Epoch 13/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.4443 - mean_absolute_error: 4.4443 - acc: 0.6110 - val_loss: 7.2219 - val_mean_absolute_error: 7.2219 - val_acc: 0.5233\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.74584\n",
      "Epoch 14/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4000 - mean_absolute_error: 4.4000 - acc: 0.6116 - val_loss: 7.1499 - val_mean_absolute_error: 7.1499 - val_acc: 0.5396\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.74584\n",
      "Epoch 15/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4087 - mean_absolute_error: 4.4087 - acc: 0.6115 - val_loss: 7.3650 - val_mean_absolute_error: 7.3650 - val_acc: 0.5394\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.74584\n",
      "Epoch 16/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3359 - mean_absolute_error: 4.3359 - acc: 0.6140 - val_loss: 7.3355 - val_mean_absolute_error: 7.3355 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.74584\n",
      "Epoch 17/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3551 - mean_absolute_error: 4.3551 - acc: 0.6149 - val_loss: 7.2684 - val_mean_absolute_error: 7.2684 - val_acc: 0.5580\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.74584\n",
      "Epoch 18/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3715 - mean_absolute_error: 4.3715 - acc: 0.6117 - val_loss: 7.3214 - val_mean_absolute_error: 7.3214 - val_acc: 0.5669\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.74584\n",
      "Epoch 19/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3506 - mean_absolute_error: 4.3506 - acc: 0.6162 - val_loss: 7.2234 - val_mean_absolute_error: 7.2234 - val_acc: 0.5703\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.74584\n",
      "Epoch 20/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4097 - mean_absolute_error: 4.4097 - acc: 0.6173 - val_loss: 7.2416 - val_mean_absolute_error: 7.2416 - val_acc: 0.5766\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.74584\n",
      "Epoch 21/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3958 - mean_absolute_error: 4.3958 - acc: 0.6154 - val_loss: 7.2590 - val_mean_absolute_error: 7.2590 - val_acc: 0.5766\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.74584\n",
      "Epoch 22/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3206 - mean_absolute_error: 4.3206 - acc: 0.6170 - val_loss: 7.2718 - val_mean_absolute_error: 7.2718 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.74584\n",
      "Epoch 23/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3381 - mean_absolute_error: 4.3381 - acc: 0.6147 - val_loss: 7.3091 - val_mean_absolute_error: 7.3091 - val_acc: 0.5725\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.74584\n",
      "Epoch 24/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3207 - mean_absolute_error: 4.3207 - acc: 0.6167 - val_loss: 7.4401 - val_mean_absolute_error: 7.4401 - val_acc: 0.5697\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.74584\n",
      "Epoch 25/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3591 - mean_absolute_error: 4.3591 - acc: 0.6203 - val_loss: 7.5967 - val_mean_absolute_error: 7.5967 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.74584\n",
      "Epoch 26/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3610 - mean_absolute_error: 4.3610 - acc: 0.6193 - val_loss: 7.5875 - val_mean_absolute_error: 7.5875 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.74584\n",
      "Epoch 27/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3725 - mean_absolute_error: 4.3725 - acc: 0.6164 - val_loss: 7.6816 - val_mean_absolute_error: 7.6816 - val_acc: 0.5679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_loss did not improve from 6.74584\n",
      "Epoch 28/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2989 - mean_absolute_error: 4.2989 - acc: 0.6153 - val_loss: 7.6872 - val_mean_absolute_error: 7.6872 - val_acc: 0.5701\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.74584\n",
      "Epoch 29/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3687 - mean_absolute_error: 4.3687 - acc: 0.6167 - val_loss: 7.2464 - val_mean_absolute_error: 7.2464 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.74584\n",
      "Epoch 30/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3010 - mean_absolute_error: 4.3010 - acc: 0.6182 - val_loss: 7.4201 - val_mean_absolute_error: 7.4201 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.74584\n",
      "Epoch 31/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3386 - mean_absolute_error: 4.3386 - acc: 0.6190 - val_loss: 7.2986 - val_mean_absolute_error: 7.2986 - val_acc: 0.5841\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.74584\n",
      "Epoch 32/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2454 - mean_absolute_error: 4.2454 - acc: 0.6182 - val_loss: 7.4875 - val_mean_absolute_error: 7.4875 - val_acc: 0.5798\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.74584\n",
      "Epoch 33/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2810 - mean_absolute_error: 4.2810 - acc: 0.6182 - val_loss: 7.3867 - val_mean_absolute_error: 7.3867 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.74584\n",
      "Epoch 34/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3556 - mean_absolute_error: 4.3556 - acc: 0.6178 - val_loss: 7.3438 - val_mean_absolute_error: 7.3438 - val_acc: 0.5854\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.74584\n",
      "Epoch 35/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2726 - mean_absolute_error: 4.2726 - acc: 0.6199 - val_loss: 7.5565 - val_mean_absolute_error: 7.5565 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.74584\n",
      "Epoch 36/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2667 - mean_absolute_error: 4.2667 - acc: 0.6182 - val_loss: 7.2528 - val_mean_absolute_error: 7.2528 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.74584\n",
      "Epoch 37/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2657 - mean_absolute_error: 4.2657 - acc: 0.6200 - val_loss: 7.4613 - val_mean_absolute_error: 7.4613 - val_acc: 0.5923\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.74584\n",
      "Epoch 38/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2918 - mean_absolute_error: 4.2918 - acc: 0.6196 - val_loss: 7.5237 - val_mean_absolute_error: 7.5237 - val_acc: 0.5895\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.74584\n",
      "Epoch 39/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.3229 - mean_absolute_error: 4.3229 - acc: 0.6158 - val_loss: 7.5235 - val_mean_absolute_error: 7.5235 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.74584\n",
      "Epoch 40/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2082 - mean_absolute_error: 4.2082 - acc: 0.6173 - val_loss: 7.6809 - val_mean_absolute_error: 7.6809 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.74584\n",
      "Epoch 41/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2476 - mean_absolute_error: 4.2476 - acc: 0.6155 - val_loss: 7.5765 - val_mean_absolute_error: 7.5765 - val_acc: 0.5918\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.74584\n",
      "Epoch 42/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2336 - mean_absolute_error: 4.2336 - acc: 0.6188 - val_loss: 7.5159 - val_mean_absolute_error: 7.5159 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.74584\n",
      "Epoch 43/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.3034 - mean_absolute_error: 4.3034 - acc: 0.6194 - val_loss: 7.6939 - val_mean_absolute_error: 7.6939 - val_acc: 0.5929\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.74584\n",
      "Epoch 44/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2224 - mean_absolute_error: 4.2224 - acc: 0.6179 - val_loss: 7.6177 - val_mean_absolute_error: 7.6177 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.74584\n",
      "Epoch 45/100\n",
      "16767/16767 [==============================] - ETA: 0s - loss: 4.3098 - mean_absolute_error: 4.3098 - acc: 0.620 - 1s 34us/step - loss: 4.2630 - mean_absolute_error: 4.2630 - acc: 0.6185 - val_loss: 7.7380 - val_mean_absolute_error: 7.7380 - val_acc: 0.5933\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.74584\n",
      "Epoch 46/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.1745 - mean_absolute_error: 4.1745 - acc: 0.6194 - val_loss: 7.8368 - val_mean_absolute_error: 7.8368 - val_acc: 0.5916\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.74584\n",
      "Epoch 47/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.1514 - mean_absolute_error: 4.1514 - acc: 0.6232 - val_loss: 7.7634 - val_mean_absolute_error: 7.7634 - val_acc: 0.5927\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.74584\n",
      "Epoch 48/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2621 - mean_absolute_error: 4.2621 - acc: 0.6205 - val_loss: 7.6016 - val_mean_absolute_error: 7.6016 - val_acc: 0.5943\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.74584\n",
      "Epoch 49/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.1773 - mean_absolute_error: 4.1773 - acc: 0.6213 - val_loss: 7.7903 - val_mean_absolute_error: 7.7903 - val_acc: 0.5934\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.74584\n",
      "Epoch 50/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.1773 - mean_absolute_error: 4.1773 - acc: 0.6193 - val_loss: 7.9901 - val_mean_absolute_error: 7.9901 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.74584\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 12us/step\n",
      "16767/16767 [==============================] - 0s 12us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16766 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16766/16766 [==============================] - 1s 60us/step - loss: 6.4884 - mean_absolute_error: 6.4884 - acc: 0.2865 - val_loss: 7.2288 - val_mean_absolute_error: 7.2288 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.74584\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 35us/step - loss: 5.3644 - mean_absolute_error: 5.3644 - acc: 0.4469 - val_loss: 7.1739 - val_mean_absolute_error: 7.1739 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.74584\n",
      "Epoch 3/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 5.0329 - mean_absolute_error: 5.0329 - acc: 0.5268 - val_loss: 7.1158 - val_mean_absolute_error: 7.1158 - val_acc: 0.4467\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.74584\n",
      "Epoch 4/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.8204 - mean_absolute_error: 4.8204 - acc: 0.5570 - val_loss: 7.0194 - val_mean_absolute_error: 7.0194 - val_acc: 0.4789\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.74584\n",
      "Epoch 5/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.6553 - mean_absolute_error: 4.6553 - acc: 0.5770 - val_loss: 6.9951 - val_mean_absolute_error: 6.9951 - val_acc: 0.4918\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.74584\n",
      "Epoch 6/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.6202 - mean_absolute_error: 4.6202 - acc: 0.5879 - val_loss: 6.9756 - val_mean_absolute_error: 6.9756 - val_acc: 0.5041\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.74584\n",
      "Epoch 7/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.6053 - mean_absolute_error: 4.6053 - acc: 0.5953 - val_loss: 6.9333 - val_mean_absolute_error: 6.9333 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.74584\n",
      "Epoch 8/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.5419 - mean_absolute_error: 4.5419 - acc: 0.5957 - val_loss: 6.9222 - val_mean_absolute_error: 6.9222 - val_acc: 0.5311\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.74584\n",
      "Epoch 9/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4777 - mean_absolute_error: 4.4777 - acc: 0.5970 - val_loss: 6.9220 - val_mean_absolute_error: 6.9220 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.74584\n",
      "Epoch 10/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.4862 - mean_absolute_error: 4.4862 - acc: 0.6047 - val_loss: 6.9166 - val_mean_absolute_error: 6.9166 - val_acc: 0.5409\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.74584\n",
      "Epoch 11/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4027 - mean_absolute_error: 4.4027 - acc: 0.6034 - val_loss: 6.8993 - val_mean_absolute_error: 6.8993 - val_acc: 0.5379\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.74584\n",
      "Epoch 12/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.4316 - mean_absolute_error: 4.4316 - acc: 0.6084 - val_loss: 6.9324 - val_mean_absolute_error: 6.9324 - val_acc: 0.5262\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.74584\n",
      "Epoch 13/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3737 - mean_absolute_error: 4.3737 - acc: 0.6074 - val_loss: 6.9179 - val_mean_absolute_error: 6.9179 - val_acc: 0.5292\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.74584\n",
      "Epoch 14/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3252 - mean_absolute_error: 4.3252 - acc: 0.6075 - val_loss: 6.8968 - val_mean_absolute_error: 6.8968 - val_acc: 0.5396\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.74584\n",
      "Epoch 15/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3202 - mean_absolute_error: 4.3202 - acc: 0.6084 - val_loss: 6.9125 - val_mean_absolute_error: 6.9125 - val_acc: 0.5359\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.74584\n",
      "Epoch 16/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3985 - mean_absolute_error: 4.3985 - acc: 0.6083 - val_loss: 6.9042 - val_mean_absolute_error: 6.9042 - val_acc: 0.5267\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.74584\n",
      "Epoch 17/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3471 - mean_absolute_error: 4.3471 - acc: 0.6092 - val_loss: 6.8836 - val_mean_absolute_error: 6.8836 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.74584\n",
      "Epoch 18/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3551 - mean_absolute_error: 4.3551 - acc: 0.6112 - val_loss: 6.9084 - val_mean_absolute_error: 6.9084 - val_acc: 0.5425\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.74584\n",
      "Epoch 19/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2565 - mean_absolute_error: 4.2565 - acc: 0.6103 - val_loss: 6.9263 - val_mean_absolute_error: 6.9263 - val_acc: 0.5359\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.74584\n",
      "Epoch 20/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3053 - mean_absolute_error: 4.3053 - acc: 0.6104 - val_loss: 6.8888 - val_mean_absolute_error: 6.8888 - val_acc: 0.5507\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.74584\n",
      "Epoch 21/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.3181 - mean_absolute_error: 4.3181 - acc: 0.6103 - val_loss: 6.9020 - val_mean_absolute_error: 6.9020 - val_acc: 0.5453\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.74584\n",
      "Epoch 22/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2732 - mean_absolute_error: 4.2732 - acc: 0.6116 - val_loss: 6.8923 - val_mean_absolute_error: 6.8923 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.74584\n",
      "Epoch 23/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2322 - mean_absolute_error: 4.2322 - acc: 0.6130 - val_loss: 6.8976 - val_mean_absolute_error: 6.8976 - val_acc: 0.5428\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.74584\n",
      "Epoch 24/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2746 - mean_absolute_error: 4.2746 - acc: 0.6097 - val_loss: 6.9262 - val_mean_absolute_error: 6.9262 - val_acc: 0.5314\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.74584\n",
      "Epoch 25/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2591 - mean_absolute_error: 4.2591 - acc: 0.6108 - val_loss: 6.8893 - val_mean_absolute_error: 6.8893 - val_acc: 0.5533\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.74584\n",
      "Epoch 26/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2806 - mean_absolute_error: 4.2806 - acc: 0.6105 - val_loss: 6.9040 - val_mean_absolute_error: 6.9040 - val_acc: 0.5488\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.74584\n",
      "Epoch 27/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.3327 - mean_absolute_error: 4.3327 - acc: 0.6089 - val_loss: 6.8947 - val_mean_absolute_error: 6.8947 - val_acc: 0.5575\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.74584\n",
      "Epoch 28/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.2263 - mean_absolute_error: 4.2263 - acc: 0.6105 - val_loss: 6.8975 - val_mean_absolute_error: 6.8975 - val_acc: 0.5538\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.74584\n",
      "Epoch 29/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2683 - mean_absolute_error: 4.2683 - acc: 0.6137 - val_loss: 6.9090 - val_mean_absolute_error: 6.9090 - val_acc: 0.5569\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.74584\n",
      "Epoch 30/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2247 - mean_absolute_error: 4.2247 - acc: 0.6107 - val_loss: 6.9128 - val_mean_absolute_error: 6.9128 - val_acc: 0.5552\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.74584\n",
      "Epoch 31/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2456 - mean_absolute_error: 4.2456 - acc: 0.6118 - val_loss: 6.9043 - val_mean_absolute_error: 6.9043 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.74584\n",
      "Epoch 32/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2593 - mean_absolute_error: 4.2593 - acc: 0.6095 - val_loss: 6.9004 - val_mean_absolute_error: 6.9004 - val_acc: 0.5531\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.74584\n",
      "Epoch 33/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2557 - mean_absolute_error: 4.2557 - acc: 0.6137 - val_loss: 6.9473 - val_mean_absolute_error: 6.9473 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.74584\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2702 - mean_absolute_error: 4.2702 - acc: 0.6120 - val_loss: 6.8963 - val_mean_absolute_error: 6.8963 - val_acc: 0.5550\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.74584\n",
      "Epoch 35/100\n",
      "16766/16766 [==============================] - 1s 35us/step - loss: 4.1959 - mean_absolute_error: 4.1959 - acc: 0.6095 - val_loss: 6.9112 - val_mean_absolute_error: 6.9112 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.74584\n",
      "Epoch 36/100\n",
      "16766/16766 [==============================] - 1s 31us/step - loss: 4.1825 - mean_absolute_error: 4.1825 - acc: 0.6040 - val_loss: 6.9215 - val_mean_absolute_error: 6.9215 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.74584\n",
      "Epoch 37/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1787 - mean_absolute_error: 4.1787 - acc: 0.6085 - val_loss: 6.9019 - val_mean_absolute_error: 6.9019 - val_acc: 0.5633\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.74584\n",
      "Epoch 38/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2451 - mean_absolute_error: 4.2451 - acc: 0.6091 - val_loss: 6.9391 - val_mean_absolute_error: 6.9391 - val_acc: 0.5559\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.74584\n",
      "Epoch 39/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.2184 - mean_absolute_error: 4.2184 - acc: 0.6089 - val_loss: 6.9283 - val_mean_absolute_error: 6.9283 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.74584\n",
      "Epoch 40/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1738 - mean_absolute_error: 4.1738 - acc: 0.6099 - val_loss: 6.9588 - val_mean_absolute_error: 6.9588 - val_acc: 0.5555\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.74584\n",
      "Epoch 41/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.2088 - mean_absolute_error: 4.2088 - acc: 0.6120 - val_loss: 6.9322 - val_mean_absolute_error: 6.9322 - val_acc: 0.5600\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.74584\n",
      "Epoch 42/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1579 - mean_absolute_error: 4.1579 - acc: 0.6096 - val_loss: 6.9145 - val_mean_absolute_error: 6.9145 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.74584\n",
      "Epoch 43/100\n",
      "16766/16766 [==============================] - 1s 32us/step - loss: 4.1567 - mean_absolute_error: 4.1567 - acc: 0.6091 - val_loss: 6.9562 - val_mean_absolute_error: 6.9562 - val_acc: 0.5607\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.74584\n",
      "Epoch 44/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1563 - mean_absolute_error: 4.1563 - acc: 0.6107 - val_loss: 6.9354 - val_mean_absolute_error: 6.9354 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.74584\n",
      "Epoch 45/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1813 - mean_absolute_error: 4.1813 - acc: 0.6100 - val_loss: 7.0076 - val_mean_absolute_error: 7.0076 - val_acc: 0.5635\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.74584\n",
      "Epoch 46/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1853 - mean_absolute_error: 4.1853 - acc: 0.6121 - val_loss: 6.9543 - val_mean_absolute_error: 6.9543 - val_acc: 0.5661\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.74584\n",
      "Epoch 47/100\n",
      "16766/16766 [==============================] - 1s 34us/step - loss: 4.1917 - mean_absolute_error: 4.1917 - acc: 0.6139 - val_loss: 6.9537 - val_mean_absolute_error: 6.9537 - val_acc: 0.5686\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.74584\n",
      "Epoch 48/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1413 - mean_absolute_error: 4.1413 - acc: 0.6142 - val_loss: 6.9664 - val_mean_absolute_error: 6.9664 - val_acc: 0.5692\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.74584\n",
      "Epoch 49/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1630 - mean_absolute_error: 4.1630 - acc: 0.6135 - val_loss: 6.9823 - val_mean_absolute_error: 6.9823 - val_acc: 0.5642\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.74584\n",
      "Epoch 50/100\n",
      "16766/16766 [==============================] - 1s 33us/step - loss: 4.1885 - mean_absolute_error: 4.1885 - acc: 0.6125 - val_loss: 6.9778 - val_mean_absolute_error: 6.9778 - val_acc: 0.5652\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.74584\n",
      "Epoch 00050: early stopping\n",
      "16767/16767 [==============================] - 0s 11us/step\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16767 samples, validate on 8384 samples\n",
      "Epoch 1/100\n",
      "16767/16767 [==============================] - 1s 51us/step - loss: 7.6424 - mean_absolute_error: 7.6424 - acc: 0.2017 - val_loss: 7.1703 - val_mean_absolute_error: 7.1703 - val_acc: 0.4962\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.74584\n",
      "Epoch 2/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 5.3613 - mean_absolute_error: 5.3613 - acc: 0.4057 - val_loss: 7.1145 - val_mean_absolute_error: 7.1145 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.74584\n",
      "Epoch 3/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.9249 - mean_absolute_error: 4.9249 - acc: 0.5195 - val_loss: 7.0621 - val_mean_absolute_error: 7.0621 - val_acc: 0.4742\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.74584\n",
      "Epoch 4/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.7919 - mean_absolute_error: 4.7919 - acc: 0.5629 - val_loss: 7.0255 - val_mean_absolute_error: 7.0255 - val_acc: 0.4803\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.74584\n",
      "Epoch 5/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.7085 - mean_absolute_error: 4.7085 - acc: 0.5870 - val_loss: 7.0023 - val_mean_absolute_error: 7.0023 - val_acc: 0.4764\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.74584\n",
      "Epoch 6/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.7217 - mean_absolute_error: 4.7217 - acc: 0.5942 - val_loss: 6.9637 - val_mean_absolute_error: 6.9637 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.74584\n",
      "Epoch 7/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.5510 - mean_absolute_error: 4.5510 - acc: 0.6055 - val_loss: 6.9718 - val_mean_absolute_error: 6.9718 - val_acc: 0.4999\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.74584\n",
      "Epoch 8/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.5272 - mean_absolute_error: 4.5272 - acc: 0.6088 - val_loss: 6.9471 - val_mean_absolute_error: 6.9471 - val_acc: 0.5219\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.74584\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5121 - mean_absolute_error: 4.5121 - acc: 0.6092 - val_loss: 6.9446 - val_mean_absolute_error: 6.9446 - val_acc: 0.5008\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.74584\n",
      "Epoch 10/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.5654 - mean_absolute_error: 4.5654 - acc: 0.6091 - val_loss: 6.9371 - val_mean_absolute_error: 6.9371 - val_acc: 0.5289\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.74584\n",
      "Epoch 11/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4575 - mean_absolute_error: 4.4575 - acc: 0.6127 - val_loss: 6.9316 - val_mean_absolute_error: 6.9316 - val_acc: 0.5116\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.74584\n",
      "Epoch 12/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4640 - mean_absolute_error: 4.4640 - acc: 0.6086 - val_loss: 6.9038 - val_mean_absolute_error: 6.9038 - val_acc: 0.5378\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.74584\n",
      "Epoch 13/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.4734 - mean_absolute_error: 4.4734 - acc: 0.6120 - val_loss: 6.9284 - val_mean_absolute_error: 6.9284 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.74584\n",
      "Epoch 14/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.4463 - mean_absolute_error: 4.4463 - acc: 0.6140 - val_loss: 6.9319 - val_mean_absolute_error: 6.9319 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.74584\n",
      "Epoch 15/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.4382 - mean_absolute_error: 4.4382 - acc: 0.6163 - val_loss: 6.9393 - val_mean_absolute_error: 6.9393 - val_acc: 0.5475\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.74584\n",
      "Epoch 16/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.4459 - mean_absolute_error: 4.4459 - acc: 0.6148 - val_loss: 6.9218 - val_mean_absolute_error: 6.9218 - val_acc: 0.5528\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.74584\n",
      "Epoch 17/100\n",
      "16767/16767 [==============================] - 1s 37us/step - loss: 4.4581 - mean_absolute_error: 4.4581 - acc: 0.6114 - val_loss: 6.9550 - val_mean_absolute_error: 6.9550 - val_acc: 0.5649\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.74584\n",
      "Epoch 18/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.4446 - mean_absolute_error: 4.4446 - acc: 0.6159 - val_loss: 6.9629 - val_mean_absolute_error: 6.9629 - val_acc: 0.5669\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.74584\n",
      "Epoch 19/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3453 - mean_absolute_error: 4.3453 - acc: 0.6117 - val_loss: 6.9817 - val_mean_absolute_error: 6.9817 - val_acc: 0.5601\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.74584\n",
      "Epoch 20/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3641 - mean_absolute_error: 4.3641 - acc: 0.6148 - val_loss: 6.9784 - val_mean_absolute_error: 6.9784 - val_acc: 0.5697\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.74584\n",
      "Epoch 21/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3991 - mean_absolute_error: 4.3991 - acc: 0.6126 - val_loss: 6.9855 - val_mean_absolute_error: 6.9855 - val_acc: 0.5787\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.74584\n",
      "Epoch 22/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.3893 - mean_absolute_error: 4.3893 - acc: 0.6148 - val_loss: 6.9936 - val_mean_absolute_error: 6.9936 - val_acc: 0.5793\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.74584\n",
      "Epoch 23/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3664 - mean_absolute_error: 4.3664 - acc: 0.6171 - val_loss: 7.0103 - val_mean_absolute_error: 7.0103 - val_acc: 0.5785\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.74584\n",
      "Epoch 24/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3814 - mean_absolute_error: 4.3814 - acc: 0.6136 - val_loss: 7.0088 - val_mean_absolute_error: 7.0088 - val_acc: 0.5753\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.74584\n",
      "Epoch 25/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3327 - mean_absolute_error: 4.3327 - acc: 0.6178 - val_loss: 7.0292 - val_mean_absolute_error: 7.0292 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.74584\n",
      "Epoch 26/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3178 - mean_absolute_error: 4.3178 - acc: 0.6169 - val_loss: 7.0850 - val_mean_absolute_error: 7.0850 - val_acc: 0.5734\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.74584\n",
      "Epoch 27/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.3289 - mean_absolute_error: 4.3289 - acc: 0.6129 - val_loss: 7.1219 - val_mean_absolute_error: 7.1219 - val_acc: 0.5756\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.74584\n",
      "Epoch 28/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2991 - mean_absolute_error: 4.2991 - acc: 0.6140 - val_loss: 7.1573 - val_mean_absolute_error: 7.1573 - val_acc: 0.5743\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.74584\n",
      "Epoch 29/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2875 - mean_absolute_error: 4.2875 - acc: 0.6172 - val_loss: 7.0783 - val_mean_absolute_error: 7.0783 - val_acc: 0.5812\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.74584\n",
      "Epoch 30/100\n",
      "16767/16767 [==============================] - 1s 40us/step - loss: 4.2994 - mean_absolute_error: 4.2994 - acc: 0.6158 - val_loss: 7.1717 - val_mean_absolute_error: 7.1717 - val_acc: 0.5742\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.74584\n",
      "Epoch 31/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2664 - mean_absolute_error: 4.2664 - acc: 0.6175 - val_loss: 7.1340 - val_mean_absolute_error: 7.1340 - val_acc: 0.5784\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.74584\n",
      "Epoch 32/100\n",
      "16767/16767 [==============================] - 1s 38us/step - loss: 4.2331 - mean_absolute_error: 4.2331 - acc: 0.6153 - val_loss: 7.1924 - val_mean_absolute_error: 7.1924 - val_acc: 0.5763\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.74584\n",
      "Epoch 33/100\n",
      "16767/16767 [==============================] - 1s 39us/step - loss: 4.2646 - mean_absolute_error: 4.2646 - acc: 0.6171 - val_loss: 7.1291 - val_mean_absolute_error: 7.1291 - val_acc: 0.5791\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.74584\n",
      "Epoch 34/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.3053 - mean_absolute_error: 4.3053 - acc: 0.6172 - val_loss: 7.3238 - val_mean_absolute_error: 7.3238 - val_acc: 0.5662\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.74584\n",
      "Epoch 35/100\n",
      "16767/16767 [==============================] - 1s 36us/step - loss: 4.2399 - mean_absolute_error: 4.2399 - acc: 0.6151 - val_loss: 7.2354 - val_mean_absolute_error: 7.2354 - val_acc: 0.5772\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.74584\n",
      "Epoch 36/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2007 - mean_absolute_error: 4.2007 - acc: 0.6141 - val_loss: 7.2758 - val_mean_absolute_error: 7.2758 - val_acc: 0.5766\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.74584\n",
      "Epoch 37/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2845 - mean_absolute_error: 4.2845 - acc: 0.6175 - val_loss: 7.2717 - val_mean_absolute_error: 7.2717 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.74584\n",
      "Epoch 38/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2190 - mean_absolute_error: 4.2190 - acc: 0.6127 - val_loss: 7.2092 - val_mean_absolute_error: 7.2092 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.74584\n",
      "Epoch 39/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2218 - mean_absolute_error: 4.2218 - acc: 0.6169 - val_loss: 7.2300 - val_mean_absolute_error: 7.2300 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.74584\n",
      "Epoch 40/100\n",
      "16767/16767 [==============================] - 1s 35us/step - loss: 4.2508 - mean_absolute_error: 4.2508 - acc: 0.6154 - val_loss: 7.2666 - val_mean_absolute_error: 7.2666 - val_acc: 0.5786\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.74584\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2393 - mean_absolute_error: 4.2393 - acc: 0.6152 - val_loss: 7.2854 - val_mean_absolute_error: 7.2854 - val_acc: 0.5788\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.74584\n",
      "Epoch 42/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2384 - mean_absolute_error: 4.2384 - acc: 0.6170 - val_loss: 7.4268 - val_mean_absolute_error: 7.4268 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.74584\n",
      "Epoch 43/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.1953 - mean_absolute_error: 4.1953 - acc: 0.6176 - val_loss: 7.4014 - val_mean_absolute_error: 7.4014 - val_acc: 0.5798\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.74584\n",
      "Epoch 44/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2254 - mean_absolute_error: 4.2254 - acc: 0.6151 - val_loss: 7.4559 - val_mean_absolute_error: 7.4559 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.74584\n",
      "Epoch 45/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.2081 - mean_absolute_error: 4.2081 - acc: 0.6181 - val_loss: 7.5918 - val_mean_absolute_error: 7.5918 - val_acc: 0.5771\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.74584\n",
      "Epoch 46/100\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 4.1862 - mean_absolute_error: 4.1862 - acc: 0.6180 - val_loss: 7.6479 - val_mean_absolute_error: 7.6479 - val_acc: 0.5774\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.74584\n",
      "Epoch 47/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2486 - mean_absolute_error: 4.2486 - acc: 0.6161 - val_loss: 7.4599 - val_mean_absolute_error: 7.4599 - val_acc: 0.5792\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.74584\n",
      "Epoch 48/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.2365 - mean_absolute_error: 4.2365 - acc: 0.6185 - val_loss: 7.3709 - val_mean_absolute_error: 7.3709 - val_acc: 0.5818\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.74584\n",
      "Epoch 49/100\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 4.1993 - mean_absolute_error: 4.1993 - acc: 0.6159 - val_loss: 7.3930 - val_mean_absolute_error: 7.3930 - val_acc: 0.5842\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.74584\n",
      "Epoch 50/100\n",
      "16767/16767 [==============================] - 1s 34us/step - loss: 4.2002 - mean_absolute_error: 4.2002 - acc: 0.6170 - val_loss: 7.4484 - val_mean_absolute_error: 7.4484 - val_acc: 0.5849\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.74584\n",
      "Epoch 00050: early stopping\n",
      "16766/16766 [==============================] - 0s 11us/step\n",
      "16767/16767 [==============================] - ETA:  - 0s 11us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 33533 samples, validate on 8384 samples\n",
      "Epoch 1/150\n",
      "33533/33533 [==============================] - 1s 41us/step - loss: 6.0991 - mean_absolute_error: 6.0991 - acc: 0.3996 - val_loss: 7.0703 - val_mean_absolute_error: 7.0703 - val_acc: 0.4076\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 6.74584\n",
      "Epoch 2/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 5.1141 - mean_absolute_error: 5.1141 - acc: 0.5657 - val_loss: 7.0825 - val_mean_absolute_error: 7.0825 - val_acc: 0.3866\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 6.74584\n",
      "Epoch 3/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.9372 - mean_absolute_error: 4.9372 - acc: 0.5883 - val_loss: 7.0075 - val_mean_absolute_error: 7.0075 - val_acc: 0.4405\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 6.74584\n",
      "Epoch 4/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.8720 - mean_absolute_error: 4.8720 - acc: 0.6001 - val_loss: 7.1780 - val_mean_absolute_error: 7.1780 - val_acc: 0.4048\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 6.74584\n",
      "Epoch 5/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.8622 - mean_absolute_error: 4.8622 - acc: 0.6082 - val_loss: 7.1425 - val_mean_absolute_error: 7.1425 - val_acc: 0.4319\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 6.74584\n",
      "Epoch 6/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.8097 - mean_absolute_error: 4.8097 - acc: 0.6115 - val_loss: 7.0508 - val_mean_absolute_error: 7.0508 - val_acc: 0.4791\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 6.74584\n",
      "Epoch 7/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.7527 - mean_absolute_error: 4.7527 - acc: 0.6104 - val_loss: 7.1718 - val_mean_absolute_error: 7.1718 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 6.74584\n",
      "Epoch 8/150\n",
      "33533/33533 [==============================] - 1s 33us/step - loss: 4.7648 - mean_absolute_error: 4.7648 - acc: 0.6153 - val_loss: 7.0290 - val_mean_absolute_error: 7.0290 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 6.74584\n",
      "Epoch 9/150\n",
      "33533/33533 [==============================] - 1s 37us/step - loss: 4.7931 - mean_absolute_error: 4.7931 - acc: 0.6140 - val_loss: 7.0853 - val_mean_absolute_error: 7.0853 - val_acc: 0.5434\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 6.74584\n",
      "Epoch 10/150\n",
      "33533/33533 [==============================] - 1s 35us/step - loss: 4.7225 - mean_absolute_error: 4.7225 - acc: 0.6143 - val_loss: 7.0938 - val_mean_absolute_error: 7.0938 - val_acc: 0.5479\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.74584\n",
      "Epoch 11/150\n",
      "33533/33533 [==============================] - 1s 36us/step - loss: 4.7441 - mean_absolute_error: 4.7441 - acc: 0.6147 - val_loss: 7.1166 - val_mean_absolute_error: 7.1166 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 6.74584\n",
      "Epoch 12/150\n",
      "33533/33533 [==============================] - 1s 37us/step - loss: 4.7521 - mean_absolute_error: 4.7521 - acc: 0.6147 - val_loss: 7.1239 - val_mean_absolute_error: 7.1239 - val_acc: 0.5688\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.74584\n",
      "Epoch 13/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.7086 - mean_absolute_error: 4.7086 - acc: 0.6149 - val_loss: 7.3156 - val_mean_absolute_error: 7.3156 - val_acc: 0.5594\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 6.74584\n",
      "Epoch 14/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6934 - mean_absolute_error: 4.6934 - acc: 0.6150 - val_loss: 7.2176 - val_mean_absolute_error: 7.2176 - val_acc: 0.5662\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.74584\n",
      "Epoch 15/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6550 - mean_absolute_error: 4.6550 - acc: 0.6147 - val_loss: 7.1460 - val_mean_absolute_error: 7.1460 - val_acc: 0.5730\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 6.74584\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.7024 - mean_absolute_error: 4.7024 - acc: 0.6122 - val_loss: 7.4505 - val_mean_absolute_error: 7.4505 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 6.74584\n",
      "Epoch 17/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6422 - mean_absolute_error: 4.6422 - acc: 0.6130 - val_loss: 7.3808 - val_mean_absolute_error: 7.3808 - val_acc: 0.5667\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 6.74584\n",
      "Epoch 18/150\n",
      "33533/33533 [==============================] - 1s 33us/step - loss: 4.6575 - mean_absolute_error: 4.6575 - acc: 0.6135 - val_loss: 7.2544 - val_mean_absolute_error: 7.2544 - val_acc: 0.5774\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.74584\n",
      "Epoch 19/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6314 - mean_absolute_error: 4.6314 - acc: 0.6131 - val_loss: 7.2404 - val_mean_absolute_error: 7.2404 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.74584\n",
      "Epoch 20/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6228 - mean_absolute_error: 4.6228 - acc: 0.6135 - val_loss: 7.3447 - val_mean_absolute_error: 7.3447 - val_acc: 0.5761\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.74584\n",
      "Epoch 21/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6238 - mean_absolute_error: 4.6238 - acc: 0.6159 - val_loss: 7.5198 - val_mean_absolute_error: 7.5198 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 6.74584\n",
      "Epoch 22/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6683 - mean_absolute_error: 4.6683 - acc: 0.6122 - val_loss: 7.5042 - val_mean_absolute_error: 7.5042 - val_acc: 0.5759\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 6.74584\n",
      "Epoch 23/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6227 - mean_absolute_error: 4.6227 - acc: 0.6149 - val_loss: 7.4924 - val_mean_absolute_error: 7.4924 - val_acc: 0.5728\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 6.74584\n",
      "Epoch 24/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6042 - mean_absolute_error: 4.6042 - acc: 0.6141 - val_loss: 7.1373 - val_mean_absolute_error: 7.1373 - val_acc: 0.5765\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 6.74584\n",
      "Epoch 25/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6500 - mean_absolute_error: 4.6500 - acc: 0.6134 - val_loss: 7.3946 - val_mean_absolute_error: 7.3946 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 6.74584\n",
      "Epoch 26/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6655 - mean_absolute_error: 4.6655 - acc: 0.6127 - val_loss: 7.2538 - val_mean_absolute_error: 7.2538 - val_acc: 0.5767\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 6.74584\n",
      "Epoch 27/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5731 - mean_absolute_error: 4.5731 - acc: 0.6155 - val_loss: 7.2699 - val_mean_absolute_error: 7.2699 - val_acc: 0.5829\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 6.74584\n",
      "Epoch 28/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5924 - mean_absolute_error: 4.5924 - acc: 0.6146 - val_loss: 7.1710 - val_mean_absolute_error: 7.1710 - val_acc: 0.5855\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 6.74584\n",
      "Epoch 29/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5897 - mean_absolute_error: 4.5897 - acc: 0.6138 - val_loss: 7.3019 - val_mean_absolute_error: 7.3019 - val_acc: 0.5722\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 6.74584\n",
      "Epoch 30/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.6442 - mean_absolute_error: 4.6442 - acc: 0.6137 - val_loss: 7.3537 - val_mean_absolute_error: 7.3537 - val_acc: 0.5732\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 6.74584\n",
      "Epoch 31/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6055 - mean_absolute_error: 4.6055 - acc: 0.6147 - val_loss: 7.5597 - val_mean_absolute_error: 7.5597 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 6.74584\n",
      "Epoch 32/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5803 - mean_absolute_error: 4.5803 - acc: 0.6142 - val_loss: 7.4185 - val_mean_absolute_error: 7.4185 - val_acc: 0.5760\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 6.74584\n",
      "Epoch 33/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5636 - mean_absolute_error: 4.5636 - acc: 0.6136 - val_loss: 7.3167 - val_mean_absolute_error: 7.3167 - val_acc: 0.5682\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 6.74584\n",
      "Epoch 34/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5951 - mean_absolute_error: 4.5951 - acc: 0.6149 - val_loss: 7.3522 - val_mean_absolute_error: 7.3522 - val_acc: 0.5724\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 6.74584\n",
      "Epoch 35/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5843 - mean_absolute_error: 4.5843 - acc: 0.6136 - val_loss: 7.2971 - val_mean_absolute_error: 7.2971 - val_acc: 0.5750\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 6.74584\n",
      "Epoch 36/150\n",
      "33533/33533 [==============================] - 1s 33us/step - loss: 4.5301 - mean_absolute_error: 4.5301 - acc: 0.6159 - val_loss: 7.5259 - val_mean_absolute_error: 7.5259 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 6.74584\n",
      "Epoch 37/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5960 - mean_absolute_error: 4.5960 - acc: 0.6108 - val_loss: 7.4867 - val_mean_absolute_error: 7.4867 - val_acc: 0.5719\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 6.74584\n",
      "Epoch 38/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5460 - mean_absolute_error: 4.5460 - acc: 0.6135 - val_loss: 7.4834 - val_mean_absolute_error: 7.4834 - val_acc: 0.5841\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 6.74584\n",
      "Epoch 39/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5459 - mean_absolute_error: 4.5459 - acc: 0.6146 - val_loss: 7.4441 - val_mean_absolute_error: 7.4441 - val_acc: 0.5776\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 6.74584\n",
      "Epoch 40/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5783 - mean_absolute_error: 4.5783 - acc: 0.6118 - val_loss: 7.1778 - val_mean_absolute_error: 7.1778 - val_acc: 0.5724\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 6.74584\n",
      "Epoch 41/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.6101 - mean_absolute_error: 4.6101 - acc: 0.6128 - val_loss: 7.2478 - val_mean_absolute_error: 7.2478 - val_acc: 0.5854\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 6.74584\n",
      "Epoch 42/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5750 - mean_absolute_error: 4.5750 - acc: 0.6150 - val_loss: 7.2653 - val_mean_absolute_error: 7.2653 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 6.74584\n",
      "Epoch 43/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5418 - mean_absolute_error: 4.5418 - acc: 0.6156 - val_loss: 7.4239 - val_mean_absolute_error: 7.4239 - val_acc: 0.5811\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 6.74584\n",
      "Epoch 44/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5707 - mean_absolute_error: 4.5707 - acc: 0.6098 - val_loss: 7.3047 - val_mean_absolute_error: 7.3047 - val_acc: 0.5790\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.74584\n",
      "Epoch 45/150\n",
      "33533/33533 [==============================] - 1s 33us/step - loss: 4.5473 - mean_absolute_error: 4.5473 - acc: 0.6146 - val_loss: 7.2966 - val_mean_absolute_error: 7.2966 - val_acc: 0.5859\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 6.74584\n",
      "Epoch 46/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5570 - mean_absolute_error: 4.5570 - acc: 0.6110 - val_loss: 7.5008 - val_mean_absolute_error: 7.5008 - val_acc: 0.5844\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.74584\n",
      "Epoch 47/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5996 - mean_absolute_error: 4.5996 - acc: 0.6104 - val_loss: 7.2168 - val_mean_absolute_error: 7.2168 - val_acc: 0.5880\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.74584\n",
      "Epoch 48/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5897 - mean_absolute_error: 4.5897 - acc: 0.6111 - val_loss: 7.3326 - val_mean_absolute_error: 7.3326 - val_acc: 0.5843\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.74584\n",
      "Epoch 49/150\n",
      "33533/33533 [==============================] - 1s 32us/step - loss: 4.5306 - mean_absolute_error: 4.5306 - acc: 0.6146 - val_loss: 7.3272 - val_mean_absolute_error: 7.3272 - val_acc: 0.5868\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.74584\n",
      "Epoch 50/150\n",
      "33533/33533 [==============================] - 1s 31us/step - loss: 4.5493 - mean_absolute_error: 4.5493 - acc: 0.6140 - val_loss: 7.2296 - val_mean_absolute_error: 7.2296 - val_acc: 0.5824\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.74584\n",
      "Epoch 00050: early stopping\n",
      "Optimizer escogido: Nadam\n",
      "Función de activación escogida: elu\n",
      "Epochs escogidos: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameter = {'optimizer' : ['Nadam', 'Adam'  ],\n",
    "            'activation' : ['relu', 'elu'],\n",
    "            'epochs':[150,100]}\n",
    "\n",
    "clf1 =  GridSearchCV(estimator=nn1, param_grid=parameter, cv=2)\n",
    "NN_2=clf1.fit(X_train, y_train, validation_data=(X_norm_test,y_test),\n",
    "                    batch_size=32, verbose=1, callbacks=[stopped,best_model])\n",
    "\n",
    "print('Optimizer escogido: ' +str(NN_2.best_params_['optimizer']))\n",
    "print('Función de activación escogida: ' +str(NN_2.best_params_['activation']))\n",
    "print('Epochs escogidos: ' +str(NN_2.best_params_['epochs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer escogido: Nadam\n",
      "Función de activación escogida: elu\n",
      "Epochs escogidos: 150\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 27)                756       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 28        \n",
      "=================================================================\n",
      "Total params: 2,296\n",
      "Trainable params: 2,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "NN_3 = load_model('Best_nn.h5')\n",
    "print('Optimizer escogido: ' +str(NN_2.best_params_['optimizer']))\n",
    "print('Función de activación escogida: ' +str(NN_2.best_params_['activation']))\n",
    "print('Epochs escogidos: ' +str(NN_2.best_params_['epochs']))\n",
    "NN_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de modelos SET TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Reg. Lineal: 8.81\n",
      "MAE Árbol Decisión: 7.91\n",
      "MAE Red Neuronal: 6.75\n"
     ]
    }
   ],
   "source": [
    "# SE CARGAN LAS MEJORES CONFIGURACINES PARA CADA UNO DE LOS MODELOS \n",
    "# se obtiene el desempeño de las predicciones en el set test\n",
    "from keras.models import load_model\n",
    "\n",
    "rl_load = pickle.load(open('rl.sav', 'rb'))\n",
    "dt_load = pickle.load(open('dt.sav', 'rb'))\n",
    "nn_load = load_model('Best_nn.h5')\n",
    "\n",
    "pred_rl = rl_load.predict(X_norm_test)\n",
    "pred_dt = dt_load.predict(X_test)\n",
    "pred_nn = nn_load.predict(X_norm_test)\n",
    "\n",
    "\n",
    "print('MAE Reg. Lineal: %.2f' %mean_absolute_error(pred_rl,y_test))\n",
    "print('MAE Árbol Decisión: %.2f' %mean_absolute_error(pred_dt,y_test))\n",
    "print('MAE Red Neuronal: %.2f' %mean_absolute_error(pred_nn,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizacion base completa train \n",
    "X_train_total = X[variables_sin_bajacont]\n",
    "X_train_total_norm = pd.DataFrame(norm_mean.transform(X_train_total), \n",
    "                           columns = variables_sin_bajacont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "41917/41917 [==============================] - 2s 36us/step - loss: 6.2245 - mean_absolute_error: 6.2245 - acc: 0.6077\n",
      "Epoch 2/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 6.0146 - mean_absolute_error: 6.0146 - acc: 0.6046\n",
      "Epoch 3/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.8534 - mean_absolute_error: 5.8534 - acc: 0.5965\n",
      "Epoch 4/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.7898 - mean_absolute_error: 5.7898 - acc: 0.5959\n",
      "Epoch 5/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.7386 - mean_absolute_error: 5.7386 - acc: 0.5925\n",
      "Epoch 6/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.6962 - mean_absolute_error: 5.6962 - acc: 0.5892\n",
      "Epoch 7/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.6405 - mean_absolute_error: 5.6405 - acc: 0.5886\n",
      "Epoch 8/150\n",
      "41917/41917 [==============================] - 1s 27us/step - loss: 5.6359 - mean_absolute_error: 5.6359 - acc: 0.5929\n",
      "Epoch 9/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.5174 - mean_absolute_error: 5.5174 - acc: 0.5923\n",
      "Epoch 10/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.5824 - mean_absolute_error: 5.5824 - acc: 0.5884\n",
      "Epoch 11/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.4990 - mean_absolute_error: 5.4990 - acc: 0.5947\n",
      "Epoch 12/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.5347 - mean_absolute_error: 5.5347 - acc: 0.5914\n",
      "Epoch 13/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.4165 - mean_absolute_error: 5.4165 - acc: 0.5893\n",
      "Epoch 14/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.4266 - mean_absolute_error: 5.4266 - acc: 0.5938\n",
      "Epoch 15/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.4220 - mean_absolute_error: 5.4220 - acc: 0.5884\n",
      "Epoch 16/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.4153 - mean_absolute_error: 5.4153 - acc: 0.5909\n",
      "Epoch 17/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.4243 - mean_absolute_error: 5.4243 - acc: 0.5921\n",
      "Epoch 18/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.3880 - mean_absolute_error: 5.3880 - acc: 0.5890\n",
      "Epoch 19/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.3521 - mean_absolute_error: 5.3521 - acc: 0.5958\n",
      "Epoch 20/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.3460 - mean_absolute_error: 5.3460 - acc: 0.5978\n",
      "Epoch 21/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.3379 - mean_absolute_error: 5.3379 - acc: 0.5990\n",
      "Epoch 22/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.3718 - mean_absolute_error: 5.3718 - acc: 0.5933\n",
      "Epoch 23/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.3250 - mean_absolute_error: 5.3250 - acc: 0.5894\n",
      "Epoch 24/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.3463 - mean_absolute_error: 5.3463 - acc: 0.5973\n",
      "Epoch 25/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.2682 - mean_absolute_error: 5.2682 - acc: 0.5938\n",
      "Epoch 26/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.3087 - mean_absolute_error: 5.3087 - acc: 0.5935\n",
      "Epoch 27/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.3068 - mean_absolute_error: 5.3068 - acc: 0.5969\n",
      "Epoch 28/150\n",
      "41917/41917 [==============================] - 1s 36us/step - loss: 5.2777 - mean_absolute_error: 5.2777 - acc: 0.5953\n",
      "Epoch 29/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.2171 - mean_absolute_error: 5.2171 - acc: 0.5974\n",
      "Epoch 30/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.3282 - mean_absolute_error: 5.3282 - acc: 0.5914\n",
      "Epoch 31/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.2394 - mean_absolute_error: 5.2394 - acc: 0.6006\n",
      "Epoch 32/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.2629 - mean_absolute_error: 5.2629 - acc: 0.5959\n",
      "Epoch 33/150\n",
      "41917/41917 [==============================] - 2s 37us/step - loss: 5.3043 - mean_absolute_error: 5.3043 - acc: 0.6000\n",
      "Epoch 34/150\n",
      "41917/41917 [==============================] - 2s 41us/step - loss: 5.2473 - mean_absolute_error: 5.2473 - acc: 0.5989\n",
      "Epoch 35/150\n",
      "41917/41917 [==============================] - 2s 43us/step - loss: 5.2582 - mean_absolute_error: 5.2582 - acc: 0.6013\n",
      "Epoch 36/150\n",
      "41917/41917 [==============================] - 2s 39us/step - loss: 5.1872 - mean_absolute_error: 5.1872 - acc: 0.6004\n",
      "Epoch 37/150\n",
      "41917/41917 [==============================] - 2s 36us/step - loss: 5.1999 - mean_absolute_error: 5.1999 - acc: 0.6030\n",
      "Epoch 38/150\n",
      "41917/41917 [==============================] - 2s 42us/step - loss: 5.2053 - mean_absolute_error: 5.2053 - acc: 0.5999\n",
      "Epoch 39/150\n",
      "41917/41917 [==============================] - 2s 39us/step - loss: 5.1992 - mean_absolute_error: 5.1992 - acc: 0.6042\n",
      "Epoch 40/150\n",
      "41917/41917 [==============================] - 2s 38us/step - loss: 5.2271 - mean_absolute_error: 5.2271 - acc: 0.5902\n",
      "Epoch 41/150\n",
      "41917/41917 [==============================] - 2s 41us/step - loss: 5.1970 - mean_absolute_error: 5.1970 - acc: 0.6017\n",
      "Epoch 42/150\n",
      "41917/41917 [==============================] - 2s 39us/step - loss: 5.1916 - mean_absolute_error: 5.1916 - acc: 0.5986\n",
      "Epoch 43/150\n",
      "41917/41917 [==============================] - 2s 38us/step - loss: 5.2175 - mean_absolute_error: 5.2175 - acc: 0.6031\n",
      "Epoch 44/150\n",
      "41917/41917 [==============================] - 2s 41us/step - loss: 5.2434 - mean_absolute_error: 5.2434 - acc: 0.6049\n",
      "Epoch 45/150\n",
      "41917/41917 [==============================] - 2s 40us/step - loss: 5.2321 - mean_absolute_error: 5.2321 - acc: 0.6052\n",
      "Epoch 46/150\n",
      "41917/41917 [==============================] - 2s 41us/step - loss: 5.2057 - mean_absolute_error: 5.2057 - acc: 0.6017\n",
      "Epoch 47/150\n",
      "41917/41917 [==============================] - 2s 44us/step - loss: 5.2195 - mean_absolute_error: 5.2195 - acc: 0.6024\n",
      "Epoch 48/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.1697 - mean_absolute_error: 5.1697 - acc: 0.6004\n",
      "Epoch 49/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1800 - mean_absolute_error: 5.1800 - acc: 0.6003\n",
      "Epoch 50/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.2203 - mean_absolute_error: 5.2203 - acc: 0.6027\n",
      "Epoch 51/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.1071 - mean_absolute_error: 5.1071 - acc: 0.6029\n",
      "Epoch 52/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.1304 - mean_absolute_error: 5.1304 - acc: 0.6033\n",
      "Epoch 53/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1372 - mean_absolute_error: 5.1372 - acc: 0.6004\n",
      "Epoch 54/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1030 - mean_absolute_error: 5.1030 - acc: 0.6033\n",
      "Epoch 55/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.2036 - mean_absolute_error: 5.2036 - acc: 0.5991\n",
      "Epoch 56/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1790 - mean_absolute_error: 5.1790 - acc: 0.6023\n",
      "Epoch 57/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.1346 - mean_absolute_error: 5.1346 - acc: 0.6069\n",
      "Epoch 58/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.1599 - mean_absolute_error: 5.1599 - acc: 0.6016\n",
      "Epoch 59/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.1477 - mean_absolute_error: 5.1477 - acc: 0.6029\n",
      "Epoch 60/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.1163 - mean_absolute_error: 5.1163 - acc: 0.6024\n",
      "Epoch 61/150\n",
      "41917/41917 [==============================] - 2s 37us/step - loss: 5.1263 - mean_absolute_error: 5.1263 - acc: 0.6019\n",
      "Epoch 62/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.1692 - mean_absolute_error: 5.1692 - acc: 0.5931\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41917/41917 [==============================] - 2s 39us/step - loss: 5.0902 - mean_absolute_error: 5.0902 - acc: 0.6042\n",
      "Epoch 64/150\n",
      "41917/41917 [==============================] - 2s 42us/step - loss: 5.1239 - mean_absolute_error: 5.1239 - acc: 0.5990\n",
      "Epoch 65/150\n",
      "41917/41917 [==============================] - 2s 38us/step - loss: 5.1325 - mean_absolute_error: 5.1325 - acc: 0.6038\n",
      "Epoch 66/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.0960 - mean_absolute_error: 5.0960 - acc: 0.6037\n",
      "Epoch 67/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.0720 - mean_absolute_error: 5.0720 - acc: 0.6039\n",
      "Epoch 68/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.0870 - mean_absolute_error: 5.0870 - acc: 0.6028\n",
      "Epoch 69/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.0965 - mean_absolute_error: 5.0965 - acc: 0.6055\n",
      "Epoch 70/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.1801 - mean_absolute_error: 5.1801 - acc: 0.6062\n",
      "Epoch 71/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.0677 - mean_absolute_error: 5.0677 - acc: 0.6046\n",
      "Epoch 72/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1091 - mean_absolute_error: 5.1091 - acc: 0.6060\n",
      "Epoch 73/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.1626 - mean_absolute_error: 5.1626 - acc: 0.6047\n",
      "Epoch 74/150\n",
      "41917/41917 [==============================] - 2s 41us/step - loss: 5.1005 - mean_absolute_error: 5.1005 - acc: 0.6068\n",
      "Epoch 75/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0653 - mean_absolute_error: 5.0653 - acc: 0.6058: 0s - loss: 5.1911 - mean_absolute_error: 5.1911 - a\n",
      "Epoch 76/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.0729 - mean_absolute_error: 5.0729 - acc: 0.6048\n",
      "Epoch 77/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0976 - mean_absolute_error: 5.0976 - acc: 0.6064\n",
      "Epoch 78/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1135 - mean_absolute_error: 5.1135 - acc: 0.6067\n",
      "Epoch 79/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.1007 - mean_absolute_error: 5.1007 - acc: 0.6083\n",
      "Epoch 80/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.0778 - mean_absolute_error: 5.0778 - acc: 0.6045\n",
      "Epoch 81/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0909 - mean_absolute_error: 5.0909 - acc: 0.6065\n",
      "Epoch 82/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.0767 - mean_absolute_error: 5.0767 - acc: 0.6050\n",
      "Epoch 83/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.0641 - mean_absolute_error: 5.0641 - acc: 0.6030: 0s - loss: 5.1010 - mean_absolute_error: 5.1010 - acc: 0.6\n",
      "Epoch 84/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.1132 - mean_absolute_error: 5.1132 - acc: 0.6022\n",
      "Epoch 85/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.0831 - mean_absolute_error: 5.0831 - acc: 0.6036\n",
      "Epoch 86/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0555 - mean_absolute_error: 5.0555 - acc: 0.6046\n",
      "Epoch 87/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.1289 - mean_absolute_error: 5.1289 - acc: 0.6069\n",
      "Epoch 88/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0583 - mean_absolute_error: 5.0583 - acc: 0.6065\n",
      "Epoch 89/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0884 - mean_absolute_error: 5.0884 - acc: 0.6088\n",
      "Epoch 90/150\n",
      "41917/41917 [==============================] - 2s 42us/step - loss: 5.0096 - mean_absolute_error: 5.0096 - acc: 0.6013\n",
      "Epoch 91/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0622 - mean_absolute_error: 5.0622 - acc: 0.6051\n",
      "Epoch 92/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0212 - mean_absolute_error: 5.0212 - acc: 0.6068\n",
      "Epoch 93/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 5.0521 - mean_absolute_error: 5.0521 - acc: 0.6077\n",
      "Epoch 94/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0723 - mean_absolute_error: 5.0723 - acc: 0.6033\n",
      "Epoch 95/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0259 - mean_absolute_error: 5.0259 - acc: 0.6091\n",
      "Epoch 96/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.1117 - mean_absolute_error: 5.1117 - acc: 0.6058\n",
      "Epoch 97/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.1093 - mean_absolute_error: 5.1093 - acc: 0.6047\n",
      "Epoch 98/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0342 - mean_absolute_error: 5.0342 - acc: 0.6042\n",
      "Epoch 99/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0460 - mean_absolute_error: 5.0460 - acc: 0.6083\n",
      "Epoch 100/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9878 - mean_absolute_error: 4.9878 - acc: 0.6052\n",
      "Epoch 101/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9929 - mean_absolute_error: 4.9929 - acc: 0.6073\n",
      "Epoch 102/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0434 - mean_absolute_error: 5.0434 - acc: 0.6033\n",
      "Epoch 103/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0346 - mean_absolute_error: 5.0346 - acc: 0.6044\n",
      "Epoch 104/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9783 - mean_absolute_error: 4.9783 - acc: 0.6072\n",
      "Epoch 105/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0753 - mean_absolute_error: 5.0753 - acc: 0.6042\n",
      "Epoch 106/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0223 - mean_absolute_error: 5.0223 - acc: 0.6018\n",
      "Epoch 107/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 5.0595 - mean_absolute_error: 5.0595 - acc: 0.6093\n",
      "Epoch 108/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9846 - mean_absolute_error: 4.9846 - acc: 0.6034\n",
      "Epoch 109/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9842 - mean_absolute_error: 4.9842 - acc: 0.6039\n",
      "Epoch 110/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0505 - mean_absolute_error: 5.0505 - acc: 0.6001\n",
      "Epoch 111/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9967 - mean_absolute_error: 4.9967 - acc: 0.6086\n",
      "Epoch 112/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9752 - mean_absolute_error: 4.9752 - acc: 0.6061\n",
      "Epoch 113/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0019 - mean_absolute_error: 5.0019 - acc: 0.6065\n",
      "Epoch 114/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0018 - mean_absolute_error: 5.0018 - acc: 0.6091\n",
      "Epoch 115/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9765 - mean_absolute_error: 4.9765 - acc: 0.6097\n",
      "Epoch 116/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9951 - mean_absolute_error: 4.9951 - acc: 0.6095\n",
      "Epoch 117/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9957 - mean_absolute_error: 4.9957 - acc: 0.6067\n",
      "Epoch 118/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0596 - mean_absolute_error: 5.0596 - acc: 0.6010\n",
      "Epoch 119/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9429 - mean_absolute_error: 4.9429 - acc: 0.6100\n",
      "Epoch 120/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9436 - mean_absolute_error: 4.9436 - acc: 0.5996\n",
      "Epoch 121/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 4.9906 - mean_absolute_error: 4.9906 - acc: 0.6068\n",
      "Epoch 122/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0177 - mean_absolute_error: 5.0177 - acc: 0.6111\n",
      "Epoch 123/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9791 - mean_absolute_error: 4.9791 - acc: 0.6059\n",
      "Epoch 124/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41917/41917 [==============================] - 1s 29us/step - loss: 5.0134 - mean_absolute_error: 5.0134 - acc: 0.6046\n",
      "Epoch 125/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 4.9355 - mean_absolute_error: 4.9355 - acc: 0.6066\n",
      "Epoch 126/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 4.9873 - mean_absolute_error: 4.9873 - acc: 0.6088\n",
      "Epoch 127/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9609 - mean_absolute_error: 4.9609 - acc: 0.6072\n",
      "Epoch 128/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 4.9835 - mean_absolute_error: 4.9835 - acc: 0.6065\n",
      "Epoch 129/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9545 - mean_absolute_error: 4.9545 - acc: 0.6048\n",
      "Epoch 130/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 5.0035 - mean_absolute_error: 5.0035 - acc: 0.6041\n",
      "Epoch 131/150\n",
      "41917/41917 [==============================] - 1s 30us/step - loss: 4.9943 - mean_absolute_error: 4.9943 - acc: 0.6028\n",
      "Epoch 132/150\n",
      "41917/41917 [==============================] - 1s 33us/step - loss: 4.9952 - mean_absolute_error: 4.9952 - acc: 0.6095\n",
      "Epoch 133/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 4.9944 - mean_absolute_error: 4.9944 - acc: 0.6038\n",
      "Epoch 134/150\n",
      "41917/41917 [==============================] - 1s 35us/step - loss: 5.0464 - mean_absolute_error: 5.0464 - acc: 0.6091\n",
      "Epoch 135/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 4.9775 - mean_absolute_error: 4.9775 - acc: 0.6084\n",
      "Epoch 136/150\n",
      "41917/41917 [==============================] - 1s 31us/step - loss: 5.0123 - mean_absolute_error: 5.0123 - acc: 0.6072\n",
      "Epoch 137/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 4.9449 - mean_absolute_error: 4.9449 - acc: 0.6085\n",
      "Epoch 138/150\n",
      "41917/41917 [==============================] - 1s 27us/step - loss: 5.0163 - mean_absolute_error: 5.0163 - acc: 0.6079\n",
      "Epoch 139/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 5.0132 - mean_absolute_error: 5.0132 - acc: 0.6026\n",
      "Epoch 140/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 4.8947 - mean_absolute_error: 4.8947 - acc: 0.6096\n",
      "Epoch 141/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 5.0037 - mean_absolute_error: 5.0037 - acc: 0.6070\n",
      "Epoch 142/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 4.9966 - mean_absolute_error: 4.9966 - acc: 0.6096\n",
      "Epoch 143/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 5.0039 - mean_absolute_error: 5.0039 - acc: 0.6091\n",
      "Epoch 144/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 4.9563 - mean_absolute_error: 4.9563 - acc: 0.6074\n",
      "Epoch 145/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 4.9502 - mean_absolute_error: 4.9502 - acc: 0.6061\n",
      "Epoch 146/150\n",
      "41917/41917 [==============================] - 1s 29us/step - loss: 4.9561 - mean_absolute_error: 4.9561 - acc: 0.6032\n",
      "Epoch 147/150\n",
      "41917/41917 [==============================] - 1s 28us/step - loss: 5.0075 - mean_absolute_error: 5.0075 - acc: 0.6040\n",
      "Epoch 148/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 4.9580 - mean_absolute_error: 4.9580 - acc: 0.6055\n",
      "Epoch 149/150\n",
      "41917/41917 [==============================] - 1s 34us/step - loss: 5.0038 - mean_absolute_error: 5.0038 - acc: 0.6071\n",
      "Epoch 150/150\n",
      "41917/41917 [==============================] - 1s 32us/step - loss: 4.9864 - mean_absolute_error: 4.9864 - acc: 0.6013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10a8404ccc0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenamiento final y predicción.¶\n",
    "#Antes de poder predecir sobre la base test, necesitamos entrenar el mejor modelo en la base data total. \n",
    "NN_3.fit(X_train_total_norm, y,\n",
    "                    batch_size=32, epochs=NN_2.best_params_['epochs'],  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediccion \n",
    "# se carga los datos a predecir \n",
    "Set_Test =pd.read_csv('X_test.csv')\n",
    "Set_Test1= Set_Test[variables_sin_bajacont]\n",
    "norm_mean_train_total = preprocessing.StandardScaler().fit(Set_Test[variables_sin_bajacont])\n",
    "test_norm = pd.DataFrame(norm_mean_train_total.transform(Set_Test[variables_sin_bajacont]), \n",
    "                            columns =Set_Test[variables_sin_bajacont].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = Set_Test.iloc[:,0]\n",
    "\n",
    "predictions = NN_3.predict(test_norm[variables_sin_bajacont])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Prediccion']\n",
    "y_target = pd.DataFrame(y_target)\n",
    "df_predict = pd.DataFrame(predictions, columns=columns)\n",
    "prediccion= pd.concat([y_target, df_predict], axis=1,)\n",
    "prediccion\n",
    "prediccion.to_csv('Diaz_Gallardo_Quilapan.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
