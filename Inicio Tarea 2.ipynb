{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links de interés:\n",
    "\n",
    "https://www.bogotobogo.com/python/scikit-learn/scikit-learn_logistic_regression.php\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observaciones - N = 20000\n",
      "Características - M = 23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carga data:\n",
    "X = pd.read_csv('X.csv', sep=';')\n",
    "X = X.loc[:,X.columns[1:]]\n",
    "y = pd.read_csv('y.csv', sep=';', header=None)\n",
    "y = y.loc[:,1]\n",
    "\n",
    "print('Observaciones - N = %.1d' %len(X))\n",
    "print('Características - M = %.1d' %len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feseg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09806241 -0.02826546  0.07255215 -0.07795947 -0.07489942 -0.06311934\n",
      "   0.11900765 -0.02278602 -0.00929583 -0.02724669 -0.03956172  0.04772154\n",
      "  -0.00575392 -0.02879252 -0.01625562  0.00743511  0.02689435  0.01387891\n",
      "  -0.00536313 -0.04746118 -0.00012684  0.01361808  0.00808978]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "rl = LogisticRegression(C=1.0, fit_intercept=False, random_state=0)\n",
    "rl.fit(X,y)\n",
    "\n",
    "print(rl.coef_)\n",
    "print(rl.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09806241, -0.02826546,  0.07255215, -0.07795947, -0.07489942,\n",
       "       -0.06311934,  0.11900765, -0.02278602, -0.00929583, -0.02724669,\n",
       "       -0.03956172,  0.04772154, -0.00575392, -0.02879252, -0.01625562,\n",
       "        0.00743511,  0.02689435,  0.01387891, -0.00536313, -0.04746118,\n",
       "       -0.00012684,  0.01361808,  0.00808978])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_opt = rl.coef_[0]\n",
    "print(len(w_opt))\n",
    "w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    \n",
    "    z = np.dot(X, theta[1:]) + theta[0]\n",
    "    \n",
    "    return 1.0 / ( 1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Logistic Regression Cost Function\n",
    "def lrCostFunction(y, hx):\n",
    "  \n",
    "    # compute cost for given theta parameters\n",
    "    j = -y.dot(np.log(hx)) - ((1 - y).dot(np.log(1-hx)))\n",
    "    \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent function to minimize the Logistic Regression Cost Function.\n",
    "def lrGradient(X, y, theta, alpha, num_iter):\n",
    "    # empty list to store the value of the cost function over number of iterations\n",
    "    cost = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        # call sigmoid function \n",
    "        hx = sigmoid(X, theta)\n",
    "        # calculate error\n",
    "        error = hx - y\n",
    "        # calculate gradient\n",
    "        grad = X.T.dot(error)\n",
    "        # update values in theta\n",
    "        theta[0] = theta[0] - alpha * error.sum()\n",
    "        theta[1:] = theta[1:] - alpha * grad\n",
    "        \n",
    "        cost.append(lrCostFunction(y, hx))\n",
    "        \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feseg\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# m = Number of training examples\n",
    "# n = number of features\n",
    "m, n = X.shape\n",
    "# initialize theta(weights) parameters to zeros\n",
    "theta = np.zeros(1+n)\n",
    "\n",
    "# set learning rate to 0.01 and number of iterations to 500\n",
    "alpha = 0.9\n",
    "num_iter = 100\n",
    "\n",
    "cost = lrGradient(X, y, theta, alpha, num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
